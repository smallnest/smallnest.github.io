<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Asynchronous Programming in Rust</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="01_getting_started/01_chapter.html"><strong aria-hidden="true">1.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="01_getting_started/02_why_async.html"><strong aria-hidden="true">1.1.</strong> Why Async?</a></li><li class="chapter-item expanded "><a href="01_getting_started/03_state_of_async_rust.html"><strong aria-hidden="true">1.2.</strong> The State of Asynchronous Rust</a></li><li class="chapter-item expanded "><a href="01_getting_started/04_async_await_primer.html"><strong aria-hidden="true">1.3.</strong> async/.await Primer</a></li></ol></li><li class="chapter-item expanded "><a href="02_execution/01_chapter.html"><strong aria-hidden="true">2.</strong> Under the Hood: Executing Futures and Tasks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="02_execution/02_future.html"><strong aria-hidden="true">2.1.</strong> The Future Trait</a></li><li class="chapter-item expanded "><a href="02_execution/03_wakeups.html"><strong aria-hidden="true">2.2.</strong> Task Wakeups with Waker</a></li><li class="chapter-item expanded "><a href="02_execution/04_executor.html"><strong aria-hidden="true">2.3.</strong> Applied: Build an Executor</a></li><li class="chapter-item expanded "><a href="02_execution/05_io.html"><strong aria-hidden="true">2.4.</strong> Executors and System IO</a></li></ol></li><li class="chapter-item expanded "><a href="03_async_await/01_chapter.html"><strong aria-hidden="true">3.</strong> async/await</a></li><li class="chapter-item expanded "><a href="04_pinning/01_chapter.html"><strong aria-hidden="true">4.</strong> Pinning</a></li><li class="chapter-item expanded "><a href="05_streams/01_chapter.html"><strong aria-hidden="true">5.</strong> Streams</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="05_streams/02_iteration_and_concurrency.html"><strong aria-hidden="true">5.1.</strong> Iteration and Concurrency</a></li></ol></li><li class="chapter-item expanded "><a href="06_multiple_futures/01_chapter.html"><strong aria-hidden="true">6.</strong> Executing Multiple Futures at a Time</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="06_multiple_futures/02_join.html"><strong aria-hidden="true">6.1.</strong> join!</a></li><li class="chapter-item expanded "><a href="06_multiple_futures/03_select.html"><strong aria-hidden="true">6.2.</strong> select!</a></li><li class="chapter-item expanded "><a href="06_multiple_futures/04_spawning.html"><strong aria-hidden="true">6.3.</strong> Spawning</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.4.</strong> TODO: Cancellation and Timeouts</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.5.</strong> TODO: FuturesUnordered</div></li></ol></li><li class="chapter-item expanded "><a href="07_workarounds/01_chapter.html"><strong aria-hidden="true">7.</strong> Workarounds to Know and Love</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="07_workarounds/02_err_in_async_blocks.html"><strong aria-hidden="true">7.1.</strong> ? in async Blocks</a></li><li class="chapter-item expanded "><a href="07_workarounds/03_send_approximation.html"><strong aria-hidden="true">7.2.</strong> Send Approximation</a></li><li class="chapter-item expanded "><a href="07_workarounds/04_recursion.html"><strong aria-hidden="true">7.3.</strong> Recursion</a></li><li class="chapter-item expanded "><a href="07_workarounds/05_async_in_traits.html"><strong aria-hidden="true">7.4.</strong> async in Traits</a></li></ol></li><li class="chapter-item expanded "><a href="08_ecosystem/00_chapter.html"><strong aria-hidden="true">8.</strong> The Async Ecosystem</a></li><li class="chapter-item expanded "><a href="09_example/00_intro.html"><strong aria-hidden="true">9.</strong> Final Project: HTTP Server</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="09_example/01_running_async_code.html"><strong aria-hidden="true">9.1.</strong> Running Asynchronous Code</a></li><li class="chapter-item expanded "><a href="09_example/02_handling_connections_concurrently.html"><strong aria-hidden="true">9.2.</strong> Handling Connections Concurrently</a></li><li class="chapter-item expanded "><a href="09_example/03_tests.html"><strong aria-hidden="true">9.3.</strong> Testing the Server</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">10.</strong> TODO: I/O</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">10.1.</strong> TODO: AsyncRead and AsyncWrite</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">11.</strong> TODO: Asynchronous Design Patterns: Solutions and Suggestions</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">11.1.</strong> TODO: Modeling Servers and the Request/Response Pattern</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">11.2.</strong> TODO: Managing Shared State</div></li></ol></li><li class="chapter-item expanded "><a href="12_appendix/01_translations.html"><strong aria-hidden="true">12.</strong> Appendix: Translations of the Book</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Asynchronous Programming in Rust</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/rust-lang/async-book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div id="01_getting_started-01_chapter"></div><h1 id="01_getting_started-01_chapter-getting-started"><a class="header" href="#01_getting_started-01_chapter-getting-started">Getting Started</a></h1>
<p>Welcome to Asynchronous Programming in Rust! If you're looking to start writing
asynchronous Rust code, you've come to the right place. Whether you're building
a web server, a database, or an operating system, this book will show you
how to use Rust's asynchronous programming tools to get the most out of your
hardware.</p>
<h2 id="01_getting_started-01_chapter-what-this-book-covers"><a class="header" href="#01_getting_started-01_chapter-what-this-book-covers">What This Book Covers</a></h2>
<p>This book aims to be a comprehensive, up-to-date guide to using Rust's async
language features and libraries, appropriate for beginners and old hands alike.</p>
<ul>
<li>
<p>The early chapters provide an introduction to async programming in general,
and to Rust's particular take on it.</p>
</li>
<li>
<p>The middle chapters discuss key utilities and control-flow tools you can use
when writing async code, and describe best-practices for structuring libraries
and applications to maximize performance and reusability.</p>
</li>
<li>
<p>The last section of the book covers the broader async ecosystem, and provides
a number of examples of how to accomplish common tasks.</p>
</li>
</ul>
<p>With that out of the way, let's explore the exciting world of Asynchronous
Programming in Rust!</p>
<div style="break-before: page; page-break-before: always;"></div><div id="01_getting_started-02_why_async"></div><h1 id="01_getting_started-02_why_async-why-async"><a class="header" href="#01_getting_started-02_why_async-why-async">Why Async?</a></h1>
<p>We all love how Rust empowers us to write fast, safe software.
But how does asynchronous programming fit into this vision?</p>
<p>Asynchronous programming, or async for short, is a <em>concurrent programming model</em>
supported by an increasing number of programming languages.
It lets you run a large number of concurrent
tasks on a small number of OS threads, while preserving much of the
look and feel of ordinary synchronous programming, through the
<code>async/await</code> syntax.</p>
<h2 id="01_getting_started-02_why_async-async-vs-other-concurrency-models"><a class="header" href="#01_getting_started-02_why_async-async-vs-other-concurrency-models">Async vs other concurrency models</a></h2>
<p>Concurrent programming is less mature and "standardized" than
regular, sequential programming. As a result, we express concurrency
differently depending on which concurrent programming model
the language is supporting.
A brief overview of the most popular concurrency models can help
you understand how asynchronous programming fits within the broader
field of concurrent programming:</p>
<ul>
<li><strong>OS threads</strong> don't require any changes to the programming model,
which makes it very easy to express concurrency. However, synchronizing
between threads can be difficult, and the performance overhead is large.
Thread pools can mitigate some of these costs, but not enough to support
massive IO-bound workloads.</li>
<li><strong>Event-driven programming</strong>, in conjunction with <em>callbacks</em>, can be very
performant, but tends to result in a verbose, "non-linear" control flow.
Data flow and error propagation is often hard to follow.</li>
<li><strong>Coroutines</strong>, like threads, don't require changes to the programming model,
which makes them easy to use. Like async, they can also support a large
number of tasks. However, they abstract away low-level details that
are important for systems programming and custom runtime implementors.</li>
<li><strong>The actor model</strong> divides all concurrent computation into units called
actors, which communicate through fallible message passing, much like
in distributed systems. The actor model can be efficiently implemented, but it leaves
many practical issues unanswered, such as flow control and retry logic.</li>
</ul>
<p>In summary, asynchronous programming allows highly performant implementations
that are suitable for low-level languages like Rust, while providing
most of the ergonomic benefits of threads and coroutines.</p>
<h2 id="01_getting_started-02_why_async-async-in-rust-vs-other-languages"><a class="header" href="#01_getting_started-02_why_async-async-in-rust-vs-other-languages">Async in Rust vs other languages</a></h2>
<p>Although asynchronous programming is supported in many languages, some
details vary across implementations. Rust's implementation of async
differs from most languages in a few ways:</p>
<ul>
<li><strong>Futures are inert</strong> in Rust and make progress only when polled. Dropping a
future stops it from making further progress.</li>
<li><strong>Async is zero-cost</strong> in Rust, which means that you only pay for what you use.
Specifically, you can use async without heap allocations and dynamic dispatch,
which is great for performance!
This also lets you use async in constrained environments, such as embedded systems.</li>
<li><strong>No built-in runtime</strong> is provided by Rust. Instead, runtimes are provided by
community maintained crates.</li>
<li><strong>Both single- and multithreaded</strong> runtimes are available in Rust, which have
different strengths and weaknesses.</li>
</ul>
<h2 id="01_getting_started-02_why_async-async-vs-threads-in-rust"><a class="header" href="#01_getting_started-02_why_async-async-vs-threads-in-rust">Async vs threads in Rust</a></h2>
<p>The primary alternative to async in Rust is using OS threads, either
directly through <a href="https://doc.rust-lang.org/std/thread/"><code>std::thread</code></a>
or indirectly through a thread pool.
Migrating from threads to async or vice versa
typically requires major refactoring work, both in terms of implementation and
(if you are building a library) any exposed public interfaces. As such,
picking the model that suits your needs early can save a lot of development time.</p>
<p><strong>OS threads</strong> are suitable for a small number of tasks, since threads come with
CPU and memory overhead. Spawning and switching between threads
is quite expensive as even idle threads consume system resources.
A thread pool library can help mitigate some of these costs, but not all.
However, threads let you reuse existing synchronous code without significant
code changes—no particular programming model is required.
In some operating systems, you can also change the priority of a thread,
which is useful for drivers and other latency sensitive applications.</p>
<p><strong>Async</strong> provides significantly reduced CPU and memory
overhead, especially for workloads with a
large amount of IO-bound tasks, such as servers and databases.
All else equal, you can have orders of magnitude more tasks than OS threads,
because an async runtime uses a small amount of (expensive) threads to handle
a large amount of (cheap) tasks.
However, async Rust results in larger binary blobs due to the state
machines generated from async functions and since each executable
bundles an async runtime.</p>
<p>On a last note, asynchronous programming is not <em>better</em> than threads,
but different.
If you don't need async for performance reasons, threads can often be
the simpler alternative.</p>
<h3 id="01_getting_started-02_why_async-example-concurrent-downloading"><a class="header" href="#01_getting_started-02_why_async-example-concurrent-downloading">Example: Concurrent downloading</a></h3>
<p>In this example our goal is to download two web pages concurrently.
In a typical threaded application we need to spawn threads
to achieve concurrency:</p>
<pre><code class="language-rust ignore">fn get_two_sites() {
    // Spawn two threads to do work.
    let thread_one = thread::spawn(|| download("https://www.foo.com"));
    let thread_two = thread::spawn(|| download("https://www.bar.com"));

    // Wait for both threads to complete.
    thread_one.join().expect("thread one panicked");
    thread_two.join().expect("thread two panicked");
}</code></pre>
<p>However, downloading a web page is a small task; creating a thread
for such a small amount of work is quite wasteful. For a larger application, it
can easily become a bottleneck. In async Rust, we can run these tasks
concurrently without extra threads:</p>
<pre><code class="language-rust ignore">async fn get_two_sites_async() {
    // Create two different "futures" which, when run to completion,
    // will asynchronously download the webpages.
    let future_one = download_async("https://www.foo.com");
    let future_two = download_async("https://www.bar.com");

    // Run both futures to completion at the same time.
    join!(future_one, future_two);
}</code></pre>
<p>Here, no extra threads are created. Additionally, all function calls are statically
dispatched, and there are no heap allocations!
However, we need to write the code to be asynchronous in the first place,
which this book will help you achieve.</p>
<h2 id="01_getting_started-02_why_async-custom-concurrency-models-in-rust"><a class="header" href="#01_getting_started-02_why_async-custom-concurrency-models-in-rust">Custom concurrency models in Rust</a></h2>
<p>On a last note, Rust doesn't force you to choose between threads and async.
You can use both models within the same application, which can be
useful when you have mixed threaded and async dependencies.
In fact, you can even use a different concurrency model altogether,
such as event-driven programming, as long as you find a library that
implements it.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="01_getting_started-03_state_of_async_rust"></div><h1 id="01_getting_started-03_state_of_async_rust-the-state-of-asynchronous-rust"><a class="header" href="#01_getting_started-03_state_of_async_rust-the-state-of-asynchronous-rust">The State of Asynchronous Rust</a></h1>
<p>Parts of async Rust are supported with the same stability guarantees as
synchronous Rust. Other parts are still maturing and will change
over time. With async Rust, you can expect:</p>
<ul>
<li>Outstanding runtime performance for typical concurrent workloads.</li>
<li>More frequent interaction with advanced language features, such as lifetimes
and pinning.</li>
<li>Some compatibility constraints, both between sync and async code, and between
different async runtimes.</li>
<li>Higher maintenance burden, due to the ongoing evolution of async runtimes
and language support.</li>
</ul>
<p>In short, async Rust is more difficult to use and can result in a higher
maintenance burden than synchronous Rust,
but gives you best-in-class performance in return.
All areas of async Rust are constantly improving,
so the impact of these issues will wear off over time.</p>
<h2 id="01_getting_started-03_state_of_async_rust-language-and-library-support"><a class="header" href="#01_getting_started-03_state_of_async_rust-language-and-library-support">Language and library support</a></h2>
<p>While asynchronous programming is supported by Rust itself,
most async applications depend on functionality provided
by community crates.
As such, you need to rely on a mixture of
language features and library support:</p>
<ul>
<li>The most fundamental traits, types and functions, such as the
<a href="https://doc.rust-lang.org/std/future/trait.Future.html"><code>Future</code></a> trait
are provided by the standard library.</li>
<li>The <code>async/await</code> syntax is supported directly by the Rust compiler.</li>
<li>Many utility types, macros and functions are provided by the
<a href="https://docs.rs/futures/"><code>futures</code></a> crate. They can be used in any async
Rust application.</li>
<li>Execution of async code, IO and task spawning are provided by "async
runtimes", such as Tokio and async-std. Most async applications, and some
async crates, depend on a specific runtime. See
<a href="#08_ecosystem-00_chapter">"The Async Ecosystem"</a> section for more
details.</li>
</ul>
<p>Some language features you may be used to from synchronous Rust are not yet
available in async Rust. Notably, Rust does not let you declare async
functions in traits. Instead, you need to use workarounds to achieve the same
result, which can be more verbose.</p>
<h2 id="01_getting_started-03_state_of_async_rust-compiling-and-debugging"><a class="header" href="#01_getting_started-03_state_of_async_rust-compiling-and-debugging">Compiling and debugging</a></h2>
<p>For the most part, compiler- and runtime errors in async Rust work
the same way as they have always done in Rust. There are a few
noteworthy differences:</p>
<h3 id="01_getting_started-03_state_of_async_rust-compilation-errors"><a class="header" href="#01_getting_started-03_state_of_async_rust-compilation-errors">Compilation errors</a></h3>
<p>Compilation errors in async Rust conform to the same high standards as
synchronous Rust, but since async Rust often depends on more complex language
features, such as lifetimes and pinning, you may encounter these types of
errors more frequently.</p>
<h3 id="01_getting_started-03_state_of_async_rust-runtime-errors"><a class="header" href="#01_getting_started-03_state_of_async_rust-runtime-errors">Runtime errors</a></h3>
<p>Whenever the compiler encounters an async function, it generates a state
machine under the hood. Stack traces in async Rust typically contain details
from these state machines, as well as function calls from
the runtime. As such, interpreting stack traces can be a bit more involved than
it would be in synchronous Rust.</p>
<h3 id="01_getting_started-03_state_of_async_rust-new-failure-modes"><a class="header" href="#01_getting_started-03_state_of_async_rust-new-failure-modes">New failure modes</a></h3>
<p>A few novel failure modes are possible in async Rust, for instance
if you call a blocking function from an async context or if you implement
the <code>Future</code> trait incorrectly. Such errors can silently pass both the
compiler and sometimes even unit tests. Having a firm understanding
of the underlying concepts, which this book aims to give you, can help you
avoid these pitfalls.</p>
<h2 id="01_getting_started-03_state_of_async_rust-compatibility-considerations"><a class="header" href="#01_getting_started-03_state_of_async_rust-compatibility-considerations">Compatibility considerations</a></h2>
<p>Asynchronous and synchronous code cannot always be combined freely.
For instance, you can't directly call an async function from a sync function.
Sync and async code also tend to promote different design patterns, which can
make it difficult to compose code intended for the different environments.</p>
<p>Even async code cannot always be combined freely. Some crates depend on a
specific async runtime to function. If so, it is usually specified in the
crate's dependency list.</p>
<p>These compatibility issues can limit your options, so make sure to
research which async runtime and what crates you may need early.
Once you have settled in with a runtime, you won't have to worry
much about compatibility.</p>
<h2 id="01_getting_started-03_state_of_async_rust-performance-characteristics"><a class="header" href="#01_getting_started-03_state_of_async_rust-performance-characteristics">Performance characteristics</a></h2>
<p>The performance of async Rust depends on the implementation of the
async runtime you're using.
Even though the runtimes that power async Rust applications are relatively new,
they perform exceptionally well for most practical workloads.</p>
<p>That said, most of the async ecosystem assumes a <em>multi-threaded</em> runtime.
This makes it difficult to enjoy the theoretical performance benefits
of single-threaded async applications, namely cheaper synchronization.
Another overlooked use-case is <em>latency sensitive tasks</em>, which are
important for drivers, GUI applications and so on. Such tasks depend
on runtime and/or OS support in order to be scheduled appropriately.
You can expect better library support for these use cases in the future.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="01_getting_started-04_async_await_primer"></div><h1 id="01_getting_started-04_async_await_primer-asyncawait-primer"><a class="header" href="#01_getting_started-04_async_await_primer-asyncawait-primer"><code>async</code>/<code>.await</code> Primer</a></h1>
<p><code>async</code>/<code>.await</code> is Rust's built-in tool for writing asynchronous functions
that look like synchronous code. <code>async</code> transforms a block of code into a
state machine that implements a trait called <code>Future</code>. Whereas calling a
blocking function in a synchronous method would block the whole thread,
blocked <code>Future</code>s will yield control of the thread, allowing other
<code>Future</code>s to run.</p>
<p>Let's add some dependencies to the <code>Cargo.toml</code> file:</p>
<pre><code class="language-toml">[dependencies]
futures = "0.3"
</code></pre>
<p>To create an asynchronous function, you can use the <code>async fn</code> syntax:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>async fn do_something() { /* ... */ }
<span class="boring">}</span></code></pre></pre>
<p>The value returned by <code>async fn</code> is a <code>Future</code>. For anything to happen,
the <code>Future</code> needs to be run on an executor.</p>
<pre><pre class="playground"><code class="language-rust edition2018">// `block_on` blocks the current thread until the provided future has run to
// completion. Other executors provide more complex behavior, like scheduling
// multiple futures onto the same thread.
use futures::executor::block_on;

async fn hello_world() {
    println!("hello, world!");
}

fn main() {
    let future = hello_world(); // Nothing is printed
    block_on(future); // `future` is run and "hello, world!" is printed
}</code></pre></pre>
<p>Inside an <code>async fn</code>, you can use <code>.await</code> to wait for the completion of
another type that implements the <code>Future</code> trait, such as the output of
another <code>async fn</code>. Unlike <code>block_on</code>, <code>.await</code> doesn't block the current
thread, but instead asynchronously waits for the future to complete, allowing
other tasks to run if the future is currently unable to make progress.</p>
<p>For example, imagine that we have three <code>async fn</code>: <code>learn_song</code>, <code>sing_song</code>,
and <code>dance</code>:</p>
<pre><code class="language-rust ignore">async fn learn_song() -&gt; Song { /* ... */ }
async fn sing_song(song: Song) { /* ... */ }
async fn dance() { /* ... */ }</code></pre>
<p>One way to do learn, sing, and dance would be to block on each of these
individually:</p>
<pre><code class="language-rust ignore">fn main() {
    let song = block_on(learn_song());
    block_on(sing_song(song));
    block_on(dance());
}</code></pre>
<p>However, we're not giving the best performance possible this way—we're
only ever doing one thing at once! Clearly we have to learn the song before
we can sing it, but it's possible to dance at the same time as learning and
singing the song. To do this, we can create two separate <code>async fn</code> which
can be run concurrently:</p>
<pre><code class="language-rust ignore">async fn learn_and_sing() {
    // Wait until the song has been learned before singing it.
    // We use `.await` here rather than `block_on` to prevent blocking the
    // thread, which makes it possible to `dance` at the same time.
    let song = learn_song().await;
    sing_song(song).await;
}

async fn async_main() {
    let f1 = learn_and_sing();
    let f2 = dance();

    // `join!` is like `.await` but can wait for multiple futures concurrently.
    // If we're temporarily blocked in the `learn_and_sing` future, the `dance`
    // future will take over the current thread. If `dance` becomes blocked,
    // `learn_and_sing` can take back over. If both futures are blocked, then
    // `async_main` is blocked and will yield to the executor.
    futures::join!(f1, f2);
}

fn main() {
    block_on(async_main());
}</code></pre>
<p>In this example, learning the song must happen before singing the song, but
both learning and singing can happen at the same time as dancing. If we used
<code>block_on(learn_song())</code> rather than <code>learn_song().await</code> in <code>learn_and_sing</code>,
the thread wouldn't be able to do anything else while <code>learn_song</code> was running.
This would make it impossible to dance at the same time. By <code>.await</code>-ing
the <code>learn_song</code> future, we allow other tasks to take over the current thread
if <code>learn_song</code> is blocked. This makes it possible to run multiple futures
to completion concurrently on the same thread.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="02_execution-01_chapter"></div><h1 id="02_execution-01_chapter-under-the-hood-executing-futures-and-tasks"><a class="header" href="#02_execution-01_chapter-under-the-hood-executing-futures-and-tasks">Under the Hood: Executing <code>Future</code>s and Tasks</a></h1>
<p>In this section, we'll cover the underlying structure of how <code>Future</code>s and
asynchronous tasks are scheduled. If you're only interested in learning
how to write higher-level code that uses existing <code>Future</code> types and aren't
interested in the details of how <code>Future</code> types work, you can skip ahead to
the <code>async</code>/<code>await</code> chapter. However, several of the topics discussed in this
chapter are useful for understanding how <code>async</code>/<code>await</code> code works,
understanding the runtime and performance properties of <code>async</code>/<code>await</code> code,
and building new asynchronous primitives. If you decide to skip this section
now, you may want to bookmark it to revisit in the future.</p>
<p>Now, with that out of the way, let's talk about the <code>Future</code> trait.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="02_execution-02_future"></div><h1 id="02_execution-02_future-the-future-trait"><a class="header" href="#02_execution-02_future-the-future-trait">The <code>Future</code> Trait</a></h1>
<p>The <code>Future</code> trait is at the center of asynchronous programming in Rust.
A <code>Future</code> is an asynchronous computation that can produce a value
(although that value may be empty, e.g. <code>()</code>). A <em>simplified</em> version of
the future trait might look something like this:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait SimpleFuture {
    type Output;
    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt;;
}

enum Poll&lt;T&gt; {
    Ready(T),
    Pending,
}
<span class="boring">}</span></code></pre></pre>
<p>Futures can be advanced by calling the <code>poll</code> function, which will drive the
future as far towards completion as possible. If the future completes, it
returns <code>Poll::Ready(result)</code>. If the future is not able to complete yet, it
returns <code>Poll::Pending</code> and arranges for the <code>wake()</code> function to be called
when the <code>Future</code> is ready to make more progress. When <code>wake()</code> is called, the
executor driving the <code>Future</code> will call <code>poll</code> again so that the <code>Future</code> can
make more progress.</p>
<p>Without <code>wake()</code>, the executor would have no way of knowing when a particular
future could make progress, and would have to be constantly polling every
future. With <code>wake()</code>, the executor knows exactly which futures are ready to
be <code>poll</code>ed.</p>
<p>For example, consider the case where we want to read from a socket that may
or may not have data available already. If there is data, we can read it
in and return <code>Poll::Ready(data)</code>, but if no data is ready, our future is
blocked and can no longer make progress. When no data is available, we
must register <code>wake</code> to be called when data becomes ready on the socket,
which will tell the executor that our future is ready to make progress.
A simple <code>SocketRead</code> future might look something like this:</p>
<pre><code class="language-rust ignore">pub struct SocketRead&lt;'a&gt; {
    socket: &amp;'a Socket,
}

impl SimpleFuture for SocketRead&lt;'_&gt; {
    type Output = Vec&lt;u8&gt;;

    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        if self.socket.has_data_to_read() {
            // The socket has data -- read it into a buffer and return it.
            Poll::Ready(self.socket.read_buf())
        } else {
            // The socket does not yet have data.
            //
            // Arrange for `wake` to be called once data is available.
            // When data becomes available, `wake` will be called, and the
            // user of this `Future` will know to call `poll` again and
            // receive data.
            self.socket.set_readable_callback(wake);
            Poll::Pending
        }
    }
}</code></pre>
<p>This model of <code>Future</code>s allows for composing together multiple asynchronous
operations without needing intermediate allocations. Running multiple futures
at once or chaining futures together can be implemented via allocation-free
state machines, like this:</p>
<pre><code class="language-rust ignore">/// A SimpleFuture that runs two other futures to completion concurrently.
///
/// Concurrency is achieved via the fact that calls to `poll` each future
/// may be interleaved, allowing each future to advance itself at its own pace.
pub struct Join&lt;FutureA, FutureB&gt; {
    // Each field may contain a future that should be run to completion.
    // If the future has already completed, the field is set to `None`.
    // This prevents us from polling a future after it has completed, which
    // would violate the contract of the `Future` trait.
    a: Option&lt;FutureA&gt;,
    b: Option&lt;FutureB&gt;,
}

impl&lt;FutureA, FutureB&gt; SimpleFuture for Join&lt;FutureA, FutureB&gt;
where
    FutureA: SimpleFuture&lt;Output = ()&gt;,
    FutureB: SimpleFuture&lt;Output = ()&gt;,
{
    type Output = ();
    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        // Attempt to complete future `a`.
        if let Some(a) = &amp;mut self.a {
            if let Poll::Ready(()) = a.poll(wake) {
                self.a.take();
            }
        }

        // Attempt to complete future `b`.
        if let Some(b) = &amp;mut self.b {
            if let Poll::Ready(()) = b.poll(wake) {
                self.b.take();
            }
        }

        if self.a.is_none() &amp;&amp; self.b.is_none() {
            // Both futures have completed -- we can return successfully
            Poll::Ready(())
        } else {
            // One or both futures returned `Poll::Pending` and still have
            // work to do. They will call `wake()` when progress can be made.
            Poll::Pending
        }
    }
}</code></pre>
<p>This shows how multiple futures can be run simultaneously without needing
separate allocations, allowing for more efficient asynchronous programs.
Similarly, multiple sequential futures can be run one after another, like this:</p>
<pre><code class="language-rust ignore">/// A SimpleFuture that runs two futures to completion, one after another.
//
// Note: for the purposes of this simple example, `AndThenFut` assumes both
// the first and second futures are available at creation-time. The real
// `AndThen` combinator allows creating the second future based on the output
// of the first future, like `get_breakfast.and_then(|food| eat(food))`.
pub struct AndThenFut&lt;FutureA, FutureB&gt; {
    first: Option&lt;FutureA&gt;,
    second: FutureB,
}

impl&lt;FutureA, FutureB&gt; SimpleFuture for AndThenFut&lt;FutureA, FutureB&gt;
where
    FutureA: SimpleFuture&lt;Output = ()&gt;,
    FutureB: SimpleFuture&lt;Output = ()&gt;,
{
    type Output = ();
    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        if let Some(first) = &amp;mut self.first {
            match first.poll(wake) {
                // We've completed the first future -- remove it and start on
                // the second!
                Poll::Ready(()) =&gt; self.first.take(),
                // We couldn't yet complete the first future.
                Poll::Pending =&gt; return Poll::Pending,
            };
        }
        // Now that the first future is done, attempt to complete the second.
        self.second.poll(wake)
    }
}</code></pre>
<p>These examples show how the <code>Future</code> trait can be used to express asynchronous
control flow without requiring multiple allocated objects and deeply nested
callbacks. With the basic control-flow out of the way, let's talk about the
real <code>Future</code> trait and how it is different.</p>
<pre><code class="language-rust ignore">trait Future {
    type Output;
    fn poll(
        // Note the change from `&amp;mut self` to `Pin&lt;&amp;mut Self&gt;`:
        self: Pin&lt;&amp;mut Self&gt;,
        // and the change from `wake: fn()` to `cx: &amp;mut Context&lt;'_&gt;`:
        cx: &amp;mut Context&lt;'_&gt;,
    ) -&gt; Poll&lt;Self::Output&gt;;
}</code></pre>
<p>The first change you'll notice is that our <code>self</code> type is no longer <code>&amp;mut Self</code>,
but has changed to <code>Pin&lt;&amp;mut Self&gt;</code>. We'll talk more about pinning in <a href="#04_pinning-01_chapter">a later
section</a>, but for now know that it allows us to create futures that
are immovable. Immovable objects can store pointers between their fields,
e.g. <code>struct MyFut { a: i32, ptr_to_a: *const i32 }</code>. Pinning is necessary
to enable async/await.</p>
<p>Secondly, <code>wake: fn()</code> has changed to <code>&amp;mut Context&lt;'_&gt;</code>. In <code>SimpleFuture</code>,
we used a call to a function pointer (<code>fn()</code>) to tell the future executor that
the future in question should be polled. However, since <code>fn()</code> is just a
function pointer, it can't store any data about <em>which</em> <code>Future</code> called <code>wake</code>.</p>
<p>In a real-world scenario, a complex application like a web server may have
thousands of different connections whose wakeups should all be
managed separately. The <code>Context</code> type solves this by providing access to
a value of type <code>Waker</code>, which can be used to wake up a specific task.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="02_execution-03_wakeups"></div><h1 id="02_execution-03_wakeups-task-wakeups-with-waker"><a class="header" href="#02_execution-03_wakeups-task-wakeups-with-waker">Task Wakeups with <code>Waker</code></a></h1>
<p>It's common that futures aren't able to complete the first time they are
<code>poll</code>ed. When this happens, the future needs to ensure that it is polled
again once it is ready to make more progress. This is done with the <code>Waker</code>
type.</p>
<p>Each time a future is polled, it is polled as part of a "task". Tasks are
the top-level futures that have been submitted to an executor.</p>
<p><code>Waker</code> provides a <code>wake()</code> method that can be used to tell the executor that
the associated task should be awoken. When <code>wake()</code> is called, the executor
knows that the task associated with the <code>Waker</code> is ready to make progress, and
its future should be polled again.</p>
<p><code>Waker</code> also implements <code>clone()</code> so that it can be copied around and stored.</p>
<p>Let's try implementing a simple timer future using <code>Waker</code>.</p>
<h2 id="02_execution-03_wakeups-applied-build-a-timer"><a class="header" href="#02_execution-03_wakeups-applied-build-a-timer">Applied: Build a Timer</a></h2>
<p>For the sake of the example, we'll just spin up a new thread when the timer
is created, sleep for the required time, and then signal the timer future
when the time window has elapsed.</p>
<p>First, start a new project with <code>cargo new --lib timer_future</code> and add the imports
we'll need to get started to <code>src/lib.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::{
    future::Future,
    pin::Pin,
    sync::{Arc, Mutex},
    task::{Context, Poll, Waker},
    thread,
    time::Duration,
};
<span class="boring">}</span></code></pre></pre>
<p>Let's start by defining the future type itself. Our future needs a way for the
thread to communicate that the timer has elapsed and the future should complete.
We'll use a shared <code>Arc&lt;Mutex&lt;..&gt;&gt;</code> value to communicate between the thread and
the future.</p>
<pre><code class="language-rust ignore">pub struct TimerFuture {
    shared_state: Arc&lt;Mutex&lt;SharedState&gt;&gt;,
}

/// Shared state between the future and the waiting thread
struct SharedState {
    /// Whether or not the sleep time has elapsed
    completed: bool,

    /// The waker for the task that `TimerFuture` is running on.
    /// The thread can use this after setting `completed = true` to tell
    /// `TimerFuture`'s task to wake up, see that `completed = true`, and
    /// move forward.
    waker: Option&lt;Waker&gt;,
}</code></pre>
<p>Now, let's actually write the <code>Future</code> implementation!</p>
<pre><code class="language-rust ignore">impl Future for TimerFuture {
    type Output = ();
    fn poll(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;Self::Output&gt; {
        // Look at the shared state to see if the timer has already completed.
        let mut shared_state = self.shared_state.lock().unwrap();
        if shared_state.completed {
            Poll::Ready(())
        } else {
            // Set waker so that the thread can wake up the current task
            // when the timer has completed, ensuring that the future is polled
            // again and sees that `completed = true`.
            //
            // It's tempting to do this once rather than repeatedly cloning
            // the waker each time. However, the `TimerFuture` can move between
            // tasks on the executor, which could cause a stale waker pointing
            // to the wrong task, preventing `TimerFuture` from waking up
            // correctly.
            //
            // N.B. it's possible to check for this using the `Waker::will_wake`
            // function, but we omit that here to keep things simple.
            shared_state.waker = Some(cx.waker().clone());
            Poll::Pending
        }
    }
}</code></pre>
<p>Pretty simple, right? If the thread has set <code>shared_state.completed = true</code>,
we're done! Otherwise, we clone the <code>Waker</code> for the current task and pass it to
<code>shared_state.waker</code> so that the thread can wake the task back up.</p>
<p>Importantly, we have to update the <code>Waker</code> every time the future is polled
because the future may have moved to a different task with a different
<code>Waker</code>. This will happen when futures are passed around between tasks after
being polled.</p>
<p>Finally, we need the API to actually construct the timer and start the thread:</p>
<pre><code class="language-rust ignore">impl TimerFuture {
    /// Create a new `TimerFuture` which will complete after the provided
    /// timeout.
    pub fn new(duration: Duration) -&gt; Self {
        let shared_state = Arc::new(Mutex::new(SharedState {
            completed: false,
            waker: None,
        }));

        // Spawn the new thread
        let thread_shared_state = shared_state.clone();
        thread::spawn(move || {
            thread::sleep(duration);
            let mut shared_state = thread_shared_state.lock().unwrap();
            // Signal that the timer has completed and wake up the last
            // task on which the future was polled, if one exists.
            shared_state.completed = true;
            if let Some(waker) = shared_state.waker.take() {
                waker.wake()
            }
        });

        TimerFuture { shared_state }
    }
}</code></pre>
<p>Woot! That's all we need to build a simple timer future. Now, if only we had
an executor to run the future on...</p>
<div style="break-before: page; page-break-before: always;"></div><div id="02_execution-04_executor"></div><h1 id="02_execution-04_executor-applied-build-an-executor"><a class="header" href="#02_execution-04_executor-applied-build-an-executor">Applied: Build an Executor</a></h1>
<p>Rust's <code>Future</code>s are lazy: they won't do anything unless actively driven to
completion. One way to drive a future to completion is to <code>.await</code> it inside
an <code>async</code> function, but that just pushes the problem one level up: who will
run the futures returned from the top-level <code>async</code> functions? The answer is
that we need a <code>Future</code> executor.</p>
<p><code>Future</code> executors take a set of top-level <code>Future</code>s and run them to completion
by calling <code>poll</code> whenever the <code>Future</code> can make progress. Typically, an
executor will <code>poll</code> a future once to start off. When <code>Future</code>s indicate that
they are ready to make progress by calling <code>wake()</code>, they are placed back
onto a queue and <code>poll</code> is called again, repeating until the <code>Future</code> has
completed.</p>
<p>In this section, we'll write our own simple executor capable of running a large
number of top-level futures to completion concurrently.</p>
<p>For this example, we depend on the <code>futures</code> crate for the <code>ArcWake</code> trait,
which provides an easy way to construct a <code>Waker</code>. Edit <code>Cargo.toml</code> to add
a new dependency:</p>
<pre><code class="language-toml">[package]
name = "timer_future"
version = "0.1.0"
authors = ["XYZ Author"]
edition = "2021"

[dependencies]
futures = "0.3"
</code></pre>
<p>Next, we need the following imports at the top of <code>src/main.rs</code>:</p>
<pre><code class="language-rust ignore">use futures::{
    future::{BoxFuture, FutureExt},
    task::{waker_ref, ArcWake},
};
use std::{
    future::Future,
    sync::mpsc::{sync_channel, Receiver, SyncSender},
    sync::{Arc, Mutex},
    task::Context,
    time::Duration,
};
// The timer we wrote in the previous section:
use timer_future::TimerFuture;</code></pre>
<p>Our executor will work by sending tasks to run over a channel. The executor
will pull events off of the channel and run them. When a task is ready to
do more work (is awoken), it can schedule itself to be polled again by
putting itself back onto the channel.</p>
<p>In this design, the executor itself just needs the receiving end of the task
channel. The user will get a sending end so that they can spawn new futures.
Tasks themselves are just futures that can reschedule themselves, so we'll
store them as a future paired with a sender that the task can use to requeue
itself.</p>
<pre><code class="language-rust ignore">/// Task executor that receives tasks off of a channel and runs them.
struct Executor {
    ready_queue: Receiver&lt;Arc&lt;Task&gt;&gt;,
}

/// `Spawner` spawns new futures onto the task channel.
#[derive(Clone)]
struct Spawner {
    task_sender: SyncSender&lt;Arc&lt;Task&gt;&gt;,
}

/// A future that can reschedule itself to be polled by an `Executor`.
struct Task {
    /// In-progress future that should be pushed to completion.
    ///
    /// The `Mutex` is not necessary for correctness, since we only have
    /// one thread executing tasks at once. However, Rust isn't smart
    /// enough to know that `future` is only mutated from one thread,
    /// so we need to use the `Mutex` to prove thread-safety. A production
    /// executor would not need this, and could use `UnsafeCell` instead.
    future: Mutex&lt;Option&lt;BoxFuture&lt;'static, ()&gt;&gt;&gt;,

    /// Handle to place the task itself back onto the task queue.
    task_sender: SyncSender&lt;Arc&lt;Task&gt;&gt;,
}

fn new_executor_and_spawner() -&gt; (Executor, Spawner) {
    // Maximum number of tasks to allow queueing in the channel at once.
    // This is just to make `sync_channel` happy, and wouldn't be present in
    // a real executor.
    const MAX_QUEUED_TASKS: usize = 10_000;
    let (task_sender, ready_queue) = sync_channel(MAX_QUEUED_TASKS);
    (Executor { ready_queue }, Spawner { task_sender })
}</code></pre>
<p>Let's also add a method to spawner to make it easy to spawn new futures.
This method will take a future type, box it, and create a new <code>Arc&lt;Task&gt;</code> with
it inside which can be enqueued onto the executor.</p>
<pre><code class="language-rust ignore">impl Spawner {
    fn spawn(&amp;self, future: impl Future&lt;Output = ()&gt; + 'static + Send) {
        let future = future.boxed();
        let task = Arc::new(Task {
            future: Mutex::new(Some(future)),
            task_sender: self.task_sender.clone(),
        });
        self.task_sender.send(task).expect("too many tasks queued");
    }
}</code></pre>
<p>To poll futures, we'll need to create a <code>Waker</code>.
As discussed in the <a href="#02_execution-03_wakeups">task wakeups section</a>, <code>Waker</code>s are responsible
for scheduling a task to be polled again once <code>wake</code> is called. Remember that
<code>Waker</code>s tell the executor exactly which task has become ready, allowing
them to poll just the futures that are ready to make progress. The easiest way
to create a new <code>Waker</code> is by implementing the <code>ArcWake</code> trait and then using
the <code>waker_ref</code> or <code>.into_waker()</code> functions to turn an <code>Arc&lt;impl ArcWake&gt;</code>
into a <code>Waker</code>. Let's implement <code>ArcWake</code> for our tasks to allow them to be
turned into <code>Waker</code>s and awoken:</p>
<pre><code class="language-rust ignore">impl ArcWake for Task {
    fn wake_by_ref(arc_self: &amp;Arc&lt;Self&gt;) {
        // Implement `wake` by sending this task back onto the task channel
        // so that it will be polled again by the executor.
        let cloned = arc_self.clone();
        arc_self
            .task_sender
            .send(cloned)
            .expect("too many tasks queued");
    }
}</code></pre>
<p>When a <code>Waker</code> is created from an <code>Arc&lt;Task&gt;</code>, calling <code>wake()</code> on it will
cause a copy of the <code>Arc</code> to be sent onto the task channel. Our executor then
needs to pick up the task and poll it. Let's implement that:</p>
<pre><code class="language-rust ignore">impl Executor {
    fn run(&amp;self) {
        while let Ok(task) = self.ready_queue.recv() {
            // Take the future, and if it has not yet completed (is still Some),
            // poll it in an attempt to complete it.
            let mut future_slot = task.future.lock().unwrap();
            if let Some(mut future) = future_slot.take() {
                // Create a `LocalWaker` from the task itself
                let waker = waker_ref(&amp;task);
                let context = &amp;mut Context::from_waker(&amp;waker);
                // `BoxFuture&lt;T&gt;` is a type alias for
                // `Pin&lt;Box&lt;dyn Future&lt;Output = T&gt; + Send + 'static&gt;&gt;`.
                // We can get a `Pin&lt;&amp;mut dyn Future + Send + 'static&gt;`
                // from it by calling the `Pin::as_mut` method.
                if future.as_mut().poll(context).is_pending() {
                    // We're not done processing the future, so put it
                    // back in its task to be run again in the future.
                    *future_slot = Some(future);
                }
            }
        }
    }
}</code></pre>
<p>Congratulations! We now have a working futures executor. We can even use it
to run <code>async/.await</code> code and custom futures, such as the <code>TimerFuture</code> we
wrote earlier:</p>
<pre><code class="language-rust edition2018 ignore">fn main() {
    let (executor, spawner) = new_executor_and_spawner();

    // Spawn a task to print before and after waiting on a timer.
    spawner.spawn(async {
        println!("howdy!");
        // Wait for our timer future to complete after two seconds.
        TimerFuture::new(Duration::new(2, 0)).await;
        println!("done!");
    });

    // Drop the spawner so that our executor knows it is finished and won't
    // receive more incoming tasks to run.
    drop(spawner);

    // Run the executor until the task queue is empty.
    // This will print "howdy!", pause, and then print "done!".
    executor.run();
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="02_execution-05_io"></div><h1 id="02_execution-05_io-executors-and-system-io"><a class="header" href="#02_execution-05_io-executors-and-system-io">Executors and System IO</a></h1>
<p>In the previous section on <a href="#02_execution-02_future">The <code>Future</code> Trait</a>, we discussed this example of
a future that performed an asynchronous read on a socket:</p>
<pre><code class="language-rust ignore">pub struct SocketRead&lt;'a&gt; {
    socket: &amp;'a Socket,
}

impl SimpleFuture for SocketRead&lt;'_&gt; {
    type Output = Vec&lt;u8&gt;;

    fn poll(&amp;mut self, wake: fn()) -&gt; Poll&lt;Self::Output&gt; {
        if self.socket.has_data_to_read() {
            // The socket has data -- read it into a buffer and return it.
            Poll::Ready(self.socket.read_buf())
        } else {
            // The socket does not yet have data.
            //
            // Arrange for `wake` to be called once data is available.
            // When data becomes available, `wake` will be called, and the
            // user of this `Future` will know to call `poll` again and
            // receive data.
            self.socket.set_readable_callback(wake);
            Poll::Pending
        }
    }
}</code></pre>
<p>This future will read available data on a socket, and if no data is available,
it will yield to the executor, requesting that its task be awoken when the
socket becomes readable again. However, it's not clear from this example how
the <code>Socket</code> type is implemented, and in particular it isn't obvious how the
<code>set_readable_callback</code> function works. How can we arrange for <code>wake()</code>
to be called once the socket becomes readable? One option would be to have
a thread that continually checks whether <code>socket</code> is readable, calling
<code>wake()</code> when appropriate. However, this would be quite inefficient, requiring
a separate thread for each blocked IO future. This would greatly reduce the
efficiency of our async code.</p>
<p>In practice, this problem is solved through integration with an IO-aware
system blocking primitive, such as <code>epoll</code> on Linux, <code>kqueue</code> on FreeBSD and
Mac OS, IOCP on Windows, and <code>port</code>s on Fuchsia (all of which are exposed
through the cross-platform Rust crate <a href="https://github.com/tokio-rs/mio"><code>mio</code></a>). These primitives all allow
a thread to block on multiple asynchronous IO events, returning once one of
the events completes. In practice, these APIs usually look something like
this:</p>
<pre><code class="language-rust ignore">struct IoBlocker {
    /* ... */
}

struct Event {
    // An ID uniquely identifying the event that occurred and was listened for.
    id: usize,

    // A set of signals to wait for, or which occurred.
    signals: Signals,
}

impl IoBlocker {
    /// Create a new collection of asynchronous IO events to block on.
    fn new() -&gt; Self { /* ... */ }

    /// Express an interest in a particular IO event.
    fn add_io_event_interest(
        &amp;self,

        /// The object on which the event will occur
        io_object: &amp;IoObject,

        /// A set of signals that may appear on the `io_object` for
        /// which an event should be triggered, paired with
        /// an ID to give to events that result from this interest.
        event: Event,
    ) { /* ... */ }

    /// Block until one of the events occurs.
    fn block(&amp;self) -&gt; Event { /* ... */ }
}

let mut io_blocker = IoBlocker::new();
io_blocker.add_io_event_interest(
    &amp;socket_1,
    Event { id: 1, signals: READABLE },
);
io_blocker.add_io_event_interest(
    &amp;socket_2,
    Event { id: 2, signals: READABLE | WRITABLE },
);
let event = io_blocker.block();

// prints e.g. "Socket 1 is now READABLE" if socket one became readable.
println!("Socket {:?} is now {:?}", event.id, event.signals);</code></pre>
<p>Futures executors can use these primitives to provide asynchronous IO objects
such as sockets that can configure callbacks to be run when a particular IO
event occurs. In the case of our <code>SocketRead</code> example above, the
<code>Socket::set_readable_callback</code> function might look like the following pseudocode:</p>
<pre><code class="language-rust ignore">impl Socket {
    fn set_readable_callback(&amp;self, waker: Waker) {
        // `local_executor` is a reference to the local executor.
        // this could be provided at creation of the socket, but in practice
        // many executor implementations pass it down through thread local
        // storage for convenience.
        let local_executor = self.local_executor;

        // Unique ID for this IO object.
        let id = self.id;

        // Store the local waker in the executor's map so that it can be called
        // once the IO event arrives.
        local_executor.event_map.insert(id, waker);
        local_executor.add_io_event_interest(
            &amp;self.socket_file_descriptor,
            Event { id, signals: READABLE },
        );
    }
}</code></pre>
<p>We can now have just one executor thread which can receive and dispatch any
IO event to the appropriate <code>Waker</code>, which will wake up the corresponding
task, allowing the executor to drive more tasks to completion before returning
to check for more IO events (and the cycle continues...).</p>
<div style="break-before: page; page-break-before: always;"></div><div id="03_async_await-01_chapter"></div><h1 id="03_async_await-01_chapter-asyncawait"><a class="header" href="#03_async_await-01_chapter-asyncawait"><code>async</code>/<code>.await</code></a></h1>
<p>In <a href="#01_getting_started-04_async_await_primer">the first chapter</a>, we took a brief look at <code>async</code>/<code>.await</code>.
This chapter will discuss <code>async</code>/<code>.await</code> in
greater detail, explaining how it works and how <code>async</code> code differs from
traditional Rust programs.</p>
<p><code>async</code>/<code>.await</code> are special pieces of Rust syntax that make it possible to
yield control of the current thread rather than blocking, allowing other
code to make progress while waiting on an operation to complete.</p>
<p>There are two main ways to use <code>async</code>: <code>async fn</code> and <code>async</code> blocks.
Each returns a value that implements the <code>Future</code> trait:</p>
<pre><code class="language-rust edition2018 ignore">
// `foo()` returns a type that implements `Future&lt;Output = u8&gt;`.
// `foo().await` will result in a value of type `u8`.
async fn foo() -&gt; u8 { 5 }

fn bar() -&gt; impl Future&lt;Output = u8&gt; {
    // This `async` block results in a type that implements
    // `Future&lt;Output = u8&gt;`.
    async {
        let x: u8 = foo().await;
        x + 5
    }
}</code></pre>
<p>As we saw in the first chapter, <code>async</code> bodies and other futures are lazy:
they do nothing until they are run. The most common way to run a <code>Future</code>
is to <code>.await</code> it. When <code>.await</code> is called on a <code>Future</code>, it will attempt
to run it to completion. If the <code>Future</code> is blocked, it will yield control
of the current thread. When more progress can be made, the <code>Future</code> will be picked
up by the executor and will resume running, allowing the <code>.await</code> to resolve.</p>
<h2 id="03_async_await-01_chapter-async-lifetimes"><a class="header" href="#03_async_await-01_chapter-async-lifetimes"><code>async</code> Lifetimes</a></h2>
<p>Unlike traditional functions, <code>async fn</code>s which take references or other
non-<code>'static</code> arguments return a <code>Future</code> which is bounded by the lifetime of
the arguments:</p>
<pre><code class="language-rust edition2018 ignore">// This function:
async fn foo(x: &amp;u8) -&gt; u8 { *x }

// Is equivalent to this function:
fn foo_expanded&lt;'a&gt;(x: &amp;'a u8) -&gt; impl Future&lt;Output = u8&gt; + 'a {
    async move { *x }
}</code></pre>
<p>This means that the future returned from an <code>async fn</code> must be <code>.await</code>ed
while its non-<code>'static</code> arguments are still valid. In the common
case of <code>.await</code>ing the future immediately after calling the function
(as in <code>foo(&amp;x).await</code>) this is not an issue. However, if storing the future
or sending it over to another task or thread, this may be an issue.</p>
<p>One common workaround for turning an <code>async fn</code> with references-as-arguments
into a <code>'static</code> future is to bundle the arguments with the call to the
<code>async fn</code> inside an <code>async</code> block:</p>
<pre><code class="language-rust edition2018 ignore">fn bad() -&gt; impl Future&lt;Output = u8&gt; {
    let x = 5;
    borrow_x(&amp;x) // ERROR: `x` does not live long enough
}

fn good() -&gt; impl Future&lt;Output = u8&gt; {
    async {
        let x = 5;
        borrow_x(&amp;x).await
    }
}</code></pre>
<p>By moving the argument into the <code>async</code> block, we extend its lifetime to match
that of the <code>Future</code> returned from the call to <code>good</code>.</p>
<h2 id="03_async_await-01_chapter-async-move"><a class="header" href="#03_async_await-01_chapter-async-move"><code>async move</code></a></h2>
<p><code>async</code> blocks and closures allow the <code>move</code> keyword, much like normal
closures. An <code>async move</code> block will take ownership of the variables it
references, allowing it to outlive the current scope, but giving up the ability
to share those variables with other code:</p>
<pre><code class="language-rust edition2018 ignore">/// `async` block:
///
/// Multiple different `async` blocks can access the same local variable
/// so long as they're executed within the variable's scope
async fn blocks() {
    let my_string = "foo".to_string();

    let future_one = async {
        // ...
        println!("{my_string}");
    };

    let future_two = async {
        // ...
        println!("{my_string}");
    };

    // Run both futures to completion, printing "foo" twice:
    let ((), ()) = futures::join!(future_one, future_two);
}

/// `async move` block:
///
/// Only one `async move` block can access the same captured variable, since
/// captures are moved into the `Future` generated by the `async move` block.
/// However, this allows the `Future` to outlive the original scope of the
/// variable:
fn move_block() -&gt; impl Future&lt;Output = ()&gt; {
    let my_string = "foo".to_string();
    async move {
        // ...
        println!("{my_string}");
    }
}</code></pre>
<h2 id="03_async_await-01_chapter-awaiting-on-a-multithreaded-executor"><a class="header" href="#03_async_await-01_chapter-awaiting-on-a-multithreaded-executor"><code>.await</code>ing on a Multithreaded Executor</a></h2>
<p>Note that, when using a multithreaded <code>Future</code> executor, a <code>Future</code> may move
between threads, so any variables used in <code>async</code> bodies must be able to travel
between threads, as any <code>.await</code> can potentially result in a switch to a new
thread.</p>
<p>This means that it is not safe to use <code>Rc</code>, <code>&amp;RefCell</code> or any other types
that don't implement the <code>Send</code> trait, including references to types that don't
implement the <code>Sync</code> trait.</p>
<p>(Caveat: it is possible to use these types as long as they aren't in scope
during a call to <code>.await</code>.)</p>
<p>Similarly, it isn't a good idea to hold a traditional non-futures-aware lock
across an <code>.await</code>, as it can cause the threadpool to lock up: one task could
take out a lock, <code>.await</code> and yield to the executor, allowing another task to
attempt to take the lock and cause a deadlock. To avoid this, use the <code>Mutex</code>
in <code>futures::lock</code> rather than the one from <code>std::sync</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="04_pinning-01_chapter"></div><h1 id="04_pinning-01_chapter-pinning"><a class="header" href="#04_pinning-01_chapter-pinning">Pinning</a></h1>
<p>To poll futures, they must be pinned using a special type called
<code>Pin&lt;T&gt;</code>. If you read the explanation of <a href="#02_execution-02_future">the <code>Future</code> trait</a> in the
previous section <a href="#02_execution-01_chapter">"Executing <code>Future</code>s and Tasks"</a>, you'll recognize
<code>Pin</code> from the <code>self: Pin&lt;&amp;mut Self&gt;</code> in the <code>Future::poll</code> method's definition.
But what does it mean, and why do we need it?</p>
<h2 id="04_pinning-01_chapter-why-pinning"><a class="header" href="#04_pinning-01_chapter-why-pinning">Why Pinning</a></h2>
<p><code>Pin</code> works in tandem with the <code>Unpin</code> marker. Pinning makes it possible
to guarantee that an object implementing <code>!Unpin</code> won't ever be moved. To understand
why this is necessary, we need to remember how <code>async</code>/<code>.await</code> works. Consider
the following code:</p>
<pre><code class="language-rust edition2018 ignore">let fut_one = /* ... */;
let fut_two = /* ... */;
async move {
    fut_one.await;
    fut_two.await;
}</code></pre>
<p>Under the hood, this creates an anonymous type that implements <code>Future</code>,
providing a <code>poll</code> method that looks something like this:</p>
<pre><code class="language-rust ignore">// The `Future` type generated by our `async { ... }` block
struct AsyncFuture {
    fut_one: FutOne,
    fut_two: FutTwo,
    state: State,
}

// List of states our `async` block can be in
enum State {
    AwaitingFutOne,
    AwaitingFutTwo,
    Done,
}

impl Future for AsyncFuture {
    type Output = ();

    fn poll(mut self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;) -&gt; Poll&lt;()&gt; {
        loop {
            match self.state {
                State::AwaitingFutOne =&gt; match self.fut_one.poll(..) {
                    Poll::Ready(()) =&gt; self.state = State::AwaitingFutTwo,
                    Poll::Pending =&gt; return Poll::Pending,
                }
                State::AwaitingFutTwo =&gt; match self.fut_two.poll(..) {
                    Poll::Ready(()) =&gt; self.state = State::Done,
                    Poll::Pending =&gt; return Poll::Pending,
                }
                State::Done =&gt; return Poll::Ready(()),
            }
        }
    }
}</code></pre>
<p>When <code>poll</code> is first called, it will poll <code>fut_one</code>. If <code>fut_one</code> can't
complete, <code>AsyncFuture::poll</code> will return. Future calls to <code>poll</code> will pick
up where the previous one left off. This process continues until the future
is able to successfully complete.</p>
<p>However, what happens if we have an <code>async</code> block that uses references?
For example:</p>
<pre><code class="language-rust edition2018 ignore">async {
    let mut x = [0; 128];
    let read_into_buf_fut = read_into_buf(&amp;mut x);
    read_into_buf_fut.await;
    println!("{:?}", x);
}</code></pre>
<p>What struct does this compile down to?</p>
<pre><code class="language-rust ignore">struct ReadIntoBuf&lt;'a&gt; {
    buf: &amp;'a mut [u8], // points to `x` below
}

struct AsyncFuture {
    x: [u8; 128],
    read_into_buf_fut: ReadIntoBuf&lt;'what_lifetime?&gt;,
}</code></pre>
<p>Here, the <code>ReadIntoBuf</code> future holds a reference into the other field of our
structure, <code>x</code>. However, if <code>AsyncFuture</code> is moved, the location of <code>x</code> will
move as well, invalidating the pointer stored in <code>read_into_buf_fut.buf</code>.</p>
<p>Pinning futures to a particular spot in memory prevents this problem, making
it safe to create references to values inside an <code>async</code> block.</p>
<h2 id="04_pinning-01_chapter-pinning-in-detail"><a class="header" href="#04_pinning-01_chapter-pinning-in-detail">Pinning in Detail</a></h2>
<p>Let's try to understand pinning by using an slightly simpler example. The problem we encounter
above is a problem that ultimately boils down to how we handle references in self-referential
types in Rust.</p>
<p>For now our example will look like this:</p>
<pre><code class="language-rust  ignore">#[derive(Debug)]
struct Test {
    a: String,
    b: *const String,
}

impl Test {
    fn new(txt: &amp;str) -&gt; Self {
        Test {
            a: String::from(txt),
            b: std::ptr::null(),
        }
    }

    fn init(&amp;mut self) {
        let self_ref: *const String = &amp;self.a;
        self.b = self_ref;
    }

    fn a(&amp;self) -&gt; &amp;str {
        &amp;self.a
    }

    fn b(&amp;self) -&gt; &amp;String {
        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
        unsafe { &amp;*(self.b) }
    }
}</code></pre>
<p><code>Test</code> provides methods to get a reference to the value of the fields <code>a</code> and <code>b</code>. Since <code>b</code> is a
reference to <code>a</code> we store it as a pointer since the borrowing rules of Rust doesn't allow us to
define this lifetime. We now have what we call a self-referential struct.</p>
<p>Our example works fine if we don't move any of our data around as you can observe by running
this example:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let mut test1 = Test::new("test1");
    test1.init();
    let mut test2 = Test::new("test2");
    test2.init();

    println!("a: {}, b: {}", test1.a(), test1.b());
    println!("a: {}, b: {}", test2.a(), test2.b());

}
<span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    // We need an `init` method to actually set our self-reference
</span><span class="boring">    fn init(&amp;mut self) {
</span><span class="boring">        let self_ref: *const String = &amp;self.a;
</span><span class="boring">        self.b = self_ref;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(&amp;self) -&gt; &amp;str {
</span><span class="boring">        &amp;self.a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(&amp;self) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>We get what we'd expect:</p>
<pre><code class="language-rust  ignore">a: test1, b: test1
a: test2, b: test2</code></pre>
<p>Let's see what happens if we swap <code>test1</code> with <code>test2</code> and thereby move the data:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let mut test1 = Test::new("test1");
    test1.init();
    let mut test2 = Test::new("test2");
    test2.init();

    println!("a: {}, b: {}", test1.a(), test1.b());
    std::mem::swap(&amp;mut test1, &amp;mut test2);
    println!("a: {}, b: {}", test2.a(), test2.b());

}
<span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(&amp;mut self) {
</span><span class="boring">        let self_ref: *const String = &amp;self.a;
</span><span class="boring">        self.b = self_ref;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(&amp;self) -&gt; &amp;str {
</span><span class="boring">        &amp;self.a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(&amp;self) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>Naively, we could think that what we should get a debug print of <code>test1</code> two times like this:</p>
<pre><code class="language-rust  ignore">a: test1, b: test1
a: test1, b: test1</code></pre>
<p>But instead we get:</p>
<pre><code class="language-rust  ignore">a: test1, b: test1
a: test1, b: test2</code></pre>
<p>The pointer to <code>test2.b</code> still points to the old location which is inside <code>test1</code>
now. The struct is not self-referential anymore, it holds a pointer to a field
in a different object. That means we can't rely on the lifetime of <code>test2.b</code> to
be tied to the lifetime of <code>test2</code> anymore.</p>
<p>If you're still not convinced, this should at least convince you:</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
    let mut test1 = Test::new("test1");
    test1.init();
    let mut test2 = Test::new("test2");
    test2.init();

    println!("a: {}, b: {}", test1.a(), test1.b());
    std::mem::swap(&amp;mut test1, &amp;mut test2);
    test1.a = "I've totally changed now!".to_string();
    println!("a: {}, b: {}", test2.a(), test2.b());

}
<span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(&amp;mut self) {
</span><span class="boring">        let self_ref: *const String = &amp;self.a;
</span><span class="boring">        self.b = self_ref;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(&amp;self) -&gt; &amp;str {
</span><span class="boring">        &amp;self.a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(&amp;self) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>The diagram below can help visualize what's going on:</p>
<p><strong>Fig 1: Before and after swap</strong>
<img src="04_pinning/../assets/swap_problem.jpg" alt="swap_problem" /></p>
<p>It's easy to get this to show undefined behavior and fail in other spectacular ways as well.</p>
<h2 id="04_pinning-01_chapter-pinning-in-practice"><a class="header" href="#04_pinning-01_chapter-pinning-in-practice">Pinning in Practice</a></h2>
<p>Let's see how pinning and the <code>Pin</code> type can help us solve this problem.</p>
<p>The <code>Pin</code> type wraps pointer types, guaranteeing that the values behind the
pointer won't be moved if it is not implementing <code>Unpin</code>. For example, <code>Pin&lt;&amp;mut T&gt;</code>, <code>Pin&lt;&amp;T&gt;</code>, <code>Pin&lt;Box&lt;T&gt;&gt;</code> all guarantee that <code>T</code> won't be moved if <code>T: !Unpin</code>.</p>
<p>Most types don't have a problem being moved. These types implement a trait
called <code>Unpin</code>. Pointers to <code>Unpin</code> types can be freely placed into or taken
out of <code>Pin</code>. For example, <code>u8</code> is <code>Unpin</code>, so <code>Pin&lt;&amp;mut u8&gt;</code> behaves just like
a normal <code>&amp;mut u8</code>.</p>
<p>However, types that can't be moved after they're pinned have a marker called
<code>!Unpin</code>. Futures created by async/await is an example of this.</p>
<h3 id="04_pinning-01_chapter-pinning-to-the-stack"><a class="header" href="#04_pinning-01_chapter-pinning-to-the-stack">Pinning to the Stack</a></h3>
<p>Back to our example. We can solve our problem by using <code>Pin</code>. Let's take a look at what
our example would look like if we required a pinned pointer instead:</p>
<pre><code class="language-rust  ignore">use std::pin::Pin;
use std::marker::PhantomPinned;

#[derive(Debug)]
struct Test {
    a: String,
    b: *const String,
    _marker: PhantomPinned,
}


impl Test {
    fn new(txt: &amp;str) -&gt; Self {
        Test {
            a: String::from(txt),
            b: std::ptr::null(),
            _marker: PhantomPinned, // This makes our type `!Unpin`
        }
    }

    fn init(self: Pin&lt;&amp;mut Self&gt;) {
        let self_ptr: *const String = &amp;self.a;
        let this = unsafe { self.get_unchecked_mut() };
        this.b = self_ptr;
    }

    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
        &amp;self.get_ref().a
    }

    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
        unsafe { &amp;*(self.b) }
    }
}</code></pre>
<p>Pinning an object to the stack will always be <code>unsafe</code> if our type implements
<code>!Unpin</code>. You can use a crate like <a href="https://docs.rs/pin-utils/"><code>pin_utils</code></a> to avoid writing
our own <code>unsafe</code> code when pinning to the stack.</p>
<p>Below, we pin the objects <code>test1</code> and <code>test2</code> to the stack:</p>
<pre><pre class="playground"><code class="language-rust">pub fn main() {
    // test1 is safe to move before we initialize it
    let mut test1 = Test::new("test1");
    // Notice how we shadow `test1` to prevent it from being accessed again
    let mut test1 = unsafe { Pin::new_unchecked(&amp;mut test1) };
    Test::init(test1.as_mut());

    let mut test2 = Test::new("test2");
    let mut test2 = unsafe { Pin::new_unchecked(&amp;mut test2) };
    Test::init(test2.as_mut());

    println!("a: {}, b: {}", Test::a(test1.as_ref()), Test::b(test1.as_ref()));
    println!("a: {}, b: {}", Test::a(test2.as_ref()), Test::b(test2.as_ref()));
}
<span class="boring">use std::pin::Pin;
</span><span class="boring">use std::marker::PhantomPinned;
</span><span class="boring">
</span><span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">    _marker: PhantomPinned,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">            // This makes our type `!Unpin`
</span><span class="boring">            _marker: PhantomPinned,
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(self: Pin&lt;&amp;mut Self&gt;) {
</span><span class="boring">        let self_ptr: *const String = &amp;self.a;
</span><span class="boring">        let this = unsafe { self.get_unchecked_mut() };
</span><span class="boring">        this.b = self_ptr;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
</span><span class="boring">        &amp;self.get_ref().a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>Now, if we try to move our data now we get a compilation error:</p>
<pre><pre class="playground"><code class="language-rust  compile_fail">pub fn main() {
    let mut test1 = Test::new("test1");
    let mut test1 = unsafe { Pin::new_unchecked(&amp;mut test1) };
    Test::init(test1.as_mut());

    let mut test2 = Test::new("test2");
    let mut test2 = unsafe { Pin::new_unchecked(&amp;mut test2) };
    Test::init(test2.as_mut());

    println!("a: {}, b: {}", Test::a(test1.as_ref()), Test::b(test1.as_ref()));
    std::mem::swap(test1.get_mut(), test2.get_mut());
    println!("a: {}, b: {}", Test::a(test2.as_ref()), Test::b(test2.as_ref()));
}
<span class="boring">use std::pin::Pin;
</span><span class="boring">use std::marker::PhantomPinned;
</span><span class="boring">
</span><span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">    _marker: PhantomPinned,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">            _marker: PhantomPinned, // This makes our type `!Unpin`
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init(self: Pin&lt;&amp;mut Self&gt;) {
</span><span class="boring">        let self_ptr: *const String = &amp;self.a;
</span><span class="boring">        let this = unsafe { self.get_unchecked_mut() };
</span><span class="boring">        this.b = self_ptr;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
</span><span class="boring">        &amp;self.get_ref().a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
<p>The type system prevents us from moving the data, as follows:</p>
<pre><code>error[E0277]: `PhantomPinned` cannot be unpinned
   --&gt; src\test.rs:56:30
    |
56  |         std::mem::swap(test1.get_mut(), test2.get_mut());
    |                              ^^^^^^^ within `test1::Test`, the trait `Unpin` is not implemented for `PhantomPinned`
    |
    = note: consider using `Box::pin`
note: required because it appears within the type `test1::Test`
   --&gt; src\test.rs:7:8
    |
7   | struct Test {
    |        ^^^^
note: required by a bound in `std::pin::Pin::&lt;&amp;'a mut T&gt;::get_mut`
   --&gt; &lt;...&gt;rustlib/src/rust\library\core\src\pin.rs:748:12
    |
748 |         T: Unpin,
    |            ^^^^^ required by this bound in `std::pin::Pin::&lt;&amp;'a mut T&gt;::get_mut`
</code></pre>
<blockquote>
<p>It's important to note that stack pinning will always rely on guarantees
you give when writing <code>unsafe</code>. While we know that the <em>pointee</em> of <code>&amp;'a mut T</code>
is pinned for the lifetime of <code>'a</code> we can't know if the data <code>&amp;'a mut T</code>
points to isn't moved after <code>'a</code> ends. If it does it will violate the Pin
contract.</p>
<p>A mistake that is easy to make is forgetting to shadow the original variable
since you could drop the <code>Pin</code> and move the data after <code>&amp;'a mut T</code>
like shown below (which violates the Pin contract):</p>
<pre><pre class="playground"><code class="language-rust">fn main() {
   let mut test1 = Test::new("test1");
   let mut test1_pin = unsafe { Pin::new_unchecked(&amp;mut test1) };
   Test::init(test1_pin.as_mut());

   drop(test1_pin);
   println!(r#"test1.b points to "test1": {:?}..."#, test1.b);

   let mut test2 = Test::new("test2");
   mem::swap(&amp;mut test1, &amp;mut test2);
   println!("... and now it points nowhere: {:?}", test1.b);
}
<span class="boring">use std::pin::Pin;
</span><span class="boring">use std::marker::PhantomPinned;
</span><span class="boring">use std::mem;
</span><span class="boring">
</span><span class="boring">#[derive(Debug)]
</span><span class="boring">struct Test {
</span><span class="boring">    a: String,
</span><span class="boring">    b: *const String,
</span><span class="boring">    _marker: PhantomPinned,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">
</span><span class="boring">impl Test {
</span><span class="boring">    fn new(txt: &amp;str) -&gt; Self {
</span><span class="boring">        Test {
</span><span class="boring">            a: String::from(txt),
</span><span class="boring">            b: std::ptr::null(),
</span><span class="boring">            // This makes our type `!Unpin`
</span><span class="boring">            _marker: PhantomPinned,
</span><span class="boring">        }
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn init&lt;'a&gt;(self: Pin&lt;&amp;'a mut Self&gt;) {
</span><span class="boring">        let self_ptr: *const String = &amp;self.a;
</span><span class="boring">        let this = unsafe { self.get_unchecked_mut() };
</span><span class="boring">        this.b = self_ptr;
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn a&lt;'a&gt;(self: Pin&lt;&amp;'a Self&gt;) -&gt; &amp;'a str {
</span><span class="boring">        &amp;self.get_ref().a
</span><span class="boring">    }
</span><span class="boring">
</span><span class="boring">    fn b&lt;'a&gt;(self: Pin&lt;&amp;'a Self&gt;) -&gt; &amp;'a String {
</span><span class="boring">        assert!(!self.b.is_null(), "Test::b called without Test::init being called first");
</span><span class="boring">        unsafe { &amp;*(self.b) }
</span><span class="boring">    }
</span><span class="boring">}</span></code></pre></pre>
</blockquote>
<h3 id="04_pinning-01_chapter-pinning-to-the-heap"><a class="header" href="#04_pinning-01_chapter-pinning-to-the-heap">Pinning to the Heap</a></h3>
<p>Pinning an <code>!Unpin</code> type to the heap gives our data a stable address so we know
that the data we point to can't move after it's pinned. In contrast to stack
pinning, we know that the data will be pinned for the lifetime of the object.</p>
<pre><pre class="playground"><code class="language-rust  edition2018">use std::pin::Pin;
use std::marker::PhantomPinned;

#[derive(Debug)]
struct Test {
    a: String,
    b: *const String,
    _marker: PhantomPinned,
}

impl Test {
    fn new(txt: &amp;str) -&gt; Pin&lt;Box&lt;Self&gt;&gt; {
        let t = Test {
            a: String::from(txt),
            b: std::ptr::null(),
            _marker: PhantomPinned,
        };
        let mut boxed = Box::pin(t);
        let self_ptr: *const String = &amp;boxed.a;
        unsafe { boxed.as_mut().get_unchecked_mut().b = self_ptr };

        boxed
    }

    fn a(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;str {
        &amp;self.get_ref().a
    }

    fn b(self: Pin&lt;&amp;Self&gt;) -&gt; &amp;String {
        unsafe { &amp;*(self.b) }
    }
}

pub fn main() {
    let test1 = Test::new("test1");
    let test2 = Test::new("test2");

    println!("a: {}, b: {}",test1.as_ref().a(), test1.as_ref().b());
    println!("a: {}, b: {}",test2.as_ref().a(), test2.as_ref().b());
}</code></pre></pre>
<p>Some functions require the futures they work with to be <code>Unpin</code>. To use a
<code>Future</code> or <code>Stream</code> that isn't <code>Unpin</code> with a function that requires
<code>Unpin</code> types, you'll first have to pin the value using either
<code>Box::pin</code> (to create a <code>Pin&lt;Box&lt;T&gt;&gt;</code>) or the <code>pin_utils::pin_mut!</code> macro
(to create a <code>Pin&lt;&amp;mut T&gt;</code>). <code>Pin&lt;Box&lt;Fut&gt;&gt;</code> and <code>Pin&lt;&amp;mut Fut&gt;</code> can both be
used as futures, and both implement <code>Unpin</code>.</p>
<p>For example:</p>
<pre><code class="language-rust edition2018 ignore">use pin_utils::pin_mut; // `pin_utils` is a handy crate available on crates.io

// A function which takes a `Future` that implements `Unpin`.
fn execute_unpin_future(x: impl Future&lt;Output = ()&gt; + Unpin) { /* ... */ }

let fut = async { /* ... */ };
execute_unpin_future(fut); // Error: `fut` does not implement `Unpin` trait

// Pinning with `Box`:
let fut = async { /* ... */ };
let fut = Box::pin(fut);
execute_unpin_future(fut); // OK

// Pinning with `pin_mut!`:
let fut = async { /* ... */ };
pin_mut!(fut);
execute_unpin_future(fut); // OK</code></pre>
<h2 id="04_pinning-01_chapter-summary"><a class="header" href="#04_pinning-01_chapter-summary">Summary</a></h2>
<ol>
<li>
<p>If <code>T: Unpin</code> (which is the default), then <code>Pin&lt;'a, T&gt;</code> is entirely
equivalent to <code>&amp;'a mut T</code>. In other words: <code>Unpin</code> means it's OK for this type
to be moved even when pinned, so <code>Pin</code> will have no effect on such a type.</p>
</li>
<li>
<p>Getting a <code>&amp;mut T</code> to a pinned T requires unsafe if <code>T: !Unpin</code>.</p>
</li>
<li>
<p>Most standard library types implement <code>Unpin</code>. The same goes for most
"normal" types you encounter in Rust. A <code>Future</code> generated by async/await is an exception to this rule.</p>
</li>
<li>
<p>You can add a <code>!Unpin</code> bound on a type on nightly with a feature flag, or
by adding <code>std::marker::PhantomPinned</code> to your type on stable.</p>
</li>
<li>
<p>You can either pin data to the stack or to the heap.</p>
</li>
<li>
<p>Pinning a <code>!Unpin</code> object to the stack requires <code>unsafe</code></p>
</li>
<li>
<p>Pinning a <code>!Unpin</code> object to the heap does not require <code>unsafe</code>. There is a shortcut for doing this using <code>Box::pin</code>.</p>
</li>
<li>
<p>For pinned data where <code>T: !Unpin</code> you have to maintain the invariant that its memory will not
get invalidated or repurposed <em>from the moment it gets pinned until when drop</em> is called. This is
an important part of the <em>pin contract</em>.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><div id="05_streams-01_chapter"></div><h1 id="05_streams-01_chapter-the-stream-trait"><a class="header" href="#05_streams-01_chapter-the-stream-trait">The <code>Stream</code> Trait</a></h1>
<p>The <code>Stream</code> trait is similar to <code>Future</code> but can yield multiple values before
completing, similar to the <code>Iterator</code> trait from the standard library:</p>
<pre><code class="language-rust ignore">trait Stream {
    /// The type of the value yielded by the stream.
    type Item;

    /// Attempt to resolve the next item in the stream.
    /// Returns `Poll::Pending` if not ready, `Poll::Ready(Some(x))` if a value
    /// is ready, and `Poll::Ready(None)` if the stream has completed.
    fn poll_next(self: Pin&lt;&amp;mut Self&gt;, cx: &amp;mut Context&lt;'_&gt;)
        -&gt; Poll&lt;Option&lt;Self::Item&gt;&gt;;
}</code></pre>
<p>One common example of a <code>Stream</code> is the <code>Receiver</code> for the channel type from
the <code>futures</code> crate. It will yield <code>Some(val)</code> every time a value is sent
from the <code>Sender</code> end, and will yield <code>None</code> once the <code>Sender</code> has been
dropped and all pending messages have been received:</p>
<pre><code class="language-rust edition2018 ignore">async fn send_recv() {
    const BUFFER_SIZE: usize = 10;
    let (mut tx, mut rx) = mpsc::channel::&lt;i32&gt;(BUFFER_SIZE);

    tx.send(1).await.unwrap();
    tx.send(2).await.unwrap();
    drop(tx);

    // `StreamExt::next` is similar to `Iterator::next`, but returns a
    // type that implements `Future&lt;Output = Option&lt;T&gt;&gt;`.
    assert_eq!(Some(1), rx.next().await);
    assert_eq!(Some(2), rx.next().await);
    assert_eq!(None, rx.next().await);
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="05_streams-02_iteration_and_concurrency"></div><h1 id="05_streams-02_iteration_and_concurrency-iteration-and-concurrency"><a class="header" href="#05_streams-02_iteration_and_concurrency-iteration-and-concurrency">Iteration and Concurrency</a></h1>
<p>Similar to synchronous <code>Iterator</code>s, there are many different ways to iterate
over and process the values in a <code>Stream</code>. There are combinator-style methods
such as <code>map</code>, <code>filter</code>, and <code>fold</code>, and their early-exit-on-error cousins
<code>try_map</code>, <code>try_filter</code>, and <code>try_fold</code>.</p>
<p>Unfortunately, <code>for</code> loops are not usable with <code>Stream</code>s, but for
imperative-style code, <code>while let</code> and the <code>next</code>/<code>try_next</code> functions can
be used:</p>
<pre><code class="language-rust edition2018 ignore">async fn sum_with_next(mut stream: Pin&lt;&amp;mut dyn Stream&lt;Item = i32&gt;&gt;) -&gt; i32 {
    use futures::stream::StreamExt; // for `next`
    let mut sum = 0;
    while let Some(item) = stream.next().await {
        sum += item;
    }
    sum
}

async fn sum_with_try_next(
    mut stream: Pin&lt;&amp;mut dyn Stream&lt;Item = Result&lt;i32, io::Error&gt;&gt;&gt;,
) -&gt; Result&lt;i32, io::Error&gt; {
    use futures::stream::TryStreamExt; // for `try_next`
    let mut sum = 0;
    while let Some(item) = stream.try_next().await? {
        sum += item;
    }
    Ok(sum)
}</code></pre>
<p>However, if we're just processing one element at a time, we're potentially
leaving behind opportunity for concurrency, which is, after all, why we're
writing async code in the first place. To process multiple items from a stream
concurrently, use the <code>for_each_concurrent</code> and <code>try_for_each_concurrent</code>
methods:</p>
<pre><code class="language-rust edition2018 ignore">async fn jump_around(
    mut stream: Pin&lt;&amp;mut dyn Stream&lt;Item = Result&lt;u8, io::Error&gt;&gt;&gt;,
) -&gt; Result&lt;(), io::Error&gt; {
    use futures::stream::TryStreamExt; // for `try_for_each_concurrent`
    const MAX_CONCURRENT_JUMPERS: usize = 100;

    stream.try_for_each_concurrent(MAX_CONCURRENT_JUMPERS, |num| async move {
        jump_n_times(num).await?;
        report_n_jumps(num).await?;
        Ok(())
    }).await?;

    Ok(())
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="06_multiple_futures-01_chapter"></div><h1 id="06_multiple_futures-01_chapter-executing-multiple-futures-at-a-time"><a class="header" href="#06_multiple_futures-01_chapter-executing-multiple-futures-at-a-time">Executing Multiple Futures at a Time</a></h1>
<p>Up until now, we've mostly executed futures by using <code>.await</code>, which blocks
the current task until a particular <code>Future</code> completes. However, real
asynchronous applications often need to execute several different
operations concurrently.</p>
<p>In this chapter, we'll cover some ways to execute multiple asynchronous
operations at the same time:</p>
<ul>
<li><code>join!</code>: waits for futures to all complete</li>
<li><code>select!</code>: waits for one of several futures to complete</li>
<li>Spawning: creates a top-level task which ambiently runs a future to completion</li>
<li><code>FuturesUnordered</code>: a group of futures which yields the result of each subfuture</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div id="06_multiple_futures-02_join"></div><h1 id="06_multiple_futures-02_join-join"><a class="header" href="#06_multiple_futures-02_join-join"><code>join!</code></a></h1>
<p>The <code>futures::join</code> macro makes it possible to wait for multiple different
futures to complete while executing them all concurrently.</p>
<h1 id="06_multiple_futures-02_join-join-1"><a class="header" href="#06_multiple_futures-02_join-join-1"><code>join!</code></a></h1>
<p>When performing multiple asynchronous operations, it's tempting to simply
<code>.await</code> them in a series:</p>
<pre><code class="language-rust edition2018 ignore">async fn get_book_and_music() -&gt; (Book, Music) {
    let book = get_book().await;
    let music = get_music().await;
    (book, music)
}</code></pre>
<p>However, this will be slower than necessary, since it won't start trying to
<code>get_music</code> until after <code>get_book</code> has completed. In some other languages,
futures are ambiently run to completion, so two operations can be
run concurrently by first calling each <code>async fn</code> to start the futures, and
then awaiting them both:</p>
<pre><code class="language-rust edition2018 ignore">// WRONG -- don't do this
async fn get_book_and_music() -&gt; (Book, Music) {
    let book_future = get_book();
    let music_future = get_music();
    (book_future.await, music_future.await)
}</code></pre>
<p>However, Rust futures won't do any work until they're actively <code>.await</code>ed.
This means that the two code snippets above will both run
<code>book_future</code> and <code>music_future</code> in series rather than running them
concurrently. To correctly run the two futures concurrently, use
<code>futures::join!</code>:</p>
<pre><code class="language-rust edition2018 ignore">use futures::join;

async fn get_book_and_music() -&gt; (Book, Music) {
    let book_fut = get_book();
    let music_fut = get_music();
    join!(book_fut, music_fut)
}</code></pre>
<p>The value returned by <code>join!</code> is a tuple containing the output of each
<code>Future</code> passed in.</p>
<h2 id="06_multiple_futures-02_join-try_join"><a class="header" href="#06_multiple_futures-02_join-try_join"><code>try_join!</code></a></h2>
<p>For futures which return <code>Result</code>, consider using <code>try_join!</code> rather than
<code>join!</code>. Since <code>join!</code> only completes once all subfutures have completed,
it'll continue processing other futures even after one of its subfutures
has returned an <code>Err</code>.</p>
<p>Unlike <code>join!</code>, <code>try_join!</code> will complete immediately if one of the subfutures
returns an error.</p>
<pre><code class="language-rust edition2018 ignore">use futures::try_join;

async fn get_book() -&gt; Result&lt;Book, String&gt; { /* ... */ Ok(Book) }
async fn get_music() -&gt; Result&lt;Music, String&gt; { /* ... */ Ok(Music) }

async fn get_book_and_music() -&gt; Result&lt;(Book, Music), String&gt; {
    let book_fut = get_book();
    let music_fut = get_music();
    try_join!(book_fut, music_fut)
}</code></pre>
<p>Note that the futures passed to <code>try_join!</code> must all have the same error type.
Consider using the <code>.map_err(|e| ...)</code> and <code>.err_into()</code> functions from
<code>futures::future::TryFutureExt</code> to consolidate the error types:</p>
<pre><code class="language-rust edition2018 ignore">use futures::{
    future::TryFutureExt,
    try_join,
};

async fn get_book() -&gt; Result&lt;Book, ()&gt; { /* ... */ Ok(Book) }
async fn get_music() -&gt; Result&lt;Music, String&gt; { /* ... */ Ok(Music) }

async fn get_book_and_music() -&gt; Result&lt;(Book, Music), String&gt; {
    let book_fut = get_book().map_err(|()| "Unable to get book".to_string());
    let music_fut = get_music();
    try_join!(book_fut, music_fut)
}</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="06_multiple_futures-03_select"></div><h1 id="06_multiple_futures-03_select-select"><a class="header" href="#06_multiple_futures-03_select-select"><code>select!</code></a></h1>
<p>The <code>futures::select</code> macro runs multiple futures simultaneously, allowing
the user to respond as soon as any future completes.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    future::FutureExt, // for `.fuse()`
    pin_mut,
    select,
};

async fn task_one() { /* ... */ }
async fn task_two() { /* ... */ }

async fn race_tasks() {
    let t1 = task_one().fuse();
    let t2 = task_two().fuse();

    pin_mut!(t1, t2);

    select! {
        () = t1 =&gt; println!("task one completed first"),
        () = t2 =&gt; println!("task two completed first"),
    }
}
<span class="boring">}</span></code></pre></pre>
<p>The function above will run both <code>t1</code> and <code>t2</code> concurrently. When either
<code>t1</code> or <code>t2</code> finishes, the corresponding handler will call <code>println!</code>, and
the function will end without completing the remaining task.</p>
<p>The basic syntax for <code>select</code> is <code>&lt;pattern&gt; = &lt;expression&gt; =&gt; &lt;code&gt;,</code>,
repeated for as many futures as you would like to <code>select</code> over.</p>
<h2 id="06_multiple_futures-03_select-default---and-complete--"><a class="header" href="#06_multiple_futures-03_select-default---and-complete--"><code>default =&gt; ...</code> and <code>complete =&gt; ...</code></a></h2>
<p><code>select</code> also supports <code>default</code> and <code>complete</code> branches.</p>
<p>A <code>default</code> branch will run if none of the futures being <code>select</code>ed
over are yet complete. A <code>select</code> with a <code>default</code> branch will
therefore always return immediately, since <code>default</code> will be run
if none of the other futures are ready.</p>
<p><code>complete</code> branches can be used to handle the case where all futures
being <code>select</code>ed over have completed and will no longer make progress.
This is often handy when looping over a <code>select!</code>.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{future, select};

async fn count() {
    let mut a_fut = future::ready(4);
    let mut b_fut = future::ready(6);
    let mut total = 0;

    loop {
        select! {
            a = a_fut =&gt; total += a,
            b = b_fut =&gt; total += b,
            complete =&gt; break,
            default =&gt; unreachable!(), // never runs (futures are ready, then complete)
        };
    }
    assert_eq!(total, 10);
}
<span class="boring">}</span></code></pre></pre>
<h2 id="06_multiple_futures-03_select-interaction-with-unpin-and-fusedfuture"><a class="header" href="#06_multiple_futures-03_select-interaction-with-unpin-and-fusedfuture">Interaction with <code>Unpin</code> and <code>FusedFuture</code></a></h2>
<p>One thing you may have noticed in the first example above is that we
had to call <code>.fuse()</code> on the futures returned by the two <code>async fn</code>s,
as well as pinning them with <code>pin_mut</code>. Both of these calls are necessary
because the futures used in <code>select</code> must implement both the <code>Unpin</code>
trait and the <code>FusedFuture</code> trait.</p>
<p><code>Unpin</code> is necessary because the futures used by <code>select</code> are not
taken by value, but by mutable reference. By not taking ownership
of the future, uncompleted futures can be used again after the
call to <code>select</code>.</p>
<p>Similarly, the <code>FusedFuture</code> trait is required because <code>select</code> must
not poll a future after it has completed. <code>FusedFuture</code> is implemented
by futures which track whether or not they have completed. This makes
it possible to use <code>select</code> in a loop, only polling the futures which
still have yet to complete. This can be seen in the example above,
where <code>a_fut</code> or <code>b_fut</code> will have completed the second time through
the loop. Because the future returned by <code>future::ready</code> implements
<code>FusedFuture</code>, it's able to tell <code>select</code> not to poll it again.</p>
<p>Note that streams have a corresponding <code>FusedStream</code> trait. Streams
which implement this trait or have been wrapped using <code>.fuse()</code>
will yield <code>FusedFuture</code> futures from their
<code>.next()</code> / <code>.try_next()</code> combinators.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    stream::{Stream, StreamExt, FusedStream},
    select,
};

async fn add_two_streams(
    mut s1: impl Stream&lt;Item = u8&gt; + FusedStream + Unpin,
    mut s2: impl Stream&lt;Item = u8&gt; + FusedStream + Unpin,
) -&gt; u8 {
    let mut total = 0;

    loop {
        let item = select! {
            x = s1.next() =&gt; x,
            x = s2.next() =&gt; x,
            complete =&gt; break,
        };
        if let Some(next_num) = item {
            total += next_num;
        }
    }

    total
}
<span class="boring">}</span></code></pre></pre>
<h2 id="06_multiple_futures-03_select-concurrent-tasks-in-a-select-loop-with-fuse-and-futuresunordered"><a class="header" href="#06_multiple_futures-03_select-concurrent-tasks-in-a-select-loop-with-fuse-and-futuresunordered">Concurrent tasks in a <code>select</code> loop with <code>Fuse</code> and <code>FuturesUnordered</code></a></h2>
<p>One somewhat hard-to-discover but handy function is <code>Fuse::terminated()</code>,
which allows constructing an empty future which is already terminated,
and can later be filled in with a future that needs to be run.</p>
<p>This can be handy when there's a task that needs to be run during a <code>select</code>
loop but which is created inside the <code>select</code> loop itself.</p>
<p>Note the use of the <code>.select_next_some()</code> function. This can be
used with <code>select</code> to only run the branch for <code>Some(_)</code> values
returned from the stream, ignoring <code>None</code>s.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    future::{Fuse, FusedFuture, FutureExt},
    stream::{FusedStream, Stream, StreamExt},
    pin_mut,
    select,
};

async fn get_new_num() -&gt; u8 { /* ... */ 5 }

async fn run_on_new_num(_: u8) { /* ... */ }

async fn run_loop(
    mut interval_timer: impl Stream&lt;Item = ()&gt; + FusedStream + Unpin,
    starting_num: u8,
) {
    let run_on_new_num_fut = run_on_new_num(starting_num).fuse();
    let get_new_num_fut = Fuse::terminated();
    pin_mut!(run_on_new_num_fut, get_new_num_fut);
    loop {
        select! {
            () = interval_timer.select_next_some() =&gt; {
                // The timer has elapsed. Start a new `get_new_num_fut`
                // if one was not already running.
                if get_new_num_fut.is_terminated() {
                    get_new_num_fut.set(get_new_num().fuse());
                }
            },
            new_num = get_new_num_fut =&gt; {
                // A new number has arrived -- start a new `run_on_new_num_fut`,
                // dropping the old one.
                run_on_new_num_fut.set(run_on_new_num(new_num).fuse());
            },
            // Run the `run_on_new_num_fut`
            () = run_on_new_num_fut =&gt; {},
            // panic if everything completed, since the `interval_timer` should
            // keep yielding values indefinitely.
            complete =&gt; panic!("`interval_timer` completed unexpectedly"),
        }
    }
}
<span class="boring">}</span></code></pre></pre>
<p>When many copies of the same future need to be run simultaneously,
use the <code>FuturesUnordered</code> type. The following example is similar
to the one above, but will run each copy of <code>run_on_new_num_fut</code>
to completion, rather than aborting them when a new one is created.
It will also print out a value returned by <code>run_on_new_num_fut</code>.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::{
    future::{Fuse, FusedFuture, FutureExt},
    stream::{FusedStream, FuturesUnordered, Stream, StreamExt},
    pin_mut,
    select,
};

async fn get_new_num() -&gt; u8 { /* ... */ 5 }

async fn run_on_new_num(_: u8) -&gt; u8 { /* ... */ 5 }

async fn run_loop(
    mut interval_timer: impl Stream&lt;Item = ()&gt; + FusedStream + Unpin,
    starting_num: u8,
) {
    let mut run_on_new_num_futs = FuturesUnordered::new();
    run_on_new_num_futs.push(run_on_new_num(starting_num));
    let get_new_num_fut = Fuse::terminated();
    pin_mut!(get_new_num_fut);
    loop {
        select! {
            () = interval_timer.select_next_some() =&gt; {
                // The timer has elapsed. Start a new `get_new_num_fut`
                // if one was not already running.
                if get_new_num_fut.is_terminated() {
                    get_new_num_fut.set(get_new_num().fuse());
                }
            },
            new_num = get_new_num_fut =&gt; {
                // A new number has arrived -- start a new `run_on_new_num_fut`.
                run_on_new_num_futs.push(run_on_new_num(new_num));
            },
            // Run the `run_on_new_num_futs` and check if any have completed
            res = run_on_new_num_futs.select_next_some() =&gt; {
                println!("run_on_new_num_fut returned {:?}", res);
            },
            // panic if everything completed, since the `interval_timer` should
            // keep yielding values indefinitely.
            complete =&gt; panic!("`interval_timer` completed unexpectedly"),
        }
    }
}

<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="06_multiple_futures-04_spawning"></div><h1 id="06_multiple_futures-04_spawning-spawning"><a class="header" href="#06_multiple_futures-04_spawning-spawning"><code>Spawning</code></a></h1>
<p>Spawning allows you to run a new asynchronous task in the background. This allows us to continue executing other code
while it runs.</p>
<p>Say we have a web server that wants to accept connections without blocking the main thread.
To achieve this, we can use the <code>async_std::task::spawn</code> function to create and run a new task that handles the
connections. This function takes a future and returns a <code>JoinHandle</code>, which can be used to wait for the result of the
task once it's completed.</p>
<pre><pre class="playground"><code class="language-rust edition2018">use async_std::{task, net::TcpListener, net::TcpStream};
use futures::AsyncWriteExt;

async fn process_request(stream: &amp;mut TcpStream) -&gt; Result&lt;(), std::io::Error&gt;{
    stream.write_all(b"HTTP/1.1 200 OK\r\n\r\n").await?;
    stream.write_all(b"Hello World").await?;
    Ok(())
}

async fn main() {
    let listener = TcpListener::bind("127.0.0.1:8080").await.unwrap();
    loop {
        // Accept a new connection
        let (mut stream, _) = listener.accept().await.unwrap();
        // Now process this request without blocking the main loop
        task::spawn(async move {process_request(&amp;mut stream).await});
    }
}</code></pre></pre>
<p>The <code>JoinHandle</code> returned by <code>spawn</code> implements the <code>Future</code> trait, so we can <code>.await</code> it to get the result of the task.
This will block the current task until the spawned task completes. If the task is not awaited, your program will
continue executing without waiting for the task, cancelling it if the function is completed before the task is finished.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::join_all;
async fn task_spawner(){
    let tasks = vec![
        task::spawn(my_task(Duration::from_secs(1))),
        task::spawn(my_task(Duration::from_secs(2))),
        task::spawn(my_task(Duration::from_secs(3))),
    ];
    // If we do not await these tasks and the function finishes, they will be dropped
    join_all(tasks).await;
}
<span class="boring">}</span></code></pre></pre>
<p>To communicate between the main task and the spawned task, we can use channels
provided by the async runtime used.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="07_workarounds-01_chapter"></div><h1 id="07_workarounds-01_chapter-workarounds-to-know-and-love"><a class="header" href="#07_workarounds-01_chapter-workarounds-to-know-and-love">Workarounds to Know and Love</a></h1>
<p>Rust's <code>async</code> support is still fairly new, and there are a handful of
highly-requested features still under active development, as well
as some subpar diagnostics. This chapter will discuss some common pain
points and explain how to work around them.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="07_workarounds-02_err_in_async_blocks"></div><h1 id="07_workarounds-02_err_in_async_blocks--in-async-blocks"><a class="header" href="#07_workarounds-02_err_in_async_blocks--in-async-blocks"><code>?</code> in <code>async</code> Blocks</a></h1>
<p>Just as in <code>async fn</code>, it's common to use <code>?</code> inside <code>async</code> blocks.
However, the return type of <code>async</code> blocks isn't explicitly stated.
This can cause the compiler to fail to infer the error type of the
<code>async</code> block.</p>
<p>For example, this code:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">struct MyError;
</span><span class="boring">async fn foo() -&gt; Result&lt;(), MyError&gt; { Ok(()) }
</span><span class="boring">async fn bar() -&gt; Result&lt;(), MyError&gt; { Ok(()) }
</span>let fut = async {
    foo().await?;
    bar().await?;
    Ok(())
};
<span class="boring">}</span></code></pre></pre>
<p>will trigger this error:</p>
<pre><code>error[E0282]: type annotations needed
 --&gt; src/main.rs:5:9
  |
4 |     let fut = async {
  |         --- consider giving `fut` a type
5 |         foo().await?;
  |         ^^^^^^^^^^^^ cannot infer type
</code></pre>
<p>Unfortunately, there's currently no way to "give <code>fut</code> a type", nor a way
to explicitly specify the return type of an <code>async</code> block.
To work around this, use the "turbofish" operator to supply the success and
error types for the <code>async</code> block:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">struct MyError;
</span><span class="boring">async fn foo() -&gt; Result&lt;(), MyError&gt; { Ok(()) }
</span><span class="boring">async fn bar() -&gt; Result&lt;(), MyError&gt; { Ok(()) }
</span>let fut = async {
    foo().await?;
    bar().await?;
    Ok::&lt;(), MyError&gt;(()) // &lt;- note the explicit type annotation here
};
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="07_workarounds-03_send_approximation"></div><h1 id="07_workarounds-03_send_approximation-send-approximation"><a class="header" href="#07_workarounds-03_send_approximation-send-approximation"><code>Send</code> Approximation</a></h1>
<p>Some <code>async fn</code> state machines are safe to be sent across threads, while
others are not. Whether or not an <code>async fn</code> <code>Future</code> is <code>Send</code> is determined
by whether a non-<code>Send</code> type is held across an <code>.await</code> point. The compiler
does its best to approximate when values may be held across an <code>.await</code>
point, but this analysis is too conservative in a number of places today.</p>
<p>For example, consider a simple non-<code>Send</code> type, perhaps a type
which contains an <code>Rc</code>:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::rc::Rc;

#[derive(Default)]
struct NotSend(Rc&lt;()&gt;);
<span class="boring">}</span></code></pre></pre>
<p>Variables of type <code>NotSend</code> can briefly appear as temporaries in <code>async fn</code>s
even when the resulting <code>Future</code> type returned by the <code>async fn</code> must be <code>Send</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">use std::rc::Rc;
</span><span class="boring">#[derive(Default)]
</span><span class="boring">struct NotSend(Rc&lt;()&gt;);
</span>async fn bar() {}
async fn foo() {
    NotSend::default();
    bar().await;
}

fn require_send(_: impl Send) {}

fn main() {
    require_send(foo());
}</code></pre></pre>
<p>However, if we change <code>foo</code> to store <code>NotSend</code> in a variable, this example no
longer compiles:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">use std::rc::Rc;
</span><span class="boring">#[derive(Default)]
</span><span class="boring">struct NotSend(Rc&lt;()&gt;);
</span><span class="boring">async fn bar() {}
</span>async fn foo() {
    let x = NotSend::default();
    bar().await;
}
<span class="boring">fn require_send(_: impl Send) {}
</span><span class="boring">fn main() {
</span><span class="boring">   require_send(foo());
</span><span class="boring">}</span></code></pre></pre>
<pre><code>error[E0277]: `std::rc::Rc&lt;()&gt;` cannot be sent between threads safely
  --&gt; src/main.rs:15:5
   |
15 |     require_send(foo());
   |     ^^^^^^^^^^^^ `std::rc::Rc&lt;()&gt;` cannot be sent between threads safely
   |
   = help: within `impl std::future::Future`, the trait `std::marker::Send` is not implemented for `std::rc::Rc&lt;()&gt;`
   = note: required because it appears within the type `NotSend`
   = note: required because it appears within the type `{NotSend, impl std::future::Future, ()}`
   = note: required because it appears within the type `[static generator@src/main.rs:7:16: 10:2 {NotSend, impl std::future::Future, ()}]`
   = note: required because it appears within the type `std::future::GenFuture&lt;[static generator@src/main.rs:7:16: 10:2 {NotSend, impl std::future::Future, ()}]&gt;`
   = note: required because it appears within the type `impl std::future::Future`
   = note: required because it appears within the type `impl std::future::Future`
note: required by `require_send`
  --&gt; src/main.rs:12:1
   |
12 | fn require_send(_: impl Send) {}
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

error: aborting due to previous error

For more information about this error, try `rustc --explain E0277`.
</code></pre>
<p>This error is correct. If we store <code>x</code> into a variable, it won't be dropped
until after the <code>.await</code>, at which point the <code>async fn</code> may be running on
a different thread. Since <code>Rc</code> is not <code>Send</code>, allowing it to travel across
threads would be unsound. One simple solution to this would be to <code>drop</code>
the <code>Rc</code> before the <code>.await</code>, but unfortunately that does not work today.</p>
<p>In order to successfully work around this issue, you may have to introduce
a block scope encapsulating any non-<code>Send</code> variables. This makes it easier
for the compiler to tell that these variables do not live across an
<code>.await</code> point.</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">use std::rc::Rc;
</span><span class="boring">#[derive(Default)]
</span><span class="boring">struct NotSend(Rc&lt;()&gt;);
</span><span class="boring">async fn bar() {}
</span>async fn foo() {
    {
        let x = NotSend::default();
    }
    bar().await;
}
<span class="boring">fn require_send(_: impl Send) {}
</span><span class="boring">fn main() {
</span><span class="boring">   require_send(foo());
</span><span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="07_workarounds-04_recursion"></div><h1 id="07_workarounds-04_recursion-recursion"><a class="header" href="#07_workarounds-04_recursion-recursion">Recursion</a></h1>
<p>Internally, <code>async fn</code> creates a state machine type containing each
sub-<code>Future</code> being <code>.await</code>ed. This makes recursive <code>async fn</code>s a little
tricky, since the resulting state machine type has to contain itself:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">async fn step_one() { /* ... */ }
</span><span class="boring">async fn step_two() { /* ... */ }
</span><span class="boring">struct StepOne;
</span><span class="boring">struct StepTwo;
</span>// This function:
async fn foo() {
    step_one().await;
    step_two().await;
}
// generates a type like this:
enum Foo {
    First(StepOne),
    Second(StepTwo),
}

// So this function:
async fn recursive() {
    recursive().await;
    recursive().await;
}

// generates a type like this:
enum Recursive {
    First(Recursive),
    Second(Recursive),
}
<span class="boring">}</span></code></pre></pre>
<p>This won't work—we've created an infinitely-sized type!
The compiler will complain:</p>
<pre><code>error[E0733]: recursion in an `async fn` requires boxing
 --&gt; src/lib.rs:1:22
  |
1 | async fn recursive() {
  |                      ^ an `async fn` cannot invoke itself directly
  |
  = note: a recursive `async fn` must be rewritten to return a boxed future.
</code></pre>
<p>In order to allow this, we have to introduce an indirection using <code>Box</code>.
Unfortunately, compiler limitations mean that just wrapping the calls to
<code>recursive()</code> in <code>Box::pin</code> isn't enough. To make this work, we have
to make <code>recursive</code> into a non-<code>async</code> function which returns a <code>.boxed()</code>
<code>async</code> block:</p>
<pre><pre class="playground"><code class="language-rust edition2018"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use futures::future::{BoxFuture, FutureExt};

fn recursive() -&gt; BoxFuture&lt;'static, ()&gt; {
    async move {
        recursive().await;
        recursive().await;
    }.boxed()
}
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="07_workarounds-05_async_in_traits"></div><h1 id="07_workarounds-05_async_in_traits-async-in-traits"><a class="header" href="#07_workarounds-05_async_in_traits-async-in-traits"><code>async</code> in Traits</a></h1>
<p>Currently, <code>async fn</code> cannot be used in traits on the stable release of Rust.
Since the 17th November 2022, an MVP of async-fn-in-trait is available on the nightly
version of the compiler tool chain, <a href="https://blog.rust-lang.org/inside-rust/2022/11/17/async-fn-in-trait-nightly.html">see here for details</a>.</p>
<p>In the meantime, there is a work around for the stable tool chain using the
<a href="https://github.com/dtolnay/async-trait">async-trait crate from crates.io</a>.</p>
<p>Note that using these trait methods will result in a heap allocation
per-function-call. This is not a significant cost for the vast majority
of applications, but should be considered when deciding whether to use
this functionality in the public API of a low-level function that is expected
to be called millions of times a second.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="08_ecosystem-00_chapter"></div><h1 id="08_ecosystem-00_chapter-the-async-ecosystem"><a class="header" href="#08_ecosystem-00_chapter-the-async-ecosystem">The Async Ecosystem</a></h1>
<p>Rust currently provides only the bare essentials for writing async code.
Importantly, executors, tasks, reactors, combinators, and low-level I/O futures and traits
are not yet provided in the standard library. In the meantime,
community-provided async ecosystems fill in these gaps.</p>
<p>The Async Foundations Team is interested in extending examples in the Async Book to cover multiple runtimes.
If you're interested in contributing to this project, please reach out to us on
<a href="https://rust-lang.zulipchat.com/#narrow/stream/201246-wg-async-foundations.2Fbook">Zulip</a>.</p>
<h2 id="08_ecosystem-00_chapter-async-runtimes"><a class="header" href="#08_ecosystem-00_chapter-async-runtimes">Async Runtimes</a></h2>
<p>Async runtimes are libraries used for executing async applications.
Runtimes usually bundle together a <em>reactor</em> with one or more <em>executors</em>.
Reactors provide subscription mechanisms for external events, like async I/O, interprocess communication, and timers.
In an async runtime, subscribers are typically futures representing low-level I/O operations.
Executors handle the scheduling and execution of tasks.
They keep track of running and suspended tasks, poll futures to completion, and wake tasks when they can make progress.
The word "executor" is frequently used interchangeably with "runtime".
Here, we use the word "ecosystem" to describe a runtime bundled with compatible traits and features.</p>
<h2 id="08_ecosystem-00_chapter-community-provided-async-crates"><a class="header" href="#08_ecosystem-00_chapter-community-provided-async-crates">Community-Provided Async Crates</a></h2>
<h3 id="08_ecosystem-00_chapter-the-futures-crate"><a class="header" href="#08_ecosystem-00_chapter-the-futures-crate">The Futures Crate</a></h3>
<p>The <a href="https://docs.rs/futures/"><code>futures</code> crate</a> contains traits and functions useful for writing async code.
This includes the <code>Stream</code>, <code>Sink</code>, <code>AsyncRead</code>, and <code>AsyncWrite</code> traits, and utilities such as combinators.
These utilities and traits may eventually become part of the standard library.</p>
<p><code>futures</code> has its own executor, but not its own reactor, so it does not support execution of async I/O or timer futures.
For this reason, it's not considered a full runtime.
A common choice is to use utilities from <code>futures</code> with an executor from another crate.</p>
<h3 id="08_ecosystem-00_chapter-popular-async-runtimes"><a class="header" href="#08_ecosystem-00_chapter-popular-async-runtimes">Popular Async Runtimes</a></h3>
<p>There is no asynchronous runtime in the standard library, and none are officially recommended.
The following crates provide popular runtimes.</p>
<ul>
<li><a href="https://docs.rs/tokio/">Tokio</a>: A popular async ecosystem with HTTP, gRPC, and tracing frameworks.</li>
<li><a href="https://docs.rs/async-std/">async-std</a>: A crate that provides asynchronous counterparts to standard library components.</li>
<li><a href="https://docs.rs/smol/">smol</a>: A small, simplified async runtime.
Provides the <code>Async</code> trait that can be used to wrap structs like <code>UnixStream</code> or <code>TcpListener</code>.</li>
<li><a href="https://fuchsia.googlesource.com/fuchsia/+/master/src/lib/fuchsia-async/">fuchsia-async</a>:
An executor for use in the Fuchsia OS.</li>
</ul>
<h2 id="08_ecosystem-00_chapter-determining-ecosystem-compatibility"><a class="header" href="#08_ecosystem-00_chapter-determining-ecosystem-compatibility">Determining Ecosystem Compatibility</a></h2>
<p>Not all async applications, frameworks, and libraries are compatible with each other, or with every OS or platform.
Most async code can be used with any ecosystem, but some frameworks and libraries require the use of a specific ecosystem.
Ecosystem constraints are not always documented, but there are several rules of thumb to determine
whether a library, trait, or function depends on a specific ecosystem.</p>
<p>Any async code that interacts with async I/O, timers, interprocess communication, or tasks
generally depends on a specific async executor or reactor.
All other async code, such as async expressions, combinators, synchronization types, and streams
are usually ecosystem independent, provided that any nested futures are also ecosystem independent.
Before beginning a project, it's recommended to research relevant async frameworks and libraries to ensure
compatibility with your chosen runtime and with each other.</p>
<p>Notably, <code>Tokio</code> uses the <code>mio</code> reactor and defines its own versions of async I/O traits,
including <code>AsyncRead</code> and <code>AsyncWrite</code>.
On its own, it's not compatible with <code>async-std</code> and <code>smol</code>,
which rely on the <a href="https://docs.rs/async-executor"><code>async-executor</code> crate</a>, and the <code>AsyncRead</code> and <code>AsyncWrite</code>
traits defined in <code>futures</code>.</p>
<p>Conflicting runtime requirements can sometimes be resolved by compatibility layers
that allow you to call code written for one runtime within another.
For example, the <a href="https://docs.rs/async_compat"><code>async_compat</code> crate</a> provides a compatibility layer between
<code>Tokio</code> and other runtimes.</p>
<p>Libraries exposing async APIs should not depend on a specific executor or reactor,
unless they need to spawn tasks or define their own async I/O or timer futures.
Ideally, only binaries should be responsible for scheduling and running tasks.</p>
<h2 id="08_ecosystem-00_chapter-single-threaded-vs-multi-threaded-executors"><a class="header" href="#08_ecosystem-00_chapter-single-threaded-vs-multi-threaded-executors">Single Threaded vs Multi-Threaded Executors</a></h2>
<p>Async executors can be single-threaded or multi-threaded.
For example, the <code>async-executor</code> crate has both a single-threaded <code>LocalExecutor</code> and a multi-threaded <code>Executor</code>.</p>
<p>A multi-threaded executor makes progress on several tasks simultaneously.
It can speed up the execution greatly for workloads with many tasks,
but synchronizing data between tasks is usually more expensive.
It is recommended to measure performance for your application
when you are choosing between a single- and a multi-threaded runtime.</p>
<p>Tasks can either be run on the thread that created them or on a separate thread.
Async runtimes often provide functionality for spawning tasks onto separate threads.
Even if tasks are executed on separate threads, they should still be non-blocking.
In order to schedule tasks on a multi-threaded executor, they must also be <code>Send</code>.
Some runtimes provide functions for spawning non-<code>Send</code> tasks,
which ensures every task is executed on the thread that spawned it.
They may also provide functions for spawning blocking tasks onto dedicated threads,
which is useful for running blocking synchronous code from other libraries.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="09_example-00_intro"></div><h1 id="09_example-00_intro-final-project-building-a-concurrent-web-server-with-async-rust"><a class="header" href="#09_example-00_intro-final-project-building-a-concurrent-web-server-with-async-rust">Final Project: Building a Concurrent Web Server with Async Rust</a></h1>
<p>In this chapter, we'll use asynchronous Rust to modify the Rust book's
<a href="https://doc.rust-lang.org/book/ch20-01-single-threaded.html">single-threaded web server</a>
to serve requests concurrently.</p>
<h2 id="09_example-00_intro-recap"><a class="header" href="#09_example-00_intro-recap">Recap</a></h2>
<p>Here's what the code looked like at the end of the lesson.</p>
<p><code>src/main.rs</code>:</p>
<pre><pre class="playground"><code class="language-rust">use std::fs;
use std::io::prelude::*;
use std::net::TcpListener;
use std::net::TcpStream;

fn main() {
    // Listen for incoming TCP connections on localhost port 7878
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();

    // Block forever, handling each request that arrives at this IP address
    for stream in listener.incoming() {
        let stream = stream.unwrap();

        handle_connection(stream);
    }
}

fn handle_connection(mut stream: TcpStream) {
    // Read the first 1024 bytes of data from the stream
    let mut buffer = [0; 1024];
    stream.read(&amp;mut buffer).unwrap();

    let get = b"GET / HTTP/1.1\r\n";

    // Respond with greetings or a 404,
    // depending on the data in the request
    let (status_line, filename) = if buffer.starts_with(get) {
        ("HTTP/1.1 200 OK\r\n\r\n", "hello.html")
    } else {
        ("HTTP/1.1 404 NOT FOUND\r\n\r\n", "404.html")
    };
    let contents = fs::read_to_string(filename).unwrap();

    // Write response back to the stream,
    // and flush the stream to ensure the response is sent back to the client
    let response = format!("{status_line}{contents}");
    stream.write_all(response.as_bytes()).unwrap();
    stream.flush().unwrap();
}</code></pre></pre>
<p><code>hello.html</code>:</p>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
  &lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;title&gt;Hello!&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;Hello!&lt;/h1&gt;
    &lt;p&gt;Hi from Rust&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p><code>404.html</code>:</p>
<pre><code class="language-html">&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
  &lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;title&gt;Hello!&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h1&gt;Oops!&lt;/h1&gt;
    &lt;p&gt;Sorry, I don't know what you're asking for.&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>If you run the server with <code>cargo run</code> and visit <code>127.0.0.1:7878</code> in your browser,
you'll be greeted with a friendly message from Ferris!</p>
<div style="break-before: page; page-break-before: always;"></div><div id="09_example-01_running_async_code"></div><h1 id="09_example-01_running_async_code-running-asynchronous-code"><a class="header" href="#09_example-01_running_async_code-running-asynchronous-code">Running Asynchronous Code</a></h1>
<p>An HTTP server should be able to serve multiple clients concurrently;
that is, it should not wait for previous requests to complete before handling the current request.
The book
<a href="https://doc.rust-lang.org/book/ch20-02-multithreaded.html#turning-our-single-threaded-server-into-a-multithreaded-server">solves this problem</a>
by creating a thread pool where each connection is handled on its own thread.
Here, instead of improving throughput by adding threads, we'll achieve the same effect using asynchronous code.</p>
<p>Let's modify <code>handle_connection</code> to return a future by declaring it an <code>async fn</code>:</p>
<pre><code class="language-rust ignore">async fn handle_connection(mut stream: TcpStream) {
    //&lt;-- snip --&gt;
}</code></pre>
<p>Adding <code>async</code> to the function declaration changes its return type
from the unit type <code>()</code> to a type that implements <code>Future&lt;Output=()&gt;</code>.</p>
<p>If we try to compile this, the compiler warns us that it will not work:</p>
<pre><code class="language-console">$ cargo check
    Checking async-rust v0.1.0 (file:///projects/async-rust)
warning: unused implementer of `std::future::Future` that must be used
  --&gt; src/main.rs:12:9
   |
12 |         handle_connection(stream);
   |         ^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
   = note: `#[warn(unused_must_use)]` on by default
   = note: futures do nothing unless you `.await` or poll them
</code></pre>
<p>Because we haven't <code>await</code>ed or <code>poll</code>ed the result of <code>handle_connection</code>,
it'll never run. If you run the server and visit <code>127.0.0.1:7878</code> in a browser,
you'll see that the connection is refused; our server is not handling requests.</p>
<p>We can't <code>await</code> or <code>poll</code> futures within synchronous code by itself.
We'll need an asynchronous runtime to handle scheduling and running futures to completion.
Please consult the <a href="#08_ecosystem-00_chapter">section on choosing a runtime</a>
for more information on asynchronous runtimes, executors, and reactors.
Any of the runtimes listed will work for this project, but for these examples,
we've chosen to use the <code>async-std</code> crate.</p>
<h2 id="09_example-01_running_async_code-adding-an-async-runtime"><a class="header" href="#09_example-01_running_async_code-adding-an-async-runtime">Adding an Async Runtime</a></h2>
<p>The following example will demonstrate refactoring synchronous code to use an async runtime; here, <code>async-std</code>.
The <code>#[async_std::main]</code> attribute from <code>async-std</code> allows us to write an asynchronous main function.
To use it, enable the <code>attributes</code> feature of <code>async-std</code> in <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[dependencies.async-std]
version = "1.6"
features = ["attributes"]
</code></pre>
<p>As a first step, we'll switch to an asynchronous main function,
and <code>await</code> the future returned by the async version of <code>handle_connection</code>.
Then, we'll test how the server responds.
Here's what that would look like:</p>
<pre><pre class="playground"><code class="language-rust">#[async_std::main]
async fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").unwrap();
    for stream in listener.incoming() {
        let stream = stream.unwrap();
        // Warning: This is not concurrent!
        handle_connection(stream).await;
    }
}</code></pre></pre>
<p>Now, let's test to see if our server can handle connections concurrently.
Simply making <code>handle_connection</code> asynchronous doesn't mean that the server
can handle multiple connections at the same time, and we'll soon see why.</p>
<p>To illustrate this, let's simulate a slow request.
When a client makes a request to <code>127.0.0.1:7878/sleep</code>,
our server will sleep for 5 seconds:</p>
<pre><code class="language-rust ignore">use std::time::Duration;
use async_std::task;

async fn handle_connection(mut stream: TcpStream) {
    let mut buffer = [0; 1024];
    stream.read(&amp;mut buffer).unwrap();

    let get = b"GET / HTTP/1.1\r\n";
    let sleep = b"GET /sleep HTTP/1.1\r\n";

    let (status_line, filename) = if buffer.starts_with(get) {
        ("HTTP/1.1 200 OK\r\n\r\n", "hello.html")
    } else if buffer.starts_with(sleep) {
        task::sleep(Duration::from_secs(5)).await;
        ("HTTP/1.1 200 OK\r\n\r\n", "hello.html")
    } else {
        ("HTTP/1.1 404 NOT FOUND\r\n\r\n", "404.html")
    };
    let contents = fs::read_to_string(filename).unwrap();

    let response = format!("{status_line}{contents}");
    stream.write(response.as_bytes()).unwrap();
    stream.flush().unwrap();
}</code></pre>
<p>This is very similar to the
<a href="https://doc.rust-lang.org/book/ch20-02-multithreaded.html#simulating-a-slow-request-in-the-current-server-implementation">simulation of a slow request</a>
from the Book, but with one important difference:
we're using the non-blocking function <code>async_std::task::sleep</code> instead of the blocking function <code>std::thread::sleep</code>.
It's important to remember that even if a piece of code is run within an <code>async fn</code> and <code>await</code>ed, it may still block.
To test whether our server handles connections concurrently, we'll need to ensure that <code>handle_connection</code> is non-blocking.</p>
<p>If you run the server, you'll see that a request to <code>127.0.0.1:7878/sleep</code>
will block any other incoming requests for 5 seconds!
This is because there are no other concurrent tasks that can make progress
while we are <code>await</code>ing the result of <code>handle_connection</code>.
In the next section, we'll see how to use async code to handle connections concurrently.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="09_example-02_handling_connections_concurrently"></div><h1 id="09_example-02_handling_connections_concurrently-handling-connections-concurrently"><a class="header" href="#09_example-02_handling_connections_concurrently-handling-connections-concurrently">Handling Connections Concurrently</a></h1>
<p>The problem with our code so far is that <code>listener.incoming()</code> is a blocking iterator.
The executor can't run other futures while <code>listener</code> waits on incoming connections,
and we can't handle a new connection until we're done with the previous one.</p>
<p>In order to fix this, we'll transform <code>listener.incoming()</code> from a blocking Iterator
to a non-blocking Stream. Streams are similar to Iterators, but can be consumed asynchronously.
For more information, see the <a href="#05_streams-01_chapter">chapter on Streams</a>.</p>
<p>Let's replace our blocking <code>std::net::TcpListener</code> with the non-blocking <code>async_std::net::TcpListener</code>,
and update our connection handler to accept an <code>async_std::net::TcpStream</code>:</p>
<pre><code class="language-rust ignore">use async_std::prelude::*;

async fn handle_connection(mut stream: TcpStream) {
    let mut buffer = [0; 1024];
    stream.read(&amp;mut buffer).await.unwrap();

    //&lt;-- snip --&gt;
    stream.write(response.as_bytes()).await.unwrap();
    stream.flush().await.unwrap();
}</code></pre>
<p>The asynchronous version of <code>TcpListener</code> implements the <code>Stream</code> trait for <code>listener.incoming()</code>,
a change which provides two benefits.
The first is that <code>listener.incoming()</code> no longer blocks the executor.
The executor can now yield to other pending futures
while there are no incoming TCP connections to be processed.</p>
<p>The second benefit is that elements from the Stream can optionally be processed concurrently,
using a Stream's <code>for_each_concurrent</code> method.
Here, we'll take advantage of this method to handle each incoming request concurrently.
We'll need to import the <code>Stream</code> trait from the <code>futures</code> crate, so our Cargo.toml now looks like this:</p>
<pre><code class="language-diff">+[dependencies]
+futures = "0.3"

 [dependencies.async-std]
 version = "1.6"
 features = ["attributes"]
</code></pre>
<p>Now, we can handle each connection concurrently by passing <code>handle_connection</code> in through a closure function.
The closure function takes ownership of each <code>TcpStream</code>, and is run as soon as a new <code>TcpStream</code> becomes available.
As long as <code>handle_connection</code> does not block, a slow request will no longer prevent other requests from completing.</p>
<pre><code class="language-rust ignore">use async_std::net::TcpListener;
use async_std::net::TcpStream;
use futures::stream::StreamExt;

#[async_std::main]
async fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").await.unwrap();
    listener
        .incoming()
        .for_each_concurrent(/* limit */ None, |tcpstream| async move {
            let tcpstream = tcpstream.unwrap();
            handle_connection(tcpstream).await;
        })
        .await;
}</code></pre>
<h1 id="09_example-02_handling_connections_concurrently-serving-requests-in-parallel"><a class="header" href="#09_example-02_handling_connections_concurrently-serving-requests-in-parallel">Serving Requests in Parallel</a></h1>
<p>Our example so far has largely presented concurrency (using async code)
as an alternative to parallelism (using threads).
However, async code and threads are not mutually exclusive.
In our example, <code>for_each_concurrent</code> processes each connection concurrently, but on the same thread.
The <code>async-std</code> crate allows us to spawn tasks onto separate threads as well.
Because <code>handle_connection</code> is both <code>Send</code> and non-blocking, it's safe to use with <code>async_std::task::spawn</code>.
Here's what that would look like:</p>
<pre><pre class="playground"><code class="language-rust">use async_std::task::spawn;

#[async_std::main]
async fn main() {
    let listener = TcpListener::bind("127.0.0.1:7878").await.unwrap();
    listener
        .incoming()
        .for_each_concurrent(/* limit */ None, |stream| async move {
            let stream = stream.unwrap();
            spawn(handle_connection(stream));
        })
        .await;
}</code></pre></pre>
<p>Now we are using both concurrency and parallelism to handle multiple requests at the same time!
See the <a href="#08_ecosystem-00_chapter-single-threading-vs-multithreading">section on multithreaded executors</a>
for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><div id="09_example-03_tests"></div><h1 id="09_example-03_tests-testing-the-tcp-server"><a class="header" href="#09_example-03_tests-testing-the-tcp-server">Testing the TCP Server</a></h1>
<p>Let's move on to testing our <code>handle_connection</code> function.</p>
<p>First, we need a <code>TcpStream</code> to work with.
In an end-to-end or integration test, we might want to make a real TCP connection
to test our code.
One strategy for doing this is to start a listener on <code>localhost</code> port 0.
Port 0 isn't a valid UNIX port, but it'll work for testing.
The operating system will pick an open TCP port for us.</p>
<p>Instead, in this example we'll write a unit test for the connection handler,
to check that the correct responses are returned for the respective inputs.
To keep our unit test isolated and deterministic, we'll replace the <code>TcpStream</code> with a mock.</p>
<p>First, we'll change the signature of <code>handle_connection</code> to make it easier to test.
<code>handle_connection</code> doesn't actually require an <code>async_std::net::TcpStream</code>;
it requires any struct that implements <code>async_std::io::Read</code>, <code>async_std::io::Write</code>, and <code>marker::Unpin</code>.
Changing the type signature to reflect this allows us to pass a mock for testing.</p>
<pre><code class="language-rust ignore">use async_std::io::{Read, Write};

async fn handle_connection(mut stream: impl Read + Write + Unpin) {</code></pre>
<p>Next, let's build a mock <code>TcpStream</code> that implements these traits.
First, let's implement the <code>Read</code> trait, with one method, <code>poll_read</code>.
Our mock <code>TcpStream</code> will contain some data that is copied into the read buffer,
and we'll return <code>Poll::Ready</code> to signify that the read is complete.</p>
<pre><code class="language-rust ignore">    use super::*;
    use futures::io::Error;
    use futures::task::{Context, Poll};

    use std::cmp::min;
    use std::pin::Pin;

    struct MockTcpStream {
        read_data: Vec&lt;u8&gt;,
        write_data: Vec&lt;u8&gt;,
    }

    impl Read for MockTcpStream {
        fn poll_read(
            self: Pin&lt;&amp;mut Self&gt;,
            _: &amp;mut Context,
            buf: &amp;mut [u8],
        ) -&gt; Poll&lt;Result&lt;usize, Error&gt;&gt; {
            let size: usize = min(self.read_data.len(), buf.len());
            buf[..size].copy_from_slice(&amp;self.read_data[..size]);
            Poll::Ready(Ok(size))
        }
    }</code></pre>
<p>Our implementation of <code>Write</code> is very similar,
although we'll need to write three methods: <code>poll_write</code>, <code>poll_flush</code>, and <code>poll_close</code>.
<code>poll_write</code> will copy any input data into the mock <code>TcpStream</code>, and return <code>Poll::Ready</code> when complete.
No work needs to be done to flush or close the mock <code>TcpStream</code>, so <code>poll_flush</code> and <code>poll_close</code>
can just return <code>Poll::Ready</code>.</p>
<pre><code class="language-rust ignore">    impl Write for MockTcpStream {
        fn poll_write(
            mut self: Pin&lt;&amp;mut Self&gt;,
            _: &amp;mut Context,
            buf: &amp;[u8],
        ) -&gt; Poll&lt;Result&lt;usize, Error&gt;&gt; {
            self.write_data = Vec::from(buf);

            Poll::Ready(Ok(buf.len()))
        }

        fn poll_flush(self: Pin&lt;&amp;mut Self&gt;, _: &amp;mut Context) -&gt; Poll&lt;Result&lt;(), Error&gt;&gt; {
            Poll::Ready(Ok(()))
        }

        fn poll_close(self: Pin&lt;&amp;mut Self&gt;, _: &amp;mut Context) -&gt; Poll&lt;Result&lt;(), Error&gt;&gt; {
            Poll::Ready(Ok(()))
        }
    }</code></pre>
<p>Lastly, our mock will need to implement <code>Unpin</code>, signifying that its location in memory can safely be moved.
For more information on pinning and the <code>Unpin</code> trait, see the <a href="#04_pinning-01_chapter">section on pinning</a>.</p>
<pre><code class="language-rust ignore">    impl Unpin for MockTcpStream {}</code></pre>
<p>Now we're ready to test the <code>handle_connection</code> function.
After setting up the <code>MockTcpStream</code> containing some initial data,
we can run <code>handle_connection</code> using the attribute <code>#[async_std::test]</code>, similarly to how we used <code>#[async_std::main]</code>.
To ensure that <code>handle_connection</code> works as intended, we'll check that the correct data
was written to the <code>MockTcpStream</code> based on its initial contents.</p>
<pre><code class="language-rust ignore">    use std::fs;

    #[async_std::test]
    async fn test_handle_connection() {
        let input_bytes = b"GET / HTTP/1.1\r\n";
        let mut contents = vec![0u8; 1024];
        contents[..input_bytes.len()].clone_from_slice(input_bytes);
        let mut stream = MockTcpStream {
            read_data: contents,
            write_data: Vec::new(),
        };

        handle_connection(&amp;mut stream).await;

        let expected_contents = fs::read_to_string("hello.html").unwrap();
        let expected_response = format!("HTTP/1.1 200 OK\r\n\r\n{}", expected_contents);
        assert!(stream.write_data.starts_with(expected_response.as_bytes()));
    }</code></pre>
<div style="break-before: page; page-break-before: always;"></div><div id="12_appendix-01_translations"></div><h1 id="12_appendix-01_translations-appendix--translations-of-the-book"><a class="header" href="#12_appendix-01_translations-appendix--translations-of-the-book">Appendix : Translations of the Book</a></h1>
<p>For resources in languages other than English.</p>
<ul>
<li><a href="https://doc.rust-lang.ru/async-book/">Русский</a></li>
<li><a href="https://jimskapt.github.io/async-book-fr/">Français</a></li>
<li><a href="https://rouzbehsbz.github.io/rust-async-book/">فارسی</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
