<!DOCTYPE html><html><head><meta charset="utf-8"><title>Spark 开发指南</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="本文档由colobu翻译自Spark官方文档 Spark Programming Guide，当前版本是1.1.1。
0.8.1版本由taobao技术部团队的月禾mm初审，以及微博上的Spark达人@crazyjvm复审。 0.8.1译文链接"><meta property="og:type" content="article"><meta property="og:title" content="Spark 开发指南"><meta property="og:url" content="https://colobu.com/2014/12/08/spark-programming-guide/"><meta property="og:site_name" content="鸟窝"><meta property="og:description" content="本文档由colobu翻译自Spark官方文档 Spark Programming Guide，当前版本是1.1.1。
0.8.1版本由taobao技术部团队的月禾mm初审，以及微博上的Spark达人@crazyjvm复审。 0.8.1译文链接"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Spark 开发指南"><meta name="twitter:description" content="本文档由colobu翻译自Spark官方文档 Spark Programming Guide，当前版本是1.1.1。
0.8.1版本由taobao技术部团队的月禾mm初审，以及微博上的Spark达人@crazyjvm复审。 0.8.1译文链接"><link rel="alternative" href="/atom.xml" title="鸟窝" type="application/atom+xml"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/style.css" type="text/css"><link href="//cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/css/jquery.fancybox.min.css" media="screen" type="text/css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" media="screen" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css"></head><body><div id="container"><div id="wrap"><header id="header"><div id="banner"></div><div id="header-outer" class="outer"><div id="header-title" class="inner"><h1 id="logo-wrap" class="animated bounceInLeft"><a href="/" id="logo">鸟窝</a></h1><h2 id="subtitle-wrap"><a href="https://item.jd.com/14283252.html" target="_blank" style="color: #e32d40;text-decoration: none"><b>《深入理解Go并发编程》新书发售中。一书在手，并发无忧</b></a></h2></div><div id="header-inner" class="inner"><nav id="main-nav"><a id="main-nav-toggle" class="nav-icon"></a> <a class="main-nav-link" href="/"><i class="fa fa-home">&nbsp;</i>首页</a> <a class="main-nav-link" href="/archives"><i class="fa fa-folder-o">&nbsp;</i>归档</a> <a class="main-nav-link" href="https://github.com/smallnest"><i class="fa fa-github">&nbsp;</i>github</a><div class="dropdown main-nav-link"><a class="main-nav-link" href="#"><i class="fa fa-bars">&nbsp;</i>网站群</a><div class="dropdown-content"><a href="/goasm"><i class="fa fa-language"></i>&nbsp;Go汇编示例</a> <a href="https://gowebexamples.com"><i class="fa fa-external-link"></i>&nbsp;Go Web开发示例</a> <a href="http://go-database-sql.org"><i class="fa fa-external-link"></i>&nbsp;Go 数据库开发教程</a> <a href="https://colobu.com/gotips/"><i class="fa fa-external-link"></i>&nbsp;Go 语言编程技巧</a><hr><a href="/perf-book"><i class="fa fa-rust"></i>&nbsp;Rust高性能编程指南</a> <a href="/rust100"><i class="fa fa-rust"></i>&nbsp;100个练习题学习Rust</a><hr><a href="http://rpcx.io"><i class="fa undefined"></i>&nbsp;RPCX官网</a> <a href="http://cn.doc.rpcx.io"><i class="fa undefined"></i>&nbsp;RPC开发指南</a></div></div><a class="main-nav-link" href="/ScalaCollectionsCookbook"><i class="fa fa-book">&nbsp;</i>Scala集合技术手册</a> <a class="main-nav-link" href="/about"><i class="fa fa-lemon-o">&nbsp;</i>关于</a></nav><nav id="sub-nav"><a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a> <a id="nav-search-btn" class="nav-icon" title="Search"></a></nav><div id="search-form-wrap"><form action="https://www.google.com/search" method="get" accept-charset="utf-8" class="search-form"><input type="search" id="search-word" name="q" maxlength="20" class="search-form-input" placeholder="Search"> <input type="hidden" name="ie" value="utf-8"> <input type="hidden" name="oe" value="utf-8"> <input type="hidden" name="hl" value="zh-CN"> <input type="submit" id="search-submit" class="search-form-submit"></form></div></div></div></header><div class="outer"><section id="main"><article id="post-spark-programming-guide" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2014/12/08/spark-programming-guide/" class="article-date"><time datetime="2014-12-08T06:36:16.000Z" itemprop="datePublished">2014年12月08日</time></a><div class="article-category"><a class="article-category-link" href="/categories/大数据/">大数据</a></div><div class="article-author">by smallnest</div></div><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">Spark 开发指南</h1></header><div class="article-entry" itemprop="articleBody"><h1 id="expanderHead" style="cursor:pointer">目录 <span id="expanderSign">[−]</span></h1><div id="article-entry-toc" data-role="collapsible" class="article-entry-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#简介"><span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#接入Spark"><span class="toc-text">接入Spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化Spark"><span class="toc-text">初始化Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#使用shell"><span class="toc-text">使用shell</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#弹性分布式数据集RDD"><span class="toc-text">弹性分布式数据集RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#并行集合(Parallelized_Collections)"><span class="toc-text">并行集合(Parallelized Collections)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#外部数据集(External_Datasets)"><span class="toc-text">外部数据集(External Datasets)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD_的操作"><span class="toc-text">RDD 的操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#基础操作"><span class="toc-text">基础操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#将function对象传给Spark"><span class="toc-text">将function对象传给Spark</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#使用键值对"><span class="toc-text">使用键值对</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#转换（transformation）"><span class="toc-text">转换（transformation）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#动作（actions）"><span class="toc-text">动作（actions）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD持久化"><span class="toc-text">RDD持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#存储级别的选择"><span class="toc-text">存储级别的选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#移除数据"><span class="toc-text">移除数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#共享变量"><span class="toc-text">共享变量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#广播变量_Broadcast_Variables"><span class="toc-text">广播变量 Broadcast Variables</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#累加器_Accumulators"><span class="toc-text">累加器 Accumulators</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#发布到集群中"><span class="toc-text">发布到集群中</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#单元测试"><span class="toc-text">单元测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从1-0以前的版本升级"><span class="toc-text">从1.0以前的版本升级</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#更多资料"><span class="toc-text">更多资料</span></a></li></ol></div><script>function show_answer(e,l){"显示答案"===e.value?e.value="隐藏答案":e.value="显示答案";var n=document.getElementById(l);"none"===n.style.display?n.style.display="block":n.style.display="none"}</script><p>本文档由<a href="http://colobu.com" target="_blank" rel="external">colobu</a>翻译自Spark官方文档 <a href="https://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">Spark Programming Guide</a>，当前版本是1.1.1。</p><p>0.8.1版本由taobao技术部团队的月禾mm初审，以及微博上的Spark达人@crazyjvm复审。 <a href="http://rdc.taobao.org/?p=2024" target="_blank" rel="external">0.8.1译文链接</a></p><a id="more"></a><h2 id="简介">简介</h2><p>总的来说，每一个Spark的应用，都是由一个驱动程序（driver program）构成，它运行用户的main函数，在一个集群上执行各种各样的并行操作。Spark提出的最主要抽象概念是弹性分布式数据集 (resilient distributed dataset,<strong>RDD</strong>)，它是元素的集合，划分到集群的各个节点上，可以被并行操作。RDDs的创建可以从HDFS(或者任意其他支持Hadoop文件系统) 上的一个文件开始，或者通过转换驱动程序（driver program）中已存在的Scala集合而来。用户也可以让Spark保留一个RDD在内存中，使其能在并行操作中被有效的重复使用。最后，RDD能自动从节点故障中恢复。</p><p>Spark的第二个抽象概念是共享变量（<strong>shared variables</strong>），可以在并行操作中使用。在默认情况下，Spark通过不同节点上的一系列任务来运行一个函数，它将每一个函数中用到的变量的拷贝传递到每一个任务中。有时候，一个变量需要在任务之间，或任务与驱动程序之间被共享。Spark 支持两种类型的共享变量：<em>广播变量</em> （broadcast variables），可以在内存的所有的结点上缓存变量；<em>累加器</em> （accumulators）：只能用于做加法的变量，例如计数或求和。</p><p>本指南将展示这些特性，并给出一些例子。读者最好比较熟悉Scala，尤其是闭包的语法。请留意，你也可以通过spark-shell脚本，来交互式地运行Spark。我们建议你在接下来的步骤中这样做。</p><h2 id="接入Spark">接入Spark</h2><p>Spark 1.1.1 需要搭配使用 Scala 2.10. 如果你用Scala 来编写应用，你需要使用相同版本的Scala，更新的大版本很可能不兼容。</p><p>要写一个Spark 应用，你需要给它加上Spark的依赖。如果你使用SBT或者Maven，Spark可以通过Maven中心库来获得：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="variable">groupId =</span> org.apache.spark</div><div class="line"><span class="variable">artifactId =</span> spark-core_2.<span class="number">10</span></div><div class="line"><span class="variable">version =</span> <span class="number">1.1</span>.<span class="number">1</span></div></pre></td></tr></table></figure><p>另外，如果你想访问一个HDFS集群，你需要根据你的HDFS版本，添加一个hadoop-client的依赖, HDFS的版本可以在<a href="https://spark.apache.org/docs/latest/hadoop-third-party-distributions.html" target="_blank" rel="external">third party distributions</a>找到：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="variable">groupId =</span> org.apache.hadoop</div><div class="line"><span class="variable">artifactId =</span> hadoop-client</div><div class="line"><span class="variable">version =</span> &lt;your-hdfs-version&gt;</div></pre></td></tr></table></figure><p>最后，你需要将一些Spark的类和隐式转换导入到你的程序中。通过如下语句：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.SparkContext</div><div class="line"><span class="keyword">import</span> org.apache.spark.SparkContext._</div><div class="line"><span class="keyword">import</span> org.apache.spark.SparkConf</div></pre></td></tr></table></figure><p>Spark 1.1.1 可以运行在Java 6及以上版本。 如果你使用Java 8, Spark支持Lambda表达式来代替实现function匿名类，否则你还是需要使用org.apache.spark.api.java.function 包下的function类.</p><p>翻译自 <a href="https://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">Spark Programming Guide</a></p><p>你需要引入Spark类以及隐式转换：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD</div><div class="line"><span class="keyword">import</span> org.apache.spark.SparkConf</div></pre></td></tr></table></figure><h2 id="初始化Spark">初始化Spark</h2><p>Spark程序需要做的第一件事情，就是创建一个SparkContext对象，它将告诉Spark如何访问一个集群。你需要创建一个包含你应用信息的SparkConf对象，把它传给JavaSparkContext 。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(appName).setMaster(master);</div><div class="line">JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</div></pre></td></tr></table></figure><p><em>appName</em>是在集群UI中显示的你应用的名字，master是一个<a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls" target="_blank" rel="external">Spark, Mesos or YARN cluster URL,</a>或者local模式运行的特殊字符串“local”。 实践中，当程序运行在集群中时，不需要在程序中硬编码master，而是使用<a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">spark-submit启动应用</a>. 然而对于本地测试和单元测试，你需要将&quot;local&quot;传给Spark。</p><h3 id="使用shell">使用shell</h3><p>使用Spark shell时, 一个特殊的交互式的SparkContext已经为你创建， 叫做sc变量. 你自己的SparkContext不会工作. 你可以使用<em>--master</em>参数指定context连接的master。你可以通过<em>--jar</em>参数增加外部jar. 例如运行bin/spark-shell在四个core上:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/spark-shell --master local[<span class="number">4</span>]</div></pre></td></tr></table></figure><p>也可以增加<code>code.jar</code>:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/spark-shell --master local[<span class="number">4</span>] --jars code.jar</div></pre></td></tr></table></figure><p>运行<code>spark-shell --help</code>查看更多的参数。</p><h2 id="弹性分布式数据集RDD">弹性分布式数据集RDD</h2><p>Spark围绕的概念是弹性分布式数据集（RDD），这是一个有容错机制并可以被并行操作的元素集合。目前有两种方式创建RDD：并行集合（Parallelized Collections）：接收一个已经存在的Scala集合，然后进行各种并行计算。 或者引用一个外部存储系统的数据集，比如共享文件系统，HDFS， HBase 或者hadoop支持的任意存储系统即可。</p><h3 id="并行集合(Parallelized_Collections)">并行集合(Parallelized Collections)</h3><p>并行集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建的。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。例如，下面的解释器输出，演示了如何从一个数组（1到5）创建一个并行集合：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> data = Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</div><div class="line"><span class="keyword">val</span> distData = sc.parallelize(data)</div></pre></td></tr></table></figure><p>一旦分布式数据集（distData）被创建好，它们将可以被并行操作。例如，我们可以调用<em>distData.reduce((a, b) =&gt; a + b)</em>来将数组的元素相加。我们会在后续的分布式数据集运算中进一步描述。</p><p>并行集合的一个重要参数是slices，表示数据集切分的份数。Spark将会在集群上为每一份数据起一个任务。典型地，你可以在集群的每个CPU上分布2-4个slices. 一般来说，Spark会尝试根据集群的状况，来自动设定slices的数目。然而，你也可以通过传递给parallelize的第二个参数来进行手动设置。（例如：sc.parallelize(data, 10)).</p><h3 id="外部数据集(External_Datasets)">外部数据集(External Datasets)</h3><p>Spark可以从Hadoop支持的文件系统创建数据集， 包括本地文件，HDFS,Cassandra,HBase，amazon S3等。Spark可以支持TextFile,SequenceFiles以及其它任何Hadoop输入格式。</p><p>Text file的RDDs可以通过SparkContext’s textFile的方式创建，该方法接受一个文件的URI地址（或者机器上的一个本地路径，或者一个hdfs://, sdn://,kfs://,其它URI). 下面是一个调用例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> distFile = sc.textFile(<span class="string">"data.txt"</span>)</div><div class="line">distFile: RDD[String] = MappedRDD@<span class="number">1</span>d4cee08</div></pre></td></tr></table></figure><p>一旦创建完成，distFile可以被进行数据集操作。例如，我们可以通过使用如下的map和reduce操作：<em>distFile.map(s =&gt; s.length).reduce((a, b) =&gt; a + b)</em>将所有数据行的长度相加。<br>读取文件时的一些注意点：</p><ul><li>如果使用本地文件系统，必须确保每个节点都能自己节点的此路径下访问相同的文件。 可以将文件复制到所有的worker上或者使用网络共享文件系统。</li><li>Spark所有的文件输入方法，包括textFile,支持文件夹，压缩文件和通配符。 比如你可以使用textFile(&quot;/my/directory&quot;), textFile(&quot;/my/directory/<em>.txt&quot;)和 textFile(&quot;/my/directory/</em>.gz&quot;)。</li><li>textFile方法也可以通过输入一个可选的第二参数，来控制文件的分片数目。默认情况下，Spark为每一块文件创建一个分片（HDFS默认的块大小为64MB)，但是你也可以通过传入一个更大的值，来指定一个更高的片值。注意，你不能指定一个比块数更小的片值。</li></ul><p>除了文本文件，Spark Scala API 也支持其它数据格式：</p><ul><li><code>SparkContext.wholeTextFiles</code>允许你读取文件夹下所有的文件，比如多个小的文本文件， 返回文件名/内容对。</li><li>对于SequenceFiles，可以使用SparkContext的sequenceFile[K, V]方法创建，其中K和V是文件中的key和values的类型。像IntWritable和Text一样，它们必须是Hadoop的Writable interface的子类。另外，对于几种通用Writable类型，Spark允许你指定原生类型来替代。例如：sequencFile[Int, String]将会自动读取IntWritable和Texts。</li><li>对于其他类型的Hadoop输入格式，你可以使用SparkContext.hadoopRDD方法，它可以接收任意类型的JobConf和输入格式类，键类型和值类型。按照像Hadoop作业一样的方法，来设置输入源就可以了。你也可以使用SparkContext.newHadoopRDD， 它基于新的MapReduce API(org.apache.hadoop.mapreduce).</li><li>RDD.saveAsObjectFile and SparkContext.objectFile支持保存RDD为一个简单格式， 包含序列化的Java对象. 尽管这不是一个高效的格式，比如Avro, 但是它提供了一个容易的方式来保存RDD。</li></ul><h3 id="RDD_的操作">RDD 的操作</h3><p>RDD支持两种操作：转换（transformation）从现有的数据集创建一个新的数据集；而动作（actions）在数据集上运行计算后，返回一个值给驱动程序。 例如，map就是一种转换，它将数据集每一个元素都传递给函数，并返回一个新的分布数据集表示结果。另一方面，reduce是一种动作，通过一些函数将所有的元素叠加起来，并将最终结果返回给Driver程序。（不过还有一个并行的reduceByKey，能返回一个分布式数据集）</p><p>Spark中的所有转换都是惰性的，也就是说，他们并不会直接计算结果。相反的，它们只是记住应用到基础数据集（例如一个文件）上的这些转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这个设计让Spark更加有效率的运行。例如，我们可以实现：通过map创建的一个新数据集，并在reduce中使用，最终只返回reduce的结果给driver，而不是整个大的新数据集。</p><p>默认情况下，每一个转换过的RDD都会在你在它之上执行一个动作时被重新计算。不过，你也可以使用persist(或者cache)方法，持久化一个RDD在内存中。在这种情况下，Spark将会在集群中，保存相关元素，下次你查询这个RDD时，它将能更快速访问。在磁盘上持久化数据集，或在集群间复制数据集也是支持的。</p><h4 id="基础操作">基础操作</h4><p>下面的代码演示了RDD的基本操作：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"data.txt"</span>)</div><div class="line"><span class="keyword">val</span> lineLengths = lines.map(s =&gt; s.length)</div><div class="line"><span class="keyword">val</span> totalLength = lineLengths.reduce((a, b) =&gt; a + b)</div></pre></td></tr></table></figure><p>第一行从一个外部文件创建了一个基本的RDD对象。这个数据集并没有加载到内存中，行只不过是一个指向文件的指针. 代码第二行定义行长度作为mao的结果， 行长度由于惰性设计并没有立即计算。最终 当我们运行reduce，这是一个action。 这时Spark将计算分解成运行在各个节点的任务。 每个节点运行它的map部分以及一个本地的reduction， 并仅将它的结果返回给驱动程序。</p><p>如果你想再使用行长度，我们可以在reduce之前增加:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">lineLengths.persist()</div></pre></td></tr></table></figure><p>它可以在lineLengths第一次计算之前被保存在内存中。</p><h4 id="将function对象传给Spark">将function对象传给Spark</h4><p>Spark API非常依赖在集群中运行的驱动程序中传递function， 对于Scala来说有两种方式实现:</p><ul><li>匿名函数语法（Anonymous function syntax）, 可以用作简短的代码。</li><li>全局单例对象的静态方法（Static methods in a global singleton object）. 例如，你可以定义MyFunctions对象，传递MyFunctions.func1, 如下所示:</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyFunctions</span> </span>{</div><div class="line">  <span class="keyword">def</span> func1(s: String): String = { ... }</div><div class="line">}</div><div class="line"></div><div class="line">myRdd.map(MyFunctions.func1)</div></pre></td></tr></table></figure><p>对于Java来说，function代表实现包<a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/function/package-summary.html" target="_blank" rel="external">org.apache.spark.api.java.function</a>的接口类。<br>也是有两种方法：</p><ul><li>实现Function 接口，匿名类或者非匿名类。</li><li>使用Java 8的lambda表达式</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">"data.txt"</span>);</div><div class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(<span class="keyword">new</span> Function&lt;String, Integer&gt;() {</div><div class="line">  <span class="keyword">public</span> Integer <span class="title">call</span>(String s) { <span class="keyword">return</span> s.length(); }</div><div class="line">});</div><div class="line"><span class="keyword">int</span> totalLength = lineLengths.reduce(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() {</div><div class="line">  <span class="keyword">public</span> Integer <span class="title">call</span>(Integer a, Integer b) { <span class="keyword">return</span> a + b; }</div><div class="line">});</div></pre></td></tr></table></figure><p>或</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">class GetLength implements Function&lt;String, Integer&gt; {</div><div class="line">  <span class="keyword">public</span> Integer <span class="title">call</span>(String s) { <span class="keyword">return</span> s.length(); }</div><div class="line">}</div><div class="line">class Sum implements Function2&lt;Integer, Integer, Integer&gt; {</div><div class="line">  <span class="keyword">public</span> Integer <span class="title">call</span>(Integer a, Integer b) { <span class="keyword">return</span> a + b; }</div><div class="line">}</div><div class="line"></div><div class="line">JavaRDD&lt;String&gt; lines = sc.textFile(<span class="string">"data.txt"</span>);</div><div class="line">JavaRDD&lt;Integer&gt; lineLengths = lines.map(<span class="keyword">new</span> GetLength());</div><div class="line"><span class="keyword">int</span> totalLength = lineLengths.reduce(<span class="keyword">new</span> Sum());</div></pre></td></tr></table></figure><h4 id="使用键值对">使用键值对</h4><p>大部分的Spark操作可以包含任意类似的对象，而一些特殊的操作只能操作键值对的RDD。 最有代表性的是“shuffle”操作， 比如根据键分组或者聚合元素。<br>在Scala中，这些操作可以使用包含<a href="http://www.scala-lang.org/api/2.10.4/index.html#scala.Tuple2" target="_blank" rel="external">Tuple2</a> 元素的RDD(Scala内建的tuple类型，只需(a, b)就可创建此类型的对象), 比需要import org.apache.spark.SparkContext._ 允许Spark隐式转换. 可以在PairRDDFunctions上应用键值对操作。</p><p>举例来说，下面的代码使用reduceByKey操作来计算行在文件中出现了多少次:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">"data.txt"</span>)</div><div class="line"><span class="keyword">val</span> pairs = lines.map(s =&gt; (s, <span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> counts = pairs.reduceByKey((a, b) =&gt; a + b)</div></pre></td></tr></table></figure><p>我们也可以使用<code>counts.sortByKey()</code>,例如按照字幕顺序排序然后使用counts.collect()继续将它们作为驱动程序的一个数组对象。</p><p><strong>注意</strong>: 当使用定制对象作为键时，必须保证equals() 和hashCode() 方法一致.</p><h4 id="转换（transformation）">转换（transformation）</h4><p>下面的列表列出了一些通用的转换。 请参考 RDD API doc (<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="external">Scala</a>, <a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html" target="_blank" rel="external">Java</a>, Python) 和 pair RDD functions doc (<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="external">Scala</a>, <a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html" target="_blank" rel="external">Java</a>) 了解细节.</p><table cellpadding="0" cellspacing="0"><tbody><tr><td valign="top"><b>&nbsp;转换</b></td><td valign="top"><b>含义</b></td></tr><tr><td valign="top"><b>map</b>(<i>func</i>)</td><td valign="top">返回一个新分布式数据集，由每一个输入元素经过<em>func</em>函数转换后组成</td></tr><tr><td valign="top"><b>filter</b>(<i>func</i>)</td><td valign="top">返回一个新数据集，由经过<em>func</em>函数计算后返回值为true的输入元素组成</td></tr><tr><td valign="top"><b>flatMap</b>(<i>func</i>)</td><td valign="top">类似于map，但是每一个输入元素可以被映射为0或多个输出元素（因此<em>func</em>应该返回一个序列，而不是单一元素）</td></tr><tr><td valign="top"><b>mapPartitions</b>(<i>func</i>)</td><td valign="top">类似于map，但独立地在RDD的每一个分块上运行，因此在类型为T的RDD上运行时，<i>func</i>的函数类型必须是Iterator[T] =&gt; Iterator[U]</td></tr><tr><td valign="top"><b>mapPartitionsWithSplit</b>(<i>func</i>)</td><td valign="top">类似于mapPartitions, 但<i>func带有</i>一个整数参数表示分块的索引值。因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Iterator[T]) =&gt; Iterator[U]</td></tr><tr><td valign="top"><b>sample</b>(<i>withReplacement</i>,<i>fraction</i>,&nbsp;<i>seed</i>)</td><td valign="top">根据<i>fraction</i>指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed用于指定随机数生成器种子</td></tr><tr><td valign="top"><b>union</b>(<i>otherDataset</i>)</td><td valign="top">返回一个新的数据集，新数据集是由源数据集和参数数据集联合而成</td></tr><tr><td valign="top"><b>distinct</b>([<i>numTasks</i>]))</td><td valign="top">返回一个包含源数据集中所有不重复元素的新数据集</td></tr><tr><td valign="top"><b>groupByKey</b>([<i>numTasks</i>])</td><td valign="top">在一个（K,V）对的数据集上调用，返回一个（K，Seq[V])对的数据集<br><b>注意：</b>默认情况下，只有8个并行任务来做操作，但是你可以传入一个可选的<samp>numTasks</samp>参数来改变它</td></tr><tr><td valign="top"><b>reduceByKey</b>(<i>func</i>, [<i>numTasks</i>])</td><td valign="top">在一个（K，V)对的数据集上调用时，返回一个（K，V）对的数据集，使用指定的<samp>reduce</samp>函数，将相同key的值聚合到一起。类似<samp>groupByKey</samp>，<samp>reduce</samp>任务个数是可以通过第二个可选参数来配置的</td></tr><tr><td><b>aggregateByKey</b>(<i>zeroValue</i>)(<i>seqOp</i>,<i>combOp</i>, [<i>numTasks</i>])</td><td>根据提供的函数进行聚合。When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral &quot;zero&quot; value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in<code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td></tr><tr><td valign="top"><b>sortByKey</b>([<i>ascending</i>], [<i>numTasks</i>])</td><td valign="top">在一个（K，V)对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的（K，V）对数据集。升序或降序由<samp>ascending</samp>布尔参数决定</td></tr><tr><td valign="top"><b>join</b>(<i>otherDataset</i>, [<i>numTasks</i>])</td><td valign="top">在类型为（K,V)和（K,W)类型的数据集上调用时，返回一个相同key对应的所有元素对在一起的(K, (V, W))数据集</td></tr><tr><td valign="top"><b>cogroup</b>(<i>otherDataset</i>, [<i>numTasks</i>])</td><td valign="top">在类型为（K,V)和（K,W)的数据集上调用，返回一个&nbsp;(K, Seq[V], Seq[W])元组的数据集。这个操作也可以称之为<samp>groupwith</samp></td></tr><tr><td valign="top"><b>cartesian</b>(<i>otherDataset</i>)</td><td valign="top">笛卡尔积，在类型为 T 和 U 类型的数据集上调用时，返回一个&nbsp;(T, U)对数据集(两两的元素对)</td></tr><tr><td><b>pipe</b>(<i>command</i>,<i>[envVars]</i>)</td><td>Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process&#39;s stdin and lines output to its stdout are returned as an RDD of strings.</td></tr><tr><td><b>coalesce</b>(<i>numPartitions</i>)</td><td>Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.</td></tr><tr><td><b>repartition</b>(<i>numPartitions</i>)</td><td>Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.</td></tr></tbody></table><h4 id="动作（actions）">动作（actions）</h4><p>下面的列表列出了一些通用的action操作. 请参考 RDD API doc (<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="external">Scala</a>, <a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaRDD.html" target="_blank" rel="external">Java</a>, Python) 和 pair RDD functions doc (<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="external">Scala</a>, <a href="https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html" target="_blank" rel="external">Java</a>) 了解细节.</p><table cellpadding="0" cellspacing="0"><tbody><tr><td valign="top"><b>&nbsp;动作</b></td><td valign="top"><b>含义</b></td></tr><tr><td valign="top"><b>reduce</b>(<i>func</i>)</td><td valign="top">通过函数<em>func</em>（接受两个参数，返回一个参数）聚集数据集中的所有元素。这个功能必须可交换且可关联的，从而可以正确的被并行执行。</td></tr><tr><td valign="top"><b>collect</b>()</td><td valign="top">在驱动程序中，以数组的形式，返回数据集的所有元素。这通常会在使用filter或者其它操作并返回一个足够小的数据子集后再使用会比较有用。</td></tr><tr><td valign="top"><b>count</b>()</td><td valign="top">返回数据集的元素的个数。</td></tr><tr><td valign="top"><b>first</b>()</td><td valign="top">返回数据集的第一个元素（类似于take（1））</td></tr><tr><td valign="top"><b>take</b>(<i>n</i>)</td><td valign="top">返回一个由数据集的前<em>n</em>个元素组成的数组。注意，这个操作目前并非并行执行，而是由驱动程序计算所有的元素</td></tr><tr><td valign="top"><b>takeSample</b>(<i>withReplacement</i>,<i>num</i>,&nbsp;<i>seed</i>)</td><td valign="top">返回一个数组，在数据集中随机采样<i>num</i>个元素组成，可以选择是否用随机数替换不足的部分，Seed用于指定的随机数生成器种子</td></tr><tr><td><b>takeOrdered</b>(<i>n</i>,<i>[ordering]</i>)</td><td>Return the first<i>n</i> elements of the RDD using either their natural order or a custom comparator.</td></tr><tr><td valign="top"><b>saveAsTextFile</b>(<i>path</i>)</td><td valign="top">将数据集的元素，以textfile的形式，保存到本地文件系统，HDFS或者任何其它hadoop支持的文件系统。对于每个元素，Spark将会调用<samp>toString</samp>方法，将它转换为文件中的文本行</td></tr><tr><td valign="top"><b>saveAsSequenceFile</b>(<i>path</i>)</td><td valign="top">将数据集的元素，以Hadoop sequencefile的格式，保存到指定的目录下，本地系统，HDFS或者任何其它hadoop支持的文件系统。这个只限于由key-value对组成，并实现了Hadoop的Writable接口，或者隐式的可以转换为Writable的RDD。（Spark包括了基本类型的转换，例如Int，Double，String，等等）</td></tr><tr><td><b>saveAsObjectFile</b>(<i>path</i>)<br>(Java and Scala)</td><td>Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using<code>SparkContext.objectFile()</code>.</td></tr><tr><td valign="top"><b>countByKey</b>()</td><td valign="top">对(K,V)类型的RDD有效，返回一个(K，Int)对的Map，表示每一个key对应的元素个数</td></tr><tr><td valign="top"><b>foreach</b>(<i>func</i>)</td><td valign="top">在数据集的每一个元素上，运行函数<em>func</em>进行更新。这通常用于边缘效果，例如更新一个累加器，或者和外部存储系统进行交互，例如HBase</td></tr></tbody></table><h3 id="RDD持久化">RDD持久化</h3><p>Spark最重要的一个功能，就是在不同操作间，持久化（或缓存）一个数据集在内存中。当你持久化一个RDD，每一个结点都将把它的计算分块结果保存在内存中，并在对此数据集（或者衍生出的数据集）进行的其它动作中重用。这将使得后续的动作(Actions)变得更加迅速（通常快10倍）。缓存是用Spark构建迭代算法的关键。</p><p>你可以用persist()或cache()方法来标记一个要被持久化的RDD，然后首次被一个动作（Action）触发时计算，它将会被保留在计算结点的内存中并重用。Cache有容错机制，如果RDD的任一分区丢失了，通过使用原先创建它的转换操作，它将会被自动重算（不需要全部重算，只计算丢失的部分）。</p><p>此外，每一个RDD都可以用不同的保存级别进行保存，从而允许你持久化数据集在硬盘，或者在内存作为序列化的Java对象（节省空间），甚至于跨结点复制。这些等级选择，是通过将一个org.apache.spark.storage.StorageLevel对象传递给persist()方法进行确定。cache()方法是使用默认存储级别的快捷方法，也就是StorageLevel.MEMORY_ONLY(将反序列化的对象存入内存）。</p><p>完整的可选存储级别如下：</p><table cellpadding="0" cellspacing="0"><tbody><tr><td valign="top"><b>存储级别</b></td><td valign="top"><b>&nbsp;意义</b></td></tr><tr><td valign="top">MEMORY_ONLY</td><td valign="top">将RDD作为反序列化的的对象存储JVM中。如果RDD不能被内存装下，一些分区将不会被缓存，并且在需要的时候被重新计算。这是是默认的级别</td></tr><tr><td valign="top">MEMORY_AND_DISK</td><td valign="top">将RDD作为反序列化的的对象存储在JVM中。如果RDD不能被与内存装下，超出的分区将被保存在硬盘上，并且在需要时被读取</td></tr><tr><td valign="top">MEMORY_ONLY_SER</td><td valign="top">将RDD作为序列化的的对象进行存储（每一分区占用一个字节数组）。通常来说，这比将对象反序列化的空间利用率更高，尤其当使用<a href="http://spark.incubator.apache.org/docs/latest/tuning.html" target="_blank" rel="external">fast serializer</a>,但在读取时会比较占用CPU</td></tr><tr><td valign="top">MEMORY_AND_DISK_SER</td><td valign="top">与<samp>MEMORY_ONLY_SER</samp>相似，但是把超出内存的分区将存储在硬盘上而不是在每次需要的时候重新计算</td></tr><tr><td valign="top">DISK_ONLY</td><td valign="top">只将RDD分区存储在硬盘上</td></tr><tr><td valign="top">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td><td valign="top">与上述的存储级别一样，但是将每一个分区都复制到两个集群结点上</td></tr><tr><td>OFF_HEAP (experimental)</td><td>Store RDD in serialized format in<a href="http://tachyon-project.org" target="_blank" rel="external">Tachyon</a>. Compared to MEMORY_ONLY_SER, OFF_HEAP reduces garbage collection overhead and allows executors to be smaller and to share a pool of memory, making it attractive in environments with large heaps or multiple concurrent applications. Furthermore, as the RDDs reside in Tachyon, the crash of an executor does not lead to losing the in-memory cache. In this mode, the memory in Tachyon is discardable. Thus, Tachyon does not attempt to reconstruct a block that it evicts from memory.</td></tr></tbody></table><p><strong>注意</strong>: 如果使用Python, 存储对象总是使用Pickle库进行序列化，所以它不关心你用何种serialized级别.</p><p>Spark也会在shuffle操作中存储一些中间对象(比如 reduceByKey), 甚至用户都没有调用persist. 这样做的目的是避免在一个节点失败后重新计算完整的输入。我们总是推荐用户持久化它们的RDD， 如果需要重用它们.</p><h4 id="存储级别的选择">存储级别的选择</h4><p>Spark的不同存储级别，旨在满足内存使用和CPU效率权衡上的不同需求。我们建议通过以下的步骤来进行选择：</p><ul><li>如果你的RDDs可以很好的与默认的存储级别(MEMORY_ONLY)契合，就不需要做任何修改了。这已经是CPU使用效率最高的选项，它使得RDDs的操作尽可能的快。</li><li>如果不行，试着使用MEMORY_ONLY_SER并且选择一个快速序列化的库使得对象在有比较高的空间使用率的情况下，依然可以较快被访问。</li><li>尽可能不要存储到硬盘上，除非计算数据集的函数，计算量特别大，或者它们过滤了大量的数据。否则，重新计算一个分区的速度，和与从硬盘中读取基本差不多快。</li><li>如果你想有快速故障恢复能力，使用复制存储级别(例如：用Spark来响应web应用的请求)。所有的存储级别都有通过重新计算丢失数据恢复错误的容错机制，但是复制存储级别可以让你在RDD上持续的运行任务，而不需要等待丢失的分区被重新计算。</li><li>如果你想要定义你自己的存储级别(比如复制因子为3而不是2)，可以使用StorageLevel 单例对象的apply()方法。</li></ul><p>在大内存或者多应用的环境中， 实验性的<code>OFF_HEAP</code>模式有几个好处：</p><ul><li>允许多个执行者共享Tachyon中的共享内存</li><li>显著的减少垃圾回收的消耗</li><li>单个executor崩溃时缓存数据不会丢失</li></ul><h4 id="移除数据">移除数据</h4><p>Spark 自动监控每个节点的缓存使用，依据LRU方式丢弃老的数据分区. 如果你想手工移除而不是等待cache移除机制，使用RDD.unpersist() 方法.</p><h2 id="共享变量">共享变量</h2><p>一般来说，当一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器，在远程机器上对变量的所有更新都不会被传播回驱动程序。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。</p><h3 id="广播变量_Broadcast_Variables">广播变量 Broadcast Variables</h3><p>广播变量允许程序员保留一个只读的变量，缓存在每一台机器上，而非每个任务保存一份拷贝。他们可以这样被使用，例如，以一种高效的方式给每个结点一个大的输入数据集。Spark会尝试使用一种高效的广播算法来传播广播变量，从而减少通信的代价。</p><p>广播变量是通过调用SparkContext.broadcast(v)方法从变量v创建的。广播变量是一个v的封装器，它的值可以通过调用value方法获得。如下模块展示了这个：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> broadcastVar = sc.broadcast(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</div><div class="line">broadcastVar: spark.Broadcast[Array[Int]] = spark.Broadcast(b5c40191-a864-<span class="number">4</span>c7d-b9bf-d87e1a4e787c)</div><div class="line"></div><div class="line">scala&gt; broadcastVar.value</div><div class="line">res0: Array[Int] = Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</div></pre></td></tr></table></figure><p>在广播变量被创建后，它应该在集群运行的任何函数中，代替v值被调用，从而v值不需要被再次传递到这些结点上。另外，对象v不能在广播后修改，这样可以保证所有结点的收到的都是一模一样的广播值。</p><h3 id="累加器_Accumulators">累加器 Accumulators</h3><p>累加器是一种只能通过关联操作进行“加”操作的变量，因此可以高效被并行支持。它们可以用来实现计数器（如MapReduce中）和求和器。Spark原生就支持Int和Double类型的累加器，开发者可以自己添加新的支持类型。</p><p>一个累加器可以通过调用SparkContext.accumulator(v)方法从一个初始值v中创建。运行在集群上的任务，可以通过使用+=来给它加值。然而，他们不能读取这个值。只有驱动程序可以使用value的方法来读取累加器的值。</p><p>如下的解释器模块，展示了如何利用累加器，将一个数组里面的所有元素相加：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">scala&gt; <span class="keyword">val</span> accum = sc.accumulator(<span class="number">0</span>, <span class="string">"My Accumulator"</span>)</div><div class="line">accum: spark.Accumulator[Int] = <span class="number">0</span></div><div class="line"></div><div class="line">scala&gt; sc.parallelize(Array(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)).foreach(x =&gt; accum += x)</div><div class="line">...</div><div class="line"><span class="number">10</span>/<span class="number">09</span>/<span class="number">29</span> <span class="number">18</span>:<span class="number">41</span>:<span class="number">08</span> INFO SparkContext: Tasks finished in <span class="number">0.317106</span> s</div><div class="line"></div><div class="line">scala&gt; accum.value</div><div class="line">res2: Int = <span class="number">10</span></div></pre></td></tr></table></figure><p>代码使用了内建的Int的累加器, 程序员也可以自己创建累加器接口AccumulatorParam的子类。. AccumulatorParam接口有两个方法， zero代表提供一个零值，addInPlace代表将两个值相加。 例如假定我们有一个Vector类，代表数学里的vector, 我们可以实现代码如下:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">VectorAccumulatorParam</span> <span class="keyword">extends</span> <span class="title">AccumulatorParam</span>[<span class="title">Vector</span>] </span>{</div><div class="line">  <span class="keyword">def</span> zero(initialValue: Vector): Vector = {</div><div class="line">    Vector.zeros(initialValue.size)</div><div class="line">  }</div><div class="line">  <span class="keyword">def</span> addInPlace(v1: Vector, v2: Vector): Vector = {</div><div class="line">    v1 += v2</div><div class="line">  }</div><div class="line">}</div><div class="line"></div><div class="line"><span class="comment">// Then, create an Accumulator of this type:</span></div><div class="line"><span class="keyword">val</span> vecAccum = sc.accumulator(<span class="keyword">new</span> Vector(...))(VectorAccumulatorParam)</div></pre></td></tr></table></figure><p>如果使用Scala, Spark也支持更通用的Accumulable接口， 可以累加不同类型的元素, SparkContext.accumulableCollection方法累加通用的collection类型.</p><h2 id="发布到集群中">发布到集群中</h2><p><a href="https://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">应用提交指南</a> 描述了如何提交应用到一个Spark集群中。简而言之，一旦你打包好你的应用（JAR for Java/Scala，或者一堆.py / .zip files for Python), bin/spark-submit脚本允许你提交它们到任意的cluster manager.</p><h2 id="单元测试">单元测试</h2><p>Spark很好的支持流行的单元测试框架。 简单的使用master URL为local创建一个SparkContext,执行你的操作然后调用 SparkContext.stop() 停止. 确保在finally块中停止context或者在单元测试框架的tearDown停止context, 因为Spark 不支持在同一个程序的同时拥有两个context.</p><h2 id="从1-0以前的版本升级">从1.0以前的版本升级</h2><p>Spark 1.0 为1.x系列版本冻结了API的改动， 没有API被标记为 “experimental” 或者 “developer API” 以在未来版本中支持。 对于Scala 用户唯一的改变是分组操作，如 groupByKey, cogroup 和 join, 返回结果类型需哦那个 (Key, Seq[Value]) 改为 (Key, Iterable[Value]).</p><p>升级指南请参照 <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x" target="_blank" rel="external">Streaming</a>, <a href="https://spark.apache.org/docs/latest/mllib-guide.html#migration-guide" target="_blank" rel="external">MLlib</a> and <a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html#migrating-from-spark-091" target="_blank" rel="external">GraphX</a>.</p><h2 id="更多资料">更多资料</h2><p>你可以在官方站点查看官方的例子 <a href="http://spark.apache.org/examples.html" target="_blank" rel="external">example</a>. 除此之外，Spark在发布包的examples的文件夹中包含了几个例子(<a href="https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples" target="_blank" rel="external">Scala</a>, <a href="https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples" target="_blank" rel="external">Java</a>, Python). 运行Java 和 Scala例子时你可以传递类名给Spark bin/run-example脚本， 例如:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/run-example SparkPi</div></pre></td></tr></table></figure><p>对于Python例子，使用spark-submit:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/spark-submit examples/src/main/python/pi.py</div></pre></td></tr></table></figure><p>想了解优化的方法, <a href="https://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="external">配置configuration</a> 和 <a href="https://spark.apache.org/docs/latest/tuning.html" target="_blank" rel="external">调优tuning</a> 指南提供了最佳实践. 特别重要的是你的数据要以有效的格式存储在内存中. 想了解发布信息， <a href="https://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="external">cluster mode overview</a> 描述了分布式操作和集群管理器支持的组件.</p><p>最后全部的API文档请访问 <a href="https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.package" target="_blank" rel="external">Scala</a>, <a href="https://spark.apache.org/docs/latest/api/java/" target="_blank" rel="external">Java</a> 和 <a href="https://spark.apache.org/docs/latest/api/python/" target="_blank" rel="external">Python</a>.</p></div><footer class="article-footer"><ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul><section id="comments"><script src="https://utteranc.es/client.js" repo="smallnest/gitalk" issue-term="title" theme="github-light" crossorigin="anonymous" async></script><noscript>为正常使用评论功能请激活JavaScript</noscript></section></footer></div><nav id="article-nav"><a href="/2014/12/09/spark-cluster-overview/" id="article-nav-newer" class="article-nav-link-wrap"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">Spark 集群模式概述</div></a> <a href="/2014/12/08/spark-quick-start/" id="article-nav-older" class="article-nav-link-wrap"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">Spark 快速入门</div></a></nav></article></section><aside id="sidebar"><div class="widget-wrap"><h3 class="widget-title">访问者地域</h3><div class="widget"><img width="100%" src="/images/widgets/gopatterns.jpg"></div></div><div class="widget-wrap"><h3 class="widget-title">微信公众号</h3><div class="widget"><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=Hf4EJSi2XvL6TMcuFSH51Qn6nf5nZ8qnjVBnWCQ4FGc"></script></div></div><div class="widget-wrap"><h3 class="widget-title">极客时间专栏</h3><div class="widget"><a href="https://time.geekbang.org/column/intro/100061801"><img width="100%" src="/images/widgets/geekbang.png"></a></div></div><div class="widget-wrap"><h3 class="widget-title">出版图书</h3><div class="widget"><a href="https://cpgo.colobu.com/"><img width="100%" src="/cpgolang/cpgo.png"></a></div><div class="widget"><a href="https://item.jd.com/14347716.html"><img width="100%" src="/100gomistakes/cover.png"></a></div><div class="widget"><a href="/ScalaCollectionsCookbook/"><img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook.jpg"> <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook_tw.png"></a></div></div><div class="widget-wrap"><h3 class="widget-title">分类</h3><div class="widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DOTNET/">DOTNET</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a><span class="category-list-count">283</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">64</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Rust/">Rust</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/go/">go</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/k8s/">k8s</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/rust/">rust</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/分享/">分享</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端开发/">前端开发</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/区块链/">区块链</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">28</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/架构/">架构</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/管理/">管理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络编程/">网络编程</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/设计模式/">设计模式</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/运维/">运维</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发编程/">高并发编程</a><span class="category-list-count">20</span></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">标签云</h3><div class="widget tagcloud"><a href="/tags/Android/" style="font-size: 15.71px">Android</a><a href="/tags/ApacheBench/" style="font-size: 11.43px">ApacheBench</a><a href="/tags/Bower/" style="font-size: 10.00px">Bower</a><a href="/tags/C/" style="font-size: 10.00px">C#</a><a href="/tags/CDN/" style="font-size: 10.00px">CDN</a><a href="/tags/CQRS/" style="font-size: 10.00px">CQRS</a><a href="/tags/CRC/" style="font-size: 10.00px">CRC</a><a href="/tags/CSS/" style="font-size: 11.43px">CSS</a><a href="/tags/CompletableFuture/" style="font-size: 10.00px">CompletableFuture</a><a href="/tags/Comsat/" style="font-size: 10.00px">Comsat</a><a href="/tags/Curator/" style="font-size: 18.57px">Curator</a><a href="/tags/DSL/" style="font-size: 10.00px">DSL</a><a href="/tags/Disruptor/" style="font-size: 10.00px">Disruptor</a><a href="/tags/Docker/" style="font-size: 11.43px">Docker</a><a href="/tags/Ember/" style="font-size: 11.43px">Ember</a><a href="/tags/FastJson/" style="font-size: 10.00px">FastJson</a><a href="/tags/Fiber/" style="font-size: 10.00px">Fiber</a><a href="/tags/GAE/" style="font-size: 10.00px">GAE</a><a href="/tags/GC/" style="font-size: 12.86px">GC</a><a href="/tags/Gnuplot/" style="font-size: 10.00px">Gnuplot</a><a href="/tags/Go/" style="font-size: 14.29px">Go</a><a href="/tags/Gradle/" style="font-size: 10.00px">Gradle</a><a href="/tags/Grunt/" style="font-size: 10.00px">Grunt</a><a href="/tags/Gulp/" style="font-size: 10.00px">Gulp</a><a href="/tags/Hadoop/" style="font-size: 10.00px">Hadoop</a><a href="/tags/Hazelcast/" style="font-size: 10.00px">Hazelcast</a><a href="/tags/IPFS/" style="font-size: 10.00px">IPFS</a><a href="/tags/Ignite/" style="font-size: 10.00px">Ignite</a><a href="/tags/JVM/" style="font-size: 10.00px">JVM</a><a href="/tags/Java/" style="font-size: 17.14px">Java</a><a href="/tags/Kafka/" style="font-size: 20.00px">Kafka</a><a href="/tags/Lambda/" style="font-size: 14.29px">Lambda</a><a href="/tags/Linux/" style="font-size: 12.86px">Linux</a><a href="/tags/LongAdder/" style="font-size: 10.00px">LongAdder</a><a href="/tags/MathJax/" style="font-size: 10.00px">MathJax</a><a href="/tags/Maven/" style="font-size: 11.43px">Maven</a><a href="/tags/Memcached/" style="font-size: 10.00px">Memcached</a><a href="/tags/Metrics/" style="font-size: 10.00px">Metrics</a><a href="/tags/Mongo/" style="font-size: 12.86px">Mongo</a><a href="/tags/Netty/" style="font-size: 15.71px">Netty</a></div></div><div class="widget-wrap"><h3 class="widget-title">归档</h3><div class="widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">October 2022</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/08/">August 2014</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/07/">July 2014</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">近期文章</h3><div class="widget"><ul><li><a href="/2024/06/04/redka-redis-with-sqlite/">Redka - 父亲是Redis，母亲是SQLite</a></li><li><a href="/2024/06/03/command-dispacher-pattern/">命令分发模式</a></li><li><a href="/2024/05/22/parse-tcp-timestamp-in-Rust/">使用Rust捕获和解析网络包</a></li><li><a href="/2024/05/20/implemenmt-pping-in-go/">使用Go语言实现 pping</a></li><li><a href="/2024/05/19/let-Rob-Pike-write-a-Red-Black-tree/">让 Rob Pike 或者字节跳动的同学实现一个红黑树</a></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">友情链接</h3><div class="widget"><ul><li><a href="http://stackshare.io" target="_blank">技术栈</a></li><li>&nbsp;</li><li><a href="https://toutiao.io/" target="_blank">开发者头条</a></li><li><a href="http://weekly.manong.io/issues/" target="_blank">码农周刊</a></li><li><a href="http://www.tuicool.com/mags" target="_blank">编程狂人周刊</a></li><li><a href="http://www.importnew.com/" target="_blank">importnew</a></li><li><a href="http://ifeve.com/" target="_blank">并发编程网</a></li><li>&nbsp;</li><li><a href="http://github.com" target="_blank">github</a></li><li><a href="http://stackoverflow.com/" target="_blank">stackoverflow</a></li><li><a href="http://www.javacodegeeks.com/" target="_blank">javacodegeeks</a></li><li><a href="http://www.infoq.com/" target="_blank">infoq</a></li><li><a href="http://www.dzone.com/links/index.html" target="_blank">dzone</a></li><li><a href="https://oj.leetcode.com/problems/" target="_blank">leetcode</a></li><li><a href="http://tutorials.jenkov.com" target="_blank">jenkov</a></li><li><a href="https://howtodoinjava.com" target="_blank">HowToDoInJava</a></li><li><a href="https://java-design-patterns.com/patterns/" target="_blank">java design patterns</a></li><li>&nbsp;</li><li><a href="https://medium.com/netflix-techblog" target="_blank">Netflix技术博客</a></li><li><a href="https://www.techiedelight.com" target="_blank">Techie Delight</a></li><li><a href="https://engineering.linkedin.com/blog" target="_blank">Linkedin技术博客</a></li><li><a href="https://blogs.dropbox.com/tech/" target="_blank">Dropbox技术博客</a></li><li><a href="https://code.fb.com" target="_blank">Facebook技术博客</a></li><li><a href="http://jm.taobao.org" target="_blank">淘宝中间件团队</a></li><li><a href="https://tech.meituan.com" target="_blank">美团技术博客</a></li><li><a href="http://blogs.360.cn" target="_blank">360技术博客</a></li><li><a href="https://xiaomi-info.github.io" target="_blank">小米信息部技术团队</a></li></ul></div></div></aside></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner">&copy; 2024 smallnest<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></div></div></footer></div><nav id="mobile-nav"><a href="/" class="mobile-nav-link"><i class="fa fa-home">&nbsp;</i>首页</a> <a href="/archives" class="mobile-nav-link"><i class="fa fa-folder-o">&nbsp;</i>归档</a> <a href="https://github.com/smallnest" class="mobile-nav-link"><i class="fa fa-github">&nbsp;</i>github</a> <a class="mobile-nav-link" href="#"><i class="fa fa-bars">&nbsp;</i>网站群</a> <a class="mobile-nav-link" href="/goasm">&nbsp;&nbsp;<i class="fa fa-language">&nbsp;</i>Go汇编示例</a> <a class="mobile-nav-link" href="https://gowebexamples.com">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go Web开发示例</a> <a class="mobile-nav-link" href="http://go-database-sql.org">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go 数据库开发教程</a> <a class="mobile-nav-link" href="https://colobu.com/gotips/">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go 语言编程技巧</a> <a class="mobile-nav-link" href="/perf-book">&nbsp;&nbsp;<i class="fa fa-rust">&nbsp;</i>Rust高性能编程指南</a> <a class="mobile-nav-link" href="/rust100">&nbsp;&nbsp;<i class="fa fa-rust">&nbsp;</i>100个练习题学习Rust</a> <a class="mobile-nav-link" href="http://rpcx.io">&nbsp;&nbsp;<i class="fa undefined">&nbsp;</i>RPCX官网</a> <a class="mobile-nav-link" href="http://cn.doc.rpcx.io">&nbsp;&nbsp;<i class="fa undefined">&nbsp;</i>RPC开发指南</a> <a href="/ScalaCollectionsCookbook" class="mobile-nav-link"><i class="fa fa-book">&nbsp;</i>Scala集合技术手册</a> <a href="/about" class="mobile-nav-link"><i class="fa fa-lemon-o">&nbsp;</i>关于</a></nav><script src="//cdn.staticfile.org/jquery/1.11.1/jquery.min.js"></script><script src="//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"></script><script src="/js/script.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"});</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.0-beta.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div id="totop" style="position:fixed;bottom:150px;right:10px;cursor: pointer;z-index: 2000"><a title="返回顶部"><img src="/images/scrollup.png"></a></div><script src="/js/totop.js" type="text/javascript"></script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?e085d87993250aab11f3e0c15f1c2785";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}();</script></div></body></html>