<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>机器学习算法精要(Python 和 R 代码) | 鸟窝</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="analyticsvidhya网站有一些非常好的机器学习的文章，这是其中的一篇。原文出处: Essentials of Machine Learning Algorithms,国内有人翻译了，可以移步：10 种机器学习算法的要点">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习算法精要(Python 和 R 代码)">
<meta property="og:url" content="http://colobu.com/2015/11/05/common-machine-learning-algorithms/">
<meta property="og:site_name" content="鸟窝">
<meta property="og:description" content="analyticsvidhya网站有一些非常好的机器学习的文章，这是其中的一篇。原文出处: Essentials of Machine Learning Algorithms,国内有人翻译了，可以移步：10 种机器学习算法的要点">
<meta property="og:image" content="1.jpg">
<meta property="og:image" content="2.png">
<meta property="og:image" content="3.png">
<meta property="og:image" content="4.png">
<meta property="og:image" content="5.jpg">
<meta property="og:image" content="6.png">
<meta property="og:image" content="7.png">
<meta property="og:image" content="8.png">
<meta property="og:image" content="9.png">
<meta property="og:image" content="10.png">
<meta property="og:image" content="11.jpg">
<meta property="og:image" content="12.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习算法精要(Python 和 R 代码)">
<meta name="twitter:description" content="analyticsvidhya网站有一些非常好的机器学习的文章，这是其中的一篇。原文出处: Essentials of Machine Learning Algorithms,国内有人翻译了，可以移步：10 种机器学习算法的要点">

  
    <link rel="alternative" href="/atom.xml" title="鸟窝" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  <link href="//cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">
  
  <link rel="stylesheet" href="//libs.cncdn.cn/fancybox/2.1.5/jquery.fancybox.css" media="screen" type="text/css">
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//cdn.staticfile.org/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->

  <script type="text/javascript">
	//visit my previous blog: http://old.colobu.com just like this http://colobu.com/?=123456
    var blog_url = location.href.toString();
	if (blog_url.indexOf("http://colobu.com/?p=") >= 0) {
		blog_url = blog_url.replace("colobu.com", "old.colobu.com");
		window.location.assign(blog_url);
	} else if (blog_url.indexOf("http://smallnest.gitcafe.com") >= 0) {
		blog_url = blog_url.replace("smallnest.gitcafe.com", "colobu.com");
		window.location.assign(blog_url);
	}  else if (blog_url.indexOf("http://smallnest.gitcafe.io") >= 0) {
		blog_url = blog_url.replace("smallnest.gitcafe.io", "colobu.com");
		window.location.assign(blog_url);
	}
</script>
  <style type="text/css">
    .news-essay
    {
      display: inline !important;
    }
    .hot-news
    {
      text-align: left !important;
    }
  </style>
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap" class="animated bounceInLeft">
        <a href="/" id="logo">鸟窝</a>
      </h1>
      
        <h2 id="subtitle-wrap" class="animated bounceInLeft">
          <a href="/" id="subtitle">大道至简 Simplicity is the ultimate form of sophistication</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/"><i class="fa fa-home">&nbsp;</i>首页</a>
        
          <a class="main-nav-link" href="/archives"><i class="fa fa-folder-o">&nbsp;</i>归档</a>
        
          <a class="main-nav-link" href="https://github.com/smallnest"><i class="fa fa-github">&nbsp;</i>github</a>
        
          <a class="main-nav-link" href="/goasm"><i class="fa fa-language">&nbsp;</i>Go汇编示例</a>
        
          <a class="main-nav-link" href="/ScalaCollectionsCookbook"><i class="fa fa-book">&nbsp;</i>Scala集合技术手册</a>
        
          <a class="main-nav-link" href="/techreview"><i class="fa fa-newspaper-o">&nbsp;</i>技术快报</a>
        
          <a class="main-nav-link" href="/about"><i class="fa fa-lemon-o">&nbsp;</i>关于</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="colobu.com">
        </form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-common-machine-learning-algorithms" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/11/05/common-machine-learning-algorithms/" class="article-date">
  <time datetime="2015-11-05T01:25:46.000Z" itemprop="datePublished">2015年11月05日</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>

	
  <div class="article-author"> by smallnest</div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      机器学习算法精要(Python 和 R 代码)
	  
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
	  
	  
      
        <blockquote>
<p><a href="http://www.analyticsvidhya.com" target="_blank" rel="external">analyticsvidhya</a>网站有一些非常好的机器学习的文章，这是其中的一篇。<br>原文出处: <a href="http://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/" target="_blank" rel="external">Essentials of Machine Learning Algorithms</a>,<br>国内有人翻译了，可以移步：<a href="http://blog.jobbole.com/92021/" target="_blank" rel="external">10 种机器学习算法的要点</a><br><a id="more"></a></p>
</blockquote>
<h3 id="Introduction">Introduction</h3>
<blockquote>
<p>Google’s self-driving cars and robots get a lot of press, but the company’s real future is in machine learning, the technology that enables computers to get smarter and more personal.</p>
<p>   – Eric Schmidt (Google Chairman)</p>
</blockquote>
<p>We are probably living in the most defining period of human history. The period when computing moved from large mainframes to PCs to cloud. But what makes it defining is not what has happened, but what is coming our way in years to come.</p>
<p>What makes this period exciting for some one like me is the democratization of the tools and techniques, which followed the boost in computing. Today, as a data scientist, I can build data crunching machines with complex algorithms for a few dollors per hour. But, reaching here wasn’t easy! I had my dark days and nights.</p>
<h3 id="Who_can_benefit_the_most_from_this_guide?">Who can benefit the most from this guide?</h3>
<p><strong>What I am giving out today is probably the most valuable guide, I have ever created.</strong><br>(这一篇指南，可能是我写过的最有价值的指南)</p>
<p>The idea behind creating this guide is to simplify the journey of aspiring data scientists and machine learning enthusiasts across the world. Through this guide, I will enable you to work on machine learning problems and gain from experience. <strong>I am providing a high level understanding about various machine learning algorithms along with R &amp; Python codes to run them. These should be sufficient to get your hands dirty.</strong><br><img src="1.jpg" alt=""><br>I have deliberately skipped the statistics behind these techniques, as you don’t need to understand them at the start. So, if you are looking for statistical understanding of these algorithms, you should look elsewhere. But, if you are looking to equip yourself to start building machine learning project, you are in for a treat.</p>
<h3 id="Broadly,_there_are_3_types_of_Machine_Learning_Algorithms-">Broadly, there are 3 types of Machine Learning Algorithms..</h3>
<p><strong>1 Supervised Learning</strong><br>监督式学习<br><strong>How it works</strong>: This algorithm consist of a target / outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data. Examples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc. (回归、决策树、随机森林、K–近邻算法、逻辑回归等)</p>
<p><strong>2 Unsupervised Learning</strong><br>非监督式学习<br><strong>How it works</strong>: In this algorithm, we do not have any target or outcome variable to predict / estimate.  It is used for clustering population in different groups, which is widely used for segmenting customers in different groups for specific intervention. Examples of Unsupervised Learning: Apriori algorithm, K-means. (关联算法, K–均值算法)</p>
<p><strong>3 Reinforcement Learning</strong><br>强化学习<br><strong>How it works</strong>:  Using this algorithm, the machine is trained to make specific decisions. It works this way: the machine is exposed to an environment where it trains itself continually using trial and error. This machine learns from past experience and tries to capture the best possible knowledge to make accurate business decisions. Example of Reinforcement Learning: Markov Decision Process</p>
<h3 id="List_of_Common_Machine_Learning_Algorithms">List of Common Machine Learning Algorithms</h3>
<p>Here is the list of commonly used machine learning algorithms. These algorithms can be applied to almost any data problem:</p>
<ol>
<li>Linear Regression (线性回归)</li>
<li>Logistic Regression (逻辑回归)</li>
<li>Decision Tree (决策树)</li>
<li>SVM (支持向量机)</li>
<li>Naive Bayes (朴素贝叶斯)</li>
<li>KNN (K最近邻算法)</li>
<li>K-Means (K均值算法)</li>
<li>Random Forest (随机森林算法)</li>
<li>Dimensionality Reduction Algorithms (降维算法)</li>
<li>Gradient Boost &amp; Adaboost</li>
</ol>
<h4 id="1_Linear_Regression">1 Linear Regression</h4>
<p>It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.</p>
<p>The best way to understand linear regression is to relive this experience of childhood. Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.</p>
<p>In this equation:</p>
<ul>
<li>Y – Dependent Variable</li>
<li>a – Slope</li>
<li>X – Independent variable</li>
<li>b – Intercept</li>
</ul>
<p>These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.</p>
<p>Look at the below example. Here we have identified the best fit line having linear equation <strong>y=0.2811x+13.9</strong>. Now using this equation, we can find the weight, knowing the height of a person.<br><img src="2.png" alt=""></p>
<p>Linear Regression is of mainly two types: Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression is characterized by one independent variable. And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. While finding best fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression.</p>
<p><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</div><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train=input_variables_values_training_datasets</div><div class="line">y_train=target_variables_values_training_datasets</div><div class="line">x_test=input_variables_values_test_datasets</div><div class="line"><span class="comment"># Create linear regression object</span></div><div class="line">linear = linear_model.LinearRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear.fit(x_train, y_train)</div><div class="line">linear.score(x_train, y_train)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, linear.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, linear.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= linear.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong><em>R</em></strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train &lt;- input_variables_values_training_datasets</div><div class="line">y_train &lt;- target_variables_values_training_datasets</div><div class="line">x_test &lt;- input_variables_values_test_datasets</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear &lt;- lm(y_train ~ ., data = x)</div><div class="line">summary(linear)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(linear,x_test)</div></pre></td></tr></table></figure>

<h4 id="2_Logistic_Regression">2 Logistic Regression</h4>
<p>Don’t get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as <strong>logit regression</strong>. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).</p>
<p>Again, let us try and understand this through a simple example.</p>
<p>Let’s say your friend gives you a puzzle to solve. There are only 2 outcome scenarios – either you solve it or you don’t. Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. The outcome to this study would be something like this – if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. This is what Logistic Regression provides you.</p>
<p>Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dds= p/ (<span class="number">1</span>-p) = probability <span class="keyword">of</span> event occurrence / probability <span class="keyword">of</span> <span class="keyword">not</span> event occurrence</div><div class="line"><span class="function"><span class="title">ln</span><span class="params">(odds)</span> = <span class="title">ln</span><span class="params">(p/(<span class="number">1</span>-p)</span>)</span></div><div class="line"><span class="title">logit</span><span class="params">(p)</span> = <span class="title">ln</span><span class="params">(p/(<span class="number">1</span>-p)</span>) = <span class="title">b0</span>+<span class="title">b1X1</span>+<span class="title">b2X2</span>+<span class="title">b3X3</span>....+<span class="title">bkXk</span></div></pre></td></tr></table></figure>

<p>Above, p is the probability of presence of the characteristic of interest. It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).</p>
<p>Now, you may ask, why take a log? For the sake of simplicity, let’s just say that this is one of the best mathematical way to replicate a step function. I can go in more details, but that will beat the purpose of this article.<br><img src="3.png" alt=""></p>
<p><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create logistic regression object</span></div><div class="line">model = LogisticRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, model.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, model.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">logistic &lt;- glm(y_train ~ ., data = x,family=<span class="string">'binomial'</span>)</div><div class="line">summary(logistic)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(logistic,x_test)</div></pre></td></tr></table></figure>

<p><strong>Furthermore...</strong></p>
<p>There are many different steps that could be tried in order to improve the model:</p>
<ul>
<li>including interaction terms</li>
<li>removing features</li>
<li>regularization techniques</li>
<li>using a non-linear model</li>
</ul>
<h4 id="3_Decision_Tree">3 Decision Tree</h4>
<p>This is one of my favorite algorithm and I use it quite frequently. It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. For more details, you can read: <a href="http://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="external">Decision Tree Simplified</a>.<br><img src="4.png" alt=""><br>In the image above, you can see that population is classified into four different groups based on multiple attributes to identify ‘if they will play or not’. To split the population into different heterogeneous groups, it uses various techniques like Gini, Information Gain, Chi-square, entropy.</p>
<p>The best way to understand how decision tree works, is to play Jezzball – a classic game from Microsoft (image below). Essentially, you have a room with moving walls and you need to create walls such that maximum area gets cleared off with out the balls.<br><img src="5.jpg" alt=""><br>So, every time you split the room with a wall, you are trying to create 2 different populations with in the same room. Decision trees work in very similar fashion by dividing a population in as different groups as possible.</p>
<p>More: <a href="http://www.analyticsvidhya.com/blog/2015/01/decision-tree-simplified/" target="_blank" rel="external">Simplified Version of Decision Tree Algorithms</a></p>
<p><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create tree object </span></div><div class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>) <span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  </span></div><div class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(rpart)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># grow tree </span></div><div class="line">fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure>

<h4 id="4_SVM_(Support_Vector_Machine)">4 SVM (Support Vector Machine)</h4>
<p>It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.</p>
<p>For example, if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as <strong>Support Vectors</strong>)</p>
<p><img src="6.png" alt=""></p>
<p>Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be farthest away.<br><img src="7.png" alt=""></p>
<p>In the example shown above, the line which splits the data into two differently classified groups is the black line, since the two closest points are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, that’s what class we can classify the new data as.</p>
<p>More: <a href="http://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/" target="_blank" rel="external">Simplified Version of Support Vector Machine</a></p>
<p><strong>Think of this algorithm as playing JezzBall in n-dimensional space. The tweaks in the game are:</strong></p>
<ul>
<li>You can draw lines / planes at any angles (rather than just horizontal or vertical as in classic game)</li>
<li>The objective of the game is to segregate balls of different colors in different rooms.</li>
<li>And the balls are not moving.</li>
</ul>
<p><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object </span></div><div class="line">model = svm.svc() <span class="comment"># there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-svm(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure>

<h4 id="5_Naive_Bayes">5 Naive Bayes</h4>
<p>It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.</p>
<p>Naive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.</p>
<p>Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:<br><img src="8.png" alt=""><br>Here,</p>
<ul>
<li>P(c|x) is the posterior probability of class (target) given predictor (attribute). </li>
<li>P(c) is the prior probability of class. </li>
<li>P(x|c) is the likelihood which is the probability of predictor given class. </li>
<li>P(x) is the prior probability of predictor.</li>
</ul>
<p><strong>Example</strong>: Let’s understand it using an example. Below I have a training data set of weather and corresponding target variable ‘Play’. Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.</p>
<p>Step 1: Convert the data set to frequency table</p>
<p>Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64.<br><img src="9.png" alt=""></p>
<p>Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction.</p>
<p><strong>Problem</strong>: Players will pay if weather is sunny, is this statement is correct?</p>
<p>We can solve it using above discussed method, so P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)</p>
<p>Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64</p>
<p>Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.</p>
<p>Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm is mostly used in text classification and with problems having multiple classes.</p>
<p><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes, Refer link</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="title">library</span>(e1071)</div><div class="line"><span class="title">x</span> &lt;- cbind(x_train,y_train)</div><div class="line"><span class="preprocessor"># Fitting model</span></div><div class="line"><span class="title">fit</span> &lt;-naiveBayes(y_train ~ ., <span class="typedef"><span class="keyword">data</span> = x)</span></div><div class="line"><span class="title">summary</span>(fit)</div><div class="line"><span class="preprocessor">#Predict Output </span></div><div class="line"><span class="title">predicted</span>= predict(fit,x_test)</div></pre></td></tr></table></figure>

<h4 id="6_KNN_(K-_Nearest_Neighbors)">6 KNN (K- Nearest Neighbors)</h4>
<p>It can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.</p>
<p>These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing KNN modeling.</p>
<p>More: <a href="http://Introduction to k-nearest neighbors : Simplified" target="_blank" rel="external">Introduction to k-nearest neighbors : Simplified.</a><br><img src="10.png" alt=""><br>KNN can easily be mapped to our real lives. If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!<br>Things to consider before selecting KNN:</p>
<ul>
<li>KNN is computationally expensive</li>
<li>Variables should be normalized else higher range variables can bias it</li>
<li>Works on pre-processing stage more before going for KNN like outlier, noise removal</li>
</ul>
<p><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">KNeighborsClassifier(n_neighbors=<span class="number">6</span>) <span class="comment"># default value for n_neighbors is 5</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="title">library</span>(knn)</div><div class="line"><span class="title">x</span> &lt;- cbind(x_train,y_train)</div><div class="line"><span class="preprocessor"># Fitting model</span></div><div class="line"><span class="title">fit</span> &lt;-knn(y_train ~ ., <span class="typedef"><span class="keyword">data</span> = x,k=5)</span></div><div class="line"><span class="title">summary</span>(fit)</div><div class="line"><span class="preprocessor">#Predict Output </span></div><div class="line"><span class="title">predicted</span>= predict(fit,x_test)</div></pre></td></tr></table></figure>

<h4 id="7_K-Means">7 K-Means</h4>
<p>It is a type of unsupervised algorithm which  solves the clustering problem. Its procedure follows a simple and easy  way to classify a given data set through a certain number of  clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups.</p>
<p>Remember figuring out shapes from ink blots? k means is somewhat similar this activity. You look at the shape and spread to decipher how many different clusters / population are present!<br><img src="11.jpg" alt=""></p>
<p><strong>How K-means forms cluster:</strong></p>
<ol>
<li>K-means picks k number of points for each cluster known as centroids.</li>
<li>Each data point forms a cluster with the closest centroids i.e. k clusters.</li>
<li>Finds the centroid of each cluster based on existing cluster members. Here we have new centroids.</li>
<li>As we have new centroids, repeat step 2 and 3. Find the closest distance for each data point from new centroids and get associated with new k-clusters. Repeat this process until convergence occurs i.e. centroids does not change.</li>
</ol>
<p><strong>How to determine value of K:</strong><br>In K-means, we have clusters and each cluster has its own centroid. Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster. Also, when the sum of square values for all the clusters are added, it becomes total within sum of square value for the cluster solution.</p>
<p>We know that as the number of cluster increases, this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k, and then much more slowly after that. Here, we can find the optimum number of cluster.<br><img src="12.png" alt=""><br><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"><span class="comment">#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">k_means = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">library(<span class="keyword">cluster</span>)</div><div class="line">fit &lt;- kmeans(X, 3) # 5 <span class="keyword">cluster</span> solution</div></pre></td></tr></table></figure>

<h4 id="8_Random_Forest">8 Random Forest</h4>
<p>Random Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest).</p>
<p>Each tree is planted &amp; grown as follows:</p>
<ol>
<li>If the number of cases in the training set is N, then sample of N cases is taken at random but with replacement. This sample will be the training set for growing the tree.</li>
<li>If there are M input variables, a number m&lt;&lt;M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.</li>
<li>Each tree is grown to the largest extent possible. There is no pruning.</li>
</ol>
<p>For more details on this algorithm, comparing with decision tree and tuning model parameters, I would suggest you to read these articles:</p>
<ol>
<li><a href="http://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/" target="_blank" rel="external">Introduction to Random forest – Simplified</a></li>
<li><a href="http://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/" target="_blank" rel="external">Comparing a CART model to Random Forest (Part 1)</a></li>
<li><a href="http://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/" target="_blank" rel="external">Comparing a Random Forest to a CART model (Part 2)</a></li>
<li><a href="http://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/" target="_blank" rel="external">Tuning the parameters of your Random Forest model</a></li>
</ol>
<p><strong>Python</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Random Forest object</span></div><div class="line">model= RandomForestClassifier()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="title">library</span>(randomForest)</div><div class="line"><span class="title">x</span> &lt;- cbind(x_train,y_train)</div><div class="line"><span class="preprocessor"># Fitting model</span></div><div class="line"><span class="title">fit</span> &lt;- randomForest(<span class="type">Species</span> ~ ., x,ntree=<span class="number">500</span>)</div><div class="line"><span class="title">summary</span>(fit)</div><div class="line"><span class="preprocessor">#Predict Output </span></div><div class="line"><span class="title">predicted</span>= predict(fit,x_test)</div></pre></td></tr></table></figure>

<h4 id="9_Dimensionality_Reduction_Algorithms">9 Dimensionality Reduction Algorithms</h4>
<p>In the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.</p>
<p>For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.</p>
<p>As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. How’d you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others.</p>
<p>To know more about this algorithms, you can read <a href="http://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/" target="_blank" rel="external">“Beginners Guide To Learn Dimension Reduction Techniques“</a>.</p>
<p><strong>Python  Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</div><div class="line"><span class="comment">#Assumed you have training and test data set as train and test</span></div><div class="line"><span class="comment"># Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</span></div><div class="line"><span class="comment"># For Factor analysis</span></div><div class="line"><span class="comment">#fa= decomposition.FactorAnalysis()</span></div><div class="line"><span class="comment"># Reduced the dimension of training dataset using PCA</span></div><div class="line">train_reduced = pca.fit_transform(train)</div><div class="line"><span class="comment">#Reduced the dimension of test dataset</span></div><div class="line">test_reduced = pca.transform(test)</div><div class="line"><span class="comment">#For more detail on this, please refer  this link.</span></div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="title">library</span><span class="params">(stats)</span></span></div><div class="line"><span class="title">pca</span> &lt;- <span class="title">princomp</span><span class="params">(train, cor = <span class="variable">TRUE</span>)</span></div><div class="line"><span class="title">train_reduced</span>  &lt;- <span class="title">predict</span><span class="params">(pca,train)</span></div><div class="line"><span class="title">test_reduced</span>  &lt;- <span class="title">predict</span><span class="params">(pca,test)</span></div></pre></td></tr></table></figure>

<h4 id="10_Gradient_Boosting_&amp;_AdaBoost">10 Gradient Boosting &amp; AdaBoost</h4>
<p>GBM &amp; AdaBoost are boosting algorithms used when we deal with plenty of data to make a prediction with high prediction power. Boosting is an ensemble learning algorithm which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix.</p>
<p>More: <a href="http://www.analyticsvidhya.com/blog/2015/05/boosting-algorithms-simplified/" target="_blank" rel="external">Know about Gradient and AdaBoost in detail</a></p>
<p><strong>Python Code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></div><div class="line">model= GradientBoostingClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">1.0</span>, max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>

<p><strong>R</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">library(caret)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fitControl &lt;- trainControl( <span class="keyword">method</span> = <span class="string">"repeatedcv"</span>, number = <span class="number">4</span>, repeats = <span class="number">4</span>)</div><div class="line">fit &lt;- train(y ~ ., data = x, <span class="keyword">method</span> = <span class="string">"gbm"</span>, trControl = fitControl,verbose = <span class="type">FALSE</span>)</div><div class="line">predicted= predict(fit,x_test,<span class="keyword">type</span>= <span class="string">"prob"</span>)[,<span class="number">2</span>]</div></pre></td></tr></table></figure>

<p>GradientBoostingClassifier and Random Forest are two different boosting tree classifier and often people ask about <a href="http://discuss.analyticsvidhya.com/t/what-is-the-fundamental-difference-between-randomforest-and-gradient-boosting-algorithms/2341" target="_blank" rel="external">the difference between these two algorithms</a>.</p>
<h4 id="End_Notes">End Notes</h4>
<p>By now, I am sure, you would have an idea of commonly used machine learning algorithms. My sole intention behind writing this article and providing the codes in R and Python is to get you started right away. If you are keen to master machine learning, start right away. Take up problems, develop a physical understanding of the process, apply these codes and see the fun!</p>
<p>Did you find this article useful ? Share your views and opinions in the comments section below.</p>

      
    </div>
    <footer class="article-footer">
	
<div class="bdsharebuttonbox"><a title="分享到新浪微博" class="bds_tsina" href="#" data-cmd="tsina"></a><a title="分享到微信" class="bds_weixin" href="#" data-cmd="weixin"></a><a title="分享到QQ空间" class="bds_qzone" href="#" data-cmd="qzone"></a><a title="分享到印象笔记" class="bds_evernotecn" href="#" data-cmd="evernotecn"></a><a title="分享到有道云笔记" class="bds_youdao" href="#" data-cmd="youdao"></a><a title="分享到百度云收藏" class="bds_bdysc" href="#" data-cmd="bdysc"></a><a class="bds_more" href="#" data-cmd="more"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
      

	  

      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2015/11/05/full-cheatsheet-machine-learning-algorithms/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          机器学习算法 Python&amp;R 速查表
        
      </div>
    </a>
  
  
    <a href="/2015/11/04/create-a-slack-bot-with-golang/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">使用Go开发一个 Slack 运维机器人</div>
    </a>
  
</nav>

  
</article>


<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
<section id="comments">
  <div id="gitalk-container"></div>  
  <script type="text/javascript">
    var gitalkOpts = {
    id: '2015/11/05/common-machine-learning-algorithms/',
    owner: 'smallnest',
    repo: 'gitalk',
    title: '机器学习算法精要(Python 和 R 代码)',
    body: 'http://colobu.com/2015/11/05/common-machine-learning-algorithms/',
    clientID: 'bc02724130ed5b7ee275',
    clientSecret: '68cb0bae2f93a8b88b09e0eb9b08c844b06a9047',
    admin: ['smallnest'],
    distractionFreeMode: false
  };

  const gitalk = new Gitalk(gitalkOpts)
  gitalk.render('gitalk-container')
	</script>
  <noscript> 为正常使用评论功能请激活JavaScript</noscript>
</section>  


</section>
        
          <aside id="sidebar">
  
      <div class="widget-wrap">
    <h3 class="widget-title">原创图书</h3>
    <div class="widget">
      <a href="/ScalaCollectionsCookbook/">
        <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook.jpg">
        <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook_tw.png">
      </a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DOTNET/">DOTNET</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a><span class="category-list-count">92</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端开发/">前端开发</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/架构/">架构</a><span class="category-list-count">24</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/管理/">管理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络编程/">网络编程</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/运维/">运维</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发编程/">高并发编程</a><span class="category-list-count">20</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Android/" style="font-size: 16.67px;">Android</a><a href="/tags/ApacheBench/" style="font-size: 11.67px;">ApacheBench</a><a href="/tags/Bower/" style="font-size: 10.00px;">Bower</a><a href="/tags/C/" style="font-size: 10.00px;">C#</a><a href="/tags/CDN/" style="font-size: 10.00px;">CDN</a><a href="/tags/CQRS/" style="font-size: 10.00px;">CQRS</a><a href="/tags/CRC/" style="font-size: 10.00px;">CRC</a><a href="/tags/CSS/" style="font-size: 11.67px;">CSS</a><a href="/tags/CompletableFuture/" style="font-size: 10.00px;">CompletableFuture</a><a href="/tags/Comsat/" style="font-size: 10.00px;">Comsat</a><a href="/tags/Curator/" style="font-size: 20.00px;">Curator</a><a href="/tags/DSL/" style="font-size: 10.00px;">DSL</a><a href="/tags/Disruptor/" style="font-size: 10.00px;">Disruptor</a><a href="/tags/Docker/" style="font-size: 11.67px;">Docker</a><a href="/tags/Ember/" style="font-size: 11.67px;">Ember</a><a href="/tags/FastJson/" style="font-size: 10.00px;">FastJson</a><a href="/tags/Fiber/" style="font-size: 10.00px;">Fiber</a><a href="/tags/GAE/" style="font-size: 10.00px;">GAE</a><a href="/tags/GC/" style="font-size: 13.33px;">GC</a><a href="/tags/Gnuplot/" style="font-size: 10.00px;">Gnuplot</a><a href="/tags/Go/" style="font-size: 13.33px;">Go</a><a href="/tags/Gradle/" style="font-size: 10.00px;">Gradle</a><a href="/tags/Grunt/" style="font-size: 10.00px;">Grunt</a><a href="/tags/Gulp/" style="font-size: 10.00px;">Gulp</a><a href="/tags/Hadoop/" style="font-size: 10.00px;">Hadoop</a><a href="/tags/Hazelcast/" style="font-size: 10.00px;">Hazelcast</a><a href="/tags/Ignite/" style="font-size: 10.00px;">Ignite</a><a href="/tags/JVM/" style="font-size: 10.00px;">JVM</a><a href="/tags/Java/" style="font-size: 18.33px;">Java</a><a href="/tags/Kafka/" style="font-size: 20.00px;">Kafka</a><a href="/tags/Lambda/" style="font-size: 15.00px;">Lambda</a><a href="/tags/Linux/" style="font-size: 13.33px;">Linux</a><a href="/tags/LongAdder/" style="font-size: 10.00px;">LongAdder</a><a href="/tags/MathJax/" style="font-size: 10.00px;">MathJax</a><a href="/tags/Maven/" style="font-size: 11.67px;">Maven</a><a href="/tags/Memcached/" style="font-size: 10.00px;">Memcached</a><a href="/tags/Metrics/" style="font-size: 10.00px;">Metrics</a><a href="/tags/Mongo/" style="font-size: 13.33px;">Mongo</a><a href="/tags/Netty/" style="font-size: 16.67px;">Netty</a><a href="/tags/Nginx/" style="font-size: 10.00px;">Nginx</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/08/">August 2014</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/07/">July 2014</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/01/31/benchmark-2018-spring-of-popular-rpc-frameworks/">流行的rpc框架benchmark 2018新春版</a>
          </li>
        
          <li>
            <a href="/2018/01/10/use-binary-package-in-go/">使用二进制形式发布go package</a>
          </li>
        
          <li>
            <a href="/2018/01/09/xtrabackup-full-increament-backup/">[转]Xtrabackup全量备份/增量备份脚本</a>
          </li>
        
          <li>
            <a href="/2017/12/29/best-practices-for-writing-high-performance-Go-code/">[转]编写高性能的Go代码的最佳实践</a>
          </li>
        
          <li>
            <a href="/2017/12/28/top-golang-articles-of-2017/">年终盘点！2017年超有价值的Golang文章</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
      <ul>
        
          <li>
			 
            <a href="http://old.colobu.com" target="_blank">以前的博客</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://stackshare.io" target="_blank">技术栈</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://weekly.manong.io/issues/" target="_blank">码农周刊</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.tuicool.com/mags" target="_blank">编程狂人周刊</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.importnew.com/" target="_blank">importnew</a>
			
          </li>
        
          <li>
			 
            <a href="http://ifeve.com/" target="_blank">并发编程网</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://github.com" target="_blank">github</a>
			
          </li>
        
          <li>
			 
            <a href="http://stackoverflow.com/" target="_blank">stackoverflow</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.javacodegeeks.com/" target="_blank">javacodegeeks</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.infoq.com/" target="_blank">infoq</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.dzone.com/links/index.html" target="_blank">dzone</a>
			
          </li>
        
          <li>
			 
            <a href="https://oj.leetcode.com/problems/" target="_blank">leetcode</a>
			
          </li>
        
          <li>
			 
            <a href="http://tutorials.jenkov.com" target="_blank">jenkov</a>
			
          </li>
        
      </ul>
    </div>
  </div>

  
      

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 smallnest<br>
	  Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="https://github.com/smallnest" class="mobile-nav-link">github</a>
  
    <a href="/goasm" class="mobile-nav-link">Go汇编示例</a>
  
    <a href="/ScalaCollectionsCookbook" class="mobile-nav-link">Scala集合技术手册</a>
  
    <a href="/techreview" class="mobile-nav-link">技术快报</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    
<script src="//cdn.staticfile.org/jquery/1.11.1/jquery.min.js"></script>
<script src="//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"></script>


<script src="/js/script.js" type="text/javascript"></script>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.0-beta.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div id="totop" style="position:fixed;bottom:150px;right:10px;cursor: pointer;z-index: 2000;">
	<a title="返回顶部"><img src="/images/scrollup.png"/></a>
</div>
<script src="/js/totop.js" type="text/javascript"></script>



<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-142561-4']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>


<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e085d87993250aab11f3e0c15f1c2785";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </div>
</body>
</html>