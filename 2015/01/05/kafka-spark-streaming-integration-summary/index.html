<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark Streaming 集成 Kafka 总结 | 鸟窝</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。
本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。当前的版本：

Spark: 1.2.0
Kafka: 0.8.1.1

Spark Streaming属于Spark的核心api，它">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Streaming 集成 Kafka 总结">
<meta property="og:url" content="http://colobu.com/2015/01/05/kafka-spark-streaming-integration-summary/">
<meta property="og:site_name" content="鸟窝">
<meta property="og:description" content="最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。
本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。当前的版本：

Spark: 1.2.0
Kafka: 0.8.1.1

Spark Streaming属于Spark的核心api，它">
<meta property="og:image" content="/images/logos/Spark.png">
<meta property="og:image" content="/images/logos/kafka.png">
<meta property="og:image" content="http://spark.apache.org/docs/latest/img/streaming-arch.png">
<meta property="og:image" content="https://spark.apache.org/docs/latest/img/streaming-flow.png">
<meta property="og:image" content="https://spark.apache.org/docs/latest/img/cluster-overview.png">
<meta property="og:image" content="http://kafka.apache.org/images/log_anatomy.png">
<meta property="og:image" content="http://kafka.apache.org/images/consumer-groups.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark Streaming 集成 Kafka 总结">
<meta name="twitter:description" content="最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。
本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。当前的版本：

Spark: 1.2.0
Kafka: 0.8.1.1

Spark Streaming属于Spark的核心api，它">

  
    <link rel="alternative" href="/atom.xml" title="鸟窝" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  <link href="//cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">
  
  <link rel="stylesheet" href="//libs.cncdn.cn/fancybox/2.1.5/jquery.fancybox.css" media="screen" type="text/css">
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//cdn.staticfile.org/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->

  <script type="text/javascript">
	//visit my previous blog: http://old.colobu.com just like this http://colobu.com/?=123456
    var blog_url = location.href.toString();
	if (blog_url.indexOf("http://colobu.com/?p=") >= 0) {
		blog_url = blog_url.replace("colobu.com", "old.colobu.com");
		window.location.assign(blog_url);
	} else if (blog_url.indexOf("http://smallnest.gitcafe.com") >= 0) {
		blog_url = blog_url.replace("smallnest.gitcafe.com", "colobu.com");
		window.location.assign(blog_url);
	}  else if (blog_url.indexOf("http://smallnest.gitcafe.io") >= 0) {
		blog_url = blog_url.replace("smallnest.gitcafe.io", "colobu.com");
		window.location.assign(blog_url);
	}
</script>
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap" class="animated bounceInLeft">
        <a href="/" id="logo">鸟窝</a>
      </h1>
      
        <h2 id="subtitle-wrap" class="animated bounceInLeft">
          <a href="/" id="subtitle">大道至简 Simplicity is the ultimate form of sophistication</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/"><i class="fa fa-home">&nbsp;</i>首页</a>
        
          <a class="main-nav-link" href="/archives"><i class="fa fa-folder-o">&nbsp;</i>归档</a>
        
          <a class="main-nav-link" href="https://github.com/smallnest"><i class="fa fa-github">&nbsp;</i>github</a>
        
          <a class="main-nav-link" href="http://tr.colobu.com"><i class="fa fa-spinner fa-pulse">&nbsp;</i>技术流</a>
        
          <a class="main-nav-link" href="/ScalaCollectionsCookbook"><i class="fa fa-book">&nbsp;</i>Scala集合技术手册</a>
        
          <a class="main-nav-link" href="/techreview"><i class="fa fa-newspaper-o">&nbsp;</i>技术快报</a>
        
          <a class="main-nav-link" href="/about"><i class="fa fa-lemon-o">&nbsp;</i>关于</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="colobu.com">
        </form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-kafka-spark-streaming-integration-summary" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/05/kafka-spark-streaming-integration-summary/" class="article-date">
  <time datetime="2015-01-05T08:04:57.000Z" itemprop="datePublished">2015年01月05日</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/大数据/">大数据</a>
  </div>

	
  <div class="article-author"> by smallnest</div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark Streaming 集成 Kafka 总结
	  
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
	  
	    <h1 id="expanderHead" style="cursor:pointer;">
		目录 <span id="expanderSign">[−]</span>
		</h1>
	    <div id="article-entry-toc" data-role="collapsible" class="article-entry-toc">
		  <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一个简单例子"><span class="toc-text">一个简单例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#核心概念"><span class="toc-text">核心概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从Kafka并行读取"><span class="toc-text">从Kafka并行读取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark并行处理"><span class="toc-text">Spark并行处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#注意事项"><span class="toc-text">注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#容错"><span class="toc-text">容错</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#worker节点失败"><span class="toc-text">worker节点失败</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#driver节点失败"><span class="toc-text">driver节点失败</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li></ol>
	    </div>
	  
	  
      
        <p><a rel="article0" href="/images/logos/Spark.png" title="" class="fancybox"><img src="/images/logos/Spark.png" alt="" align="left"></a><a rel="article0" href="/images/logos/kafka.png" title="" class="fancybox"><img src="/images/logos/kafka.png" alt="" align="left" style="margin-right: 30px;margin-left: 30px;"></a><br>最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。</p>
<p>本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。<br>当前的版本：</p>
<ul>
<li>Spark: 1.2.0</li>
<li>Kafka: 0.8.1.1</li>
</ul>
<p>Spark Streaming属于Spark的核心api，它支持高吞吐量、支持容错的实时流数据处理。 有以下特点:</p>
<ul>
<li><p>易于使用<br>提供了和批处理一致的高级操作API，可以进行map, reduce, join, window。</p>
</li>
<li><p>容错<br>Spark Streaming可以恢复你计算的状态， 包括lost work和operator state (比如 sliding windows)。 支持worker节点和driver 节点恢复。</p>
</li>
<li><p>Spark集成<br>可以结合批处理流和交互式查询。 可以重用批处理的代码。还可以直接使用内置的机器学习算法、图算法包来处理数据。<br>它可以接受来自文件系统, Akka actors, rsKafka, Flume, Twitter, ZeroMQ和TCP Socket的数据源或者你自己定义的输入源。</p>
<a id="more"></a>
<p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt=""></p>
</li>
</ul>
<p>它的工作流程像下面的图所示一样，接受到实时数据后，给数据分批次，然后传给Spark Engine处理最后生成该批次的结果流。<br><img src="https://spark.apache.org/docs/latest/img/streaming-flow.png" alt=""></p>
<p>Spark Streaming提供了一个高级的抽象模型，叫做<strong>discretized stream</strong>或者叫做<strong>DStream</strong>,它代表了一个持续的数据流。DStream既可以从Kafka, Flume, 和 Kinesis中产生, 或者在其它DStream上应用高级操作得到。 内部实现上一个DStream代表一个RDD序列。</p>
<h2 id="一个简单例子">一个简单例子</h2>
<p>在我们开始进入编写我们自己的Spark Streaming程序细节之前， 让我们先快速的看一个简单的Sparking Streaming程序是什么样子的。 这个程序接收网络发过来的文本数据，让我们统计一下文本中单词的数量。 全部代码如下：</p>
<div class="tabs">  
  <div class="tab">
    <input class="tab-radio" type="radio" id="tab-1" name="tab-group-1" checked>
    <label class="tab-label" for="tab-1">Scala</label>    
    <div class="tab-panel">
      <div class="tab-content">
首先, 我们导入Spark Streaming类名以及StreamingContext的一些隐式转换到我们的环境中， 这样可以为我们需要的类(比如DStream)增加一些有用的方法。. StreamingContext是所有功能的主入口。 我们创建了一个本地StreamingContext， 它使用两个线程， 批处理间隔为1秒.

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.StreamingContext._</div><div class="line"></div><div class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span></div><div class="line"><span class="comment">// The master requires 2 cores to prevent from a starvation scenario.</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">1</span>))</div></pre></td></tr></table></figure>

使用这个context, 我们可以创建一个DStream， 代表来自TCP源的流数据。需要指定主机名和端口(如 localhost 和 9999).

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></div><div class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</div></pre></td></tr></table></figure>

这一行代表从数据服务器接受到的数据流. DStream中每条记录是一行文本. 接下来, 我们想使用空格分隔每一行，这样就可以得到文本中的单词。

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Split each line into words</span></div><div class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</div></pre></td></tr></table></figure>

flatMap 是一个一对多的DStream操作， 它从源DStream中的每一个Record产生多个Record， 这些新产生的Record组成了一个新的DStream。 在我们的例子中， 每一行文本被分成了多个单词， 结果得到单词流DStream. 下一步， 我们想统计以下单词的数量.

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.StreamingContext._</div><div class="line"><span class="comment">// Count each word in each batch</span></div><div class="line"><span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></div><div class="line">wordCounts.print()</div></pre></td></tr></table></figure>

单词DStream 被mapped (one-to-one transformation) 成*(word, 1)对*的DStream ,然后reduced 得到每一批单词的频度. 
最后， wordCounts.print()会打印出每一秒产生的一些单词的统计值。

注意当这些行执行时，Spark Streaming仅仅设置这些计算， 它并没有马上被执行。 当所有的计算设置完后，我们可以调用下面的代码启动处理

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ssc.start()             <span class="comment">// Start the computation</span></div><div class="line">ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></div></pre></td></tr></table></figure>

完整的代码可以在例子 [NetworkWordCount](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala) 中找到.

如果你已经下载并编译了Spark, 你可以按照下面的命令运行例子. 你要先运行Netcat工具作为数据服务器

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ nc -lk <span class="number">9999</span></div></pre></td></tr></table></figure>

然后, 在另一个终端中, 你可以启动例子

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/run-example streaming.NetworkWordCount localhost <span class="number">9999</span></div></pre></td></tr></table></figure>

然后, 在netcat服务器中输入的每一行都会被统计，然后统计结果被输出到屏幕上。 类似下面的输出

<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># TERMINAL 2: RUNNING NetworkWordCount</span></div><div class="line"></div><div class="line">$ ./bin/run-example streaming.NetworkWordCount localhost <span class="number">9999</span></div><div class="line">...</div><div class="line">-------------------------------------------</div><div class="line">Time: <span class="number">1357008430000</span> ms</div><div class="line">-------------------------------------------</div><div class="line">(hello,<span class="number">1</span>)</div><div class="line">(world,<span class="number">1</span>)</div><div class="line">...</div></pre></td></tr></table></figure>

      </div>
    </div> 
  </div>
  
  <div class="tab">
    <input class="tab-radio" type="radio" id="tab-2" name="tab-group-1">
    <label class="tab-label" for="tab-2">Java</label>    
    <div class="tab-panel">
      <div class="tab-content">
首先我们创建一个JavaStreamingContext对象， 它是处理流的功能的主入口. 我们创建了一个本地的StreamingContext， 使用两个线程, 批处理间隔为1秒.

<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.*;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.*;</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.*;</div><div class="line"><span class="keyword">import</span> scala.Tuple2;</div><div class="line"></div><div class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second</span></div><div class="line">SparkConf conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</div><div class="line">JavaStreamingContext jssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">1</span>))</div></pre></td></tr></table></figure>

使用这个context, 我们可以创建一个DStream， 代表来自TCP源的流数据。需要指定主机名和端口(如 localhost 和 9999).

<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></div><div class="line">JavaReceiverInputDStream&lt;String&gt; lines = jssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>);</div></pre></td></tr></table></figure>

这一行代表从数据服务器接受到的数据流. DStream中每条记录是一行文本. 接下来, 我们想使用空格分隔每一行，这样就可以得到文本中的单词。

<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Split each line into words</span></div><div class="line">JavaDStream&lt;String&gt; words = lines.flatMap(</div><div class="line">  <span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() {</div><div class="line">    <span class="annotation">@Override</span> <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span>(String x) {</div><div class="line">      <span class="keyword">return</span> Arrays.asList(x.split(<span class="string">" "</span>));</div><div class="line">    }</div><div class="line">  });</div></pre></td></tr></table></figure>

flatMap 是一个一对多的DStream操作， 它从源DStream中的每一个Record产生多个Record， 这些新产生的Record组成了一个新的DStream。 在我们的例子中， 每一行文本被分成了多个单词， 结果得到单词流DStream. 
下一步， 我们想统计以下单词的数量.

<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Count each word in each batch</span></div><div class="line">JavaPairDStream&lt;String, Integer&gt; pairs = words.map(</div><div class="line">  <span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() {</div><div class="line">    <span class="annotation">@Override</span> <span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span>(String s) <span class="keyword">throws</span> Exception {</div><div class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, <span class="number">1</span>);</div><div class="line">    }</div><div class="line">  });</div><div class="line">JavaPairDStream&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(</div><div class="line">  <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() {</div><div class="line">    <span class="annotation">@Override</span> <span class="keyword">public</span> Integer <span class="title">call</span>(Integer i1, Integer i2) <span class="keyword">throws</span> Exception {</div><div class="line">      <span class="keyword">return</span> i1 + i2;</div><div class="line">    }</div><div class="line">  });</div><div class="line"></div><div class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></div><div class="line">wordCounts.print();</div></pre></td></tr></table></figure>


单词DStream 被mapped (one-to-one transformation) 成*(word, 1)对*的DStream ,然后reduced 得到每一批单词的频度. 
最后， wordCounts.print()会打印出每一秒产生的一些单词的统计值。

注意当这些行执行时，Spark Streaming仅仅设置这些计算， 它并没有马上被执行。 当所有的计算设置完后，我们可以调用下面的代码启动处理

<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">jssc.start();              <span class="comment">// Start the computation</span></div><div class="line">jssc.awaitTermination();   <span class="comment">// Wait for the computation to terminate</span></div></pre></td></tr></table></figure>


完整的的代码看例子 [JavaNetworkWordCount](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java).
如果你已经下载并编译了Spark, 你可以按照下面的命令运行例子. 你要先运行Netcat工具作为数据服务器

<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ nc -lk <span class="number">9999</span></div></pre></td></tr></table></figure>


然后, 在另一个终端中, 你可以启动例子

<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="number">9999</span></div></pre></td></tr></table></figure>

然后, 在netcat服务器中输入的每一行都会被统计，然后统计结果被输出到屏幕上。 类似下面的输出

<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># TERMINAL 2: RUNNING JavaNetworkWordCount</span></div><div class="line"></div><div class="line">$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="number">9999</span></div><div class="line">...</div><div class="line">-------------------------------------------</div><div class="line">Time: <span class="number">1357008430000</span> ms</div><div class="line">-------------------------------------------</div><div class="line">(hello,<span class="number">1</span>)</div><div class="line">(world,<span class="number">1</span>)</div><div class="line">...</div></pre></td></tr></table></figure>

      </div>
    </div> 
  </div>
</div>


<h2 id="核心概念">核心概念</h2>
<p>本节介绍一些Spark和Kafka的概念<br><i class="fa fa-cog"></i> <strong>Spark cluster</strong>:<br>一个Spark集群至少包含一个worker节点。<br><img src="https://spark.apache.org/docs/latest/img/cluster-overview.png" alt=""></p>
<p><i class="fa fa-cog"></i> <strong>worker node</strong>:<br>一个工作节点可以执行一个或者多个executor.</p>
<p><i class="fa fa-cog"></i> <strong>executor</strong>:<br>executor就是一个进程， 负责启在一个worker节点上启动应用，运行task执行计算，存储数据到内存或者磁盘上。 每个Spark应用都有自己的executor。一个executor拥有一定数量的cores, 也被叫做“slots”， 可以执行指派给它的task。</p>
<p><i class="fa fa-cog"></i> <strong>job</strong>:<br>一个并行的计算单元，包含多个task。 在执行Spark action (比如 save, collect)产生; 在log中可以看到这个词。</p>
<p><i class="fa fa-cog"></i> <strong>task</strong>:<br>一个task就是一个工作单元， 可以发送给一个executor执行。 它执行你的应用的实际计算的部分工作。 每个task占用父executor的一个slot (core)。</p>
<p><i class="fa fa-cog"></i> <strong>stage</strong>:<br>每个job都被分隔成多个彼此依赖称之为stage的task(类似MapReduce中的map 和 reduce stage);</p>
<p><i class="fa fa-cog"></i> <strong>共享变量</strong>: 普通可序列化的变量复制到远程各个节点。在远程节点上的更新并不会返回到原始节点。因为我们需要共享变量。 Spark提供了两种类型的共享变量。</p>
<pre><code><span class="keyword">*</span> Broadcast 变量。  SparkContext.broadcast(v)通过创建， <span class="keyword">*</span><span class="keyword">*</span>只读<span class="keyword">*</span><span class="keyword">*</span>。
<span class="keyword">*</span> Accumulator: 累加器，通过SparkContext.accumulator(v)创建，在任务中只能调用add或者+操作，不能读取值。只有驱动程序才可以读取值。
</code></pre><p><i class="fa fa-cog"></i> <strong>receiver</strong>:<br>receiver长时间（可能7*24小时）运行在executor。 每个receiver负责一个 input DStream (例如 一个 读取Kafka消息的input stream)。 每个receiver， 加上input DStream会占用一个core/slot.</p>
<p><i class="fa fa-cog"></i> <strong>input DStream</strong>:<br>一个input DStream是一个特殊的DStream， 将Spark Streaming连接到一个外部数据源来读取数据。 </p>
<p><i class="fa fa-cog"></i> <strong>kafka topic</strong>:<br>topic是发布消息发布的category 或者 feed名. 对于每个topic, Kafka管理一个分区的log，如下图所示：<br><img src="http://kafka.apache.org/images/log_anatomy.png" alt=""><br>分区内的消息都是有序不可变的。</p>
<p><i class="fa fa-cog"></i> <strong>kafka partition</strong>:<br>partitions的设计目的有多个.最根本原因是kafka基于文件存储.通过分区,可以将日志内容分散到多个server上,来避免文件尺寸达到单机磁盘的上限,每个partiton都会被当前server(kafka实例)保存;可以将一个topic切分多任意多个partitions(备注:基于sharding),来消息保存/消费的效率.此外越多的partitions意味着可以容纳更多的consumer,有效提升并发消费的能力.</p>
<p><i class="fa fa-cog"></i> <strong>kafka consumer group</strong>:<br>在kafka中，每个消费者要标记自己在那个组中。<br>如果所有的消费者都在同一个组中，则类似传统的queue消息模式，消息只发给一个消费者。<br>如果消费者都在不同的组中， 则类似发布-订阅消息模式。 每个消费者都会得到所有的消息。<br>最通用的模式是混用这两种模式，如下图：<br><img src="http://kafka.apache.org/images/consumer-groups.png" alt=""></p>
<p>关于kafka和消费者线程， 遵循下面的约束：<br>如果你的消费者读取包含10个分区的 <code>test</code>的topic,</p>
<ul>
<li>如果你配置你的消费者只使用1个线程， 则它负责读取十个分区</li>
<li>如果你配置你的消费者只使用5个线程， 则每个线程负责读取2个分区</li>
<li>如果你配置你的消费者只使用10个线程， 则每个线程负责读取1个分区</li>
<li>如果你配置你的消费者只使用14个线程， 则10个线程各负责读取1个分区,4个空闲</li>
<li>如果你配置你的消费者只使用8个线程，  则6个线程个负责一个分区，2个线程各负责2个分区</li>
</ul>
<h2 id="从Kafka并行读取">从Kafka并行读取</h2>
<p>有几种方法可以并行的读取Kafka的消息。</p>
<p>Spark的KafkaInputDStream (也叫做Kafka “connector”)使用 Kafka high-level consumer API读取数据，所以有两种方式可以并行的读取数据。</p>
<ul>
<li>多个input DStream： Spark为每个input dstream运行一个receiver. 这意味着多个input dstream可以运行在多个core上并行读取。 如果它们使用相同的topic,则相当于一个load balancer， 一个时间点上只有一个receiver读取。 如果不同的topic，可以同时读取。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ??? <span class="comment">// ignore for now</span></div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"test"</span>, <span class="comment">/* ignore rest */</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> numInputDStreams = <span class="number">5</span></div><div class="line"><span class="keyword">val</span> kafkaDStreams = (<span class="number">1</span> to numInputDStreams).map { _ =&gt; KafkaUtils.createStream(...) }</div></pre></td></tr></table></figure>

<ul>
<li>每个input dstream的消费者线程数。 同一个receiver可以运行多个线程。 可以配置和分区相同的线程。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ??? <span class="comment">// ignore for now</span></div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"terran"</span>, ...)</div><div class="line"></div><div class="line"><span class="keyword">val</span> consumerThreadsPerInputDstream = <span class="number">3</span></div><div class="line"><span class="keyword">val</span> topics = Map(<span class="string">"test"</span> -&gt; consumerThreadsPerInputDstream)</div><div class="line"><span class="keyword">val</span> stream = KafkaUtils.createStream(ssc, kafkaParams, topics, ...)</div></pre></td></tr></table></figure>

<p>或者你还可以混合这两种情况：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ???</div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"terran"</span>, ...)</div><div class="line"></div><div class="line"><span class="keyword">val</span> numDStreams = <span class="number">5</span></div><div class="line"><span class="keyword">val</span> topics = Map(<span class="string">"zerg.hydra"</span> -&gt; <span class="number">1</span>)</div><div class="line"><span class="keyword">val</span> kafkaDStreams = (<span class="number">1</span> to numDStreams).map { _ =&gt;</div><div class="line">    KafkaUtils.createStream(ssc, kafkaParams, topics, ...)</div><div class="line">  }</div></pre></td></tr></table></figure>

<h2 id="Spark并行处理">Spark并行处理</h2>
<p>上面介绍了Kafka的并行化读取的控制，在Spark中我们可以进行并行化处理。类似Kafka，Spark将parallelism设置的与（RDD）分区数量有关， 通过在每个RDD分区上运行task进行。在有些文档中，分区仍然被称为“slices”。<br>同样两个控制手段：</p>
<ul>
<li>input DStreams的数量</li>
<li>DStream transformation的重分配(repartition): 这里将获得一个全新的DStream，其parallelism等级可能增加、减少，或者保持原样。在DStream中每个返回的RDD都有指定的N个分区。DStream由一系列的RDD组成，DStream.repartition则是通过RDD.repartition实现。<br>因此，repartition是从processing parallelism分隔read parallelism的主要途径。在这里，我们可以设置processing tasks的数量，也就是说设置处理过程中所有core的数量。间接上，我们同样设置了投入machines/NICs的数量。</li>
</ul>
<p>一个DStream转换相关是 union。这个方法同样在StreamingContext中，它将从多个DStream中返回一个统一的DStream，它将拥有相同的类型和滑动时间。union会将多个 DStreams压缩到一个 DStreams或者RDD中，但是需要注意的是，这里的parallelism并不会发生改变。</p>
<p>你的用例将决定你如何分区。如果你的用例是CPU密集型的，你希望对test topic进行5 read parallelism读取。也就是说，每个消费者进程使用5个receiver，但是却可以将processing parallelism提升到20。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ???</div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"terran"</span>, ...)</div><div class="line"><span class="keyword">val</span> readParallelism = <span class="number">5</span></div><div class="line"><span class="keyword">val</span> topics = Map(<span class="string">"test"</span> -&gt; <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> kafkaDStreams = (<span class="number">1</span> to readParallelism).map { _ =&gt;</div><div class="line">    KafkaUtils.createStream(ssc, kafkaParams, topics, ...)</div><div class="line">  }</div><div class="line"><span class="comment">//&gt; collection of five *input* DStreams = handled by five receivers/tasks</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> unionDStream = ssc.union(kafkaDStreams) <span class="comment">// often unnecessary, just showcasing how to do it</span></div><div class="line"><span class="comment">//&gt; single DStream</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> processingParallelism = <span class="number">20</span></div><div class="line"><span class="keyword">val</span> processingDStream = unionDStream(processingParallelism)</div><div class="line"><span class="comment">//&gt; single DStream but now with 20 partitions</span></div></pre></td></tr></table></figure>

<h2 id="注意事项">注意事项</h2>
<p>在对Kafka进行读写上仍然存在一些含糊不清的问题，你可以在类似 <a href="http://apache-spark-user-list.1001560.n3.nabble.com/Multiple-Kafka-Receivers-and-Union-td14901.html" target="_blank" rel="external">Multiple Kafka Receivers and Union</a>和 <a href="http://apache-spark-user-list.1001560.n3.nabble.com/How-to-scale-more-consumer-to-Kafka-stream-td13883.html" target="_blank" rel="external">How to scale more consumer to Kafka stream mailing list</a>的讨论中发现。</p>
<ul>
<li><p>Spark 1.1并不会恢复那些已经接收却没有进行处理的原始数据（<a href="https://www.mail-archive.com/user@spark.apache.org/msg10572.html" target="_blank" rel="external">查看</a>）。因此，在某些情况下，你的Spark可能会丢失数据。Tathagata Das指出驱动恢复问题会在Spark的1.2版本中解决，现在已经提供<a href="https://github.com/apache/spark/blob/4b4b50c9e596673c1534df97effad50d107a8007/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/ReliableKafkaReceiver.scala" target="_blank" rel="external">Reliable Receiver</a> 和Unreliable Receiver两种Receiver。</p>
</li>
<li><p>1.1版本中的Kafka连接器是基于Kafka的高级消费者API。这样就会造成一个问题，Spark Streaming不可以依赖其自身的KafkaInputDStream将数据从Kafka中重新发送，从而无法解决下游数据丢失问题（比如Spark服务器发生故障）。<br>Dibyendu Bhattacharya 实现了使用简单消费者API: <a href="https://github.com/dibbhatt/kafka-spark-consumer" target="_blank" rel="external">kafka-spark-consumer</a>.</p>
</li>
<li><p>使用最新的Spark和Kafka,一些bugs已经在最新的Spark和Kafka中修复。</p>
</li>
<li><p>在使用window操作时，window duration和sliding duration必须是DStream批处理的duration的整数倍。</p>
</li>
<li><p>如果分配给应用的core的数量小于或者等于input DStream/receiver数量，则系统只接收数据， 没有额外的core处理数据</p>
</li>
<li><p>接上一条， 你在本地进行测试的时候，如果将master URL设置为“local”的话，则只有一个core运行任务，这明显违反上一条， 只能接收数据，无法处理。</p>
</li>
<li><p>Kafak Topic 的分区和 Spark RDD的分区没有任何关系。 它俩是分别设置的。</p>
</li>
</ul>
<h2 id="容错">容错</h2>
<p>有两种情况的机器失败。</p>
<h3 id="worker节点失败">worker节点失败</h3>
<p>receiver接收到的消息在集群间有备份。如果只是一个节点失败， Spark可以恢复。<br>但是如果是receiver所在的那个节点失败，可能会有一点点数据丢失。 但是Receiver可以在其它节点上恢复启动，继续接收数据。</p>
<h3 id="driver节点失败">driver节点失败</h3>
<p>如果7*24工作的应用， 如果driver节点失败，Spark Streaming也可以恢复。 Spark streaming定期的把元数据写到HDFS中。 你需要设置checkpoint 文件夹。<br>为了支持恢复，必须遵循下面的处理：</p>
<ol>
<li>当应用首次启动时， 它会创建一个新的StreamingContext, 设置所有的流，然后启动start().</li>
<li>当应用因失败而恢复时， 它会从checkpoint文件中的checkpoint重建StreamingContext.</li>
</ol>
<p>就像这样：</p>
<div class="tabs">  
  <div class="tab">
    <input class="tab-radio" type="radio" id="tab-3" name="tab-group-2" checked>
    <label class="tab-label" for="tab-3">Scala</label>    
    <div class="tab-panel">
      <div class="tab-content">

<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Function to create and setup a new StreamingContext</span></div><div class="line"><span class="keyword">def</span> functionToCreateContext(): StreamingContext = {</div><div class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> StreamingContext(...)   <span class="comment">// new context</span></div><div class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(...) <span class="comment">// create DStreams</span></div><div class="line">    ...</div><div class="line">    ssc.checkpoint(checkpointDirectory)   <span class="comment">// set checkpoint directory</span></div><div class="line">    ssc</div><div class="line">}</div><div class="line"></div><div class="line"><span class="comment">// Get StreamingContext from checkpoint data or create a new one</span></div><div class="line"><span class="keyword">val</span> context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)</div><div class="line"></div><div class="line"><span class="comment">// Do additional setup on context that needs to be done,</span></div><div class="line"><span class="comment">// irrespective of whether it is being started or restarted</span></div><div class="line">context. ...</div><div class="line"></div><div class="line"><span class="comment">// Start the context</span></div><div class="line">context.start()</div><div class="line">context.awaitTermination()</div></pre></td></tr></table></figure>

      </div>
    </div> 
  </div>
<div class="tab">
    <input class="tab-radio" type="radio" id="tab-4" name="tab-group-2">
    <label class="tab-label" for="tab-4">Java</label>    
    <div class="tab-panel">
      <div class="tab-content">	  

<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a factory object that can create a and setup a new JavaStreamingContext</span></div><div class="line">JavaStreamingContextFactory contextFactory = <span class="keyword">new</span> JavaStreamingContextFactory() {</div><div class="line">  <span class="annotation">@Override</span> <span class="keyword">public</span> JavaStreamingContext <span class="title">create</span>() {</div><div class="line">    JavaStreamingContext jssc = <span class="keyword">new</span> JavaStreamingContext(...);  <span class="comment">// new context</span></div><div class="line">    JavaDStream&lt;String&gt; lines = jssc.socketTextStream(...);     <span class="comment">// create DStreams</span></div><div class="line">    ...</div><div class="line">    jssc.checkpoint(checkpointDirectory);                       <span class="comment">// set checkpoint directory</span></div><div class="line">    <span class="keyword">return</span> jssc;</div><div class="line">  }</div><div class="line">};</div><div class="line"></div><div class="line"><span class="comment">// Get JavaStreamingContext from checkpoint data or create a new one</span></div><div class="line">JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);</div><div class="line"></div><div class="line"><span class="comment">// Do additional setup on context that needs to be done,</span></div><div class="line"><span class="comment">// irrespective of whether it is being started or restarted</span></div><div class="line">context. ...</div><div class="line"></div><div class="line"><span class="comment">// Start the context</span></div><div class="line">context.start();</div><div class="line">context.awaitTermination();</div></pre></td></tr></table></figure>

     </div>
   </div> 
 </div>
</div>

<h2 id="参考">参考</h2>
<ol>
<li><a href="http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/" target="_blank" rel="external">http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/</a></li>
<li><a href="https://spark.apache.org/docs/1.2.0/streaming-kafka-integration.html" target="_blank" rel="external">https://spark.apache.org/docs/1.2.0/streaming-kafka-integration.html</a></li>
<li><a href="https://spark.apache.org/docs/1.2.0/streaming-programming-guide.html" target="_blank" rel="external">https://spark.apache.org/docs/1.2.0/streaming-programming-guide.html</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
	
<div class="bdsharebuttonbox"><a title="分享到新浪微博" class="bds_tsina" href="#" data-cmd="tsina"></a><a title="分享到微信" class="bds_weixin" href="#" data-cmd="weixin"></a><a title="分享到QQ空间" class="bds_qzone" href="#" data-cmd="qzone"></a><a title="分享到印象笔记" class="bds_evernotecn" href="#" data-cmd="evernotecn"></a><a title="分享到有道云笔记" class="bds_youdao" href="#" data-cmd="youdao"></a><a title="分享到百度云收藏" class="bds_bdysc" href="#" data-cmd="bdysc"></a><a class="bds_more" href="#" data-cmd="more"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
      

	  
        <a href="http://colobu.com/2015/01/05/kafka-spark-streaming-integration-summary/#comments" class="article-comment-link">评论</a>
      

      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2015/01/07/Protobuf-language-guide/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [译]Protobuf 语法指南
        
      </div>
    </a>
  
  
    <a href="/2015/01/04/How-to-Leak-a-Context-Handlers-Inner-Classes/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Android中Handler引起的内存泄露</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div class="ds-thread" data-thread-key="2015/01/05/kafka-spark-streaming-integration-summary/" data-title="Spark Streaming 集成 Kafka 总结" data-url="http://colobu.com/2015/01/05/kafka-spark-streaming-integration-summary/"></div>

</section>

</section>
        
          <aside id="sidebar">
  
      <div class="widget-wrap">
    <h3 class="widget-title">原创图书</h3>
    <div class="widget">
      <a href="/ScalaCollectionsCookbook/">
        <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook.jpg">
        <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook_tw.png">
      </a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DOTNET/">DOTNET</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a><span class="category-list-count">49</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">58</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端开发/">前端开发</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">54</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/架构/">架构</a><span class="category-list-count">21</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/管理/">管理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络编程/">网络编程</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/运维/">运维</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发编程/">高并发编程</a><span class="category-list-count">20</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Android/" style="font-size: 15.71px;">Android</a><a href="/tags/ApacheBench/" style="font-size: 11.43px;">ApacheBench</a><a href="/tags/Bower/" style="font-size: 10.00px;">Bower</a><a href="/tags/C/" style="font-size: 10.00px;">C#</a><a href="/tags/CDN/" style="font-size: 10.00px;">CDN</a><a href="/tags/CQRS/" style="font-size: 10.00px;">CQRS</a><a href="/tags/CRC/" style="font-size: 10.00px;">CRC</a><a href="/tags/CSS/" style="font-size: 11.43px;">CSS</a><a href="/tags/CompletableFuture/" style="font-size: 10.00px;">CompletableFuture</a><a href="/tags/Comsat/" style="font-size: 10.00px;">Comsat</a><a href="/tags/Curator/" style="font-size: 20.00px;">Curator</a><a href="/tags/DSL/" style="font-size: 10.00px;">DSL</a><a href="/tags/Disruptor/" style="font-size: 10.00px;">Disruptor</a><a href="/tags/Docker/" style="font-size: 11.43px;">Docker</a><a href="/tags/Ember/" style="font-size: 11.43px;">Ember</a><a href="/tags/FastJson/" style="font-size: 10.00px;">FastJson</a><a href="/tags/Fiber/" style="font-size: 10.00px;">Fiber</a><a href="/tags/GAE/" style="font-size: 10.00px;">GAE</a><a href="/tags/GC/" style="font-size: 12.86px;">GC</a><a href="/tags/Gnuplot/" style="font-size: 10.00px;">Gnuplot</a><a href="/tags/Go/" style="font-size: 11.43px;">Go</a><a href="/tags/Gradle/" style="font-size: 10.00px;">Gradle</a><a href="/tags/Grunt/" style="font-size: 10.00px;">Grunt</a><a href="/tags/Gulp/" style="font-size: 10.00px;">Gulp</a><a href="/tags/Hadoop/" style="font-size: 10.00px;">Hadoop</a><a href="/tags/Hazelcast/" style="font-size: 10.00px;">Hazelcast</a><a href="/tags/Ignite/" style="font-size: 10.00px;">Ignite</a><a href="/tags/JVM/" style="font-size: 10.00px;">JVM</a><a href="/tags/Java/" style="font-size: 17.14px;">Java</a><a href="/tags/Kafka/" style="font-size: 18.57px;">Kafka</a><a href="/tags/Lambda/" style="font-size: 14.29px;">Lambda</a><a href="/tags/Linux/" style="font-size: 12.86px;">Linux</a><a href="/tags/LongAdder/" style="font-size: 10.00px;">LongAdder</a><a href="/tags/MathJax/" style="font-size: 10.00px;">MathJax</a><a href="/tags/Maven/" style="font-size: 11.43px;">Maven</a><a href="/tags/Memcached/" style="font-size: 10.00px;">Memcached</a><a href="/tags/Metrics/" style="font-size: 10.00px;">Metrics</a><a href="/tags/Mongo/" style="font-size: 12.86px;">Mongo</a><a href="/tags/Netty/" style="font-size: 15.71px;">Netty</a><a href="/tags/Node/" style="font-size: 10.00px;">Node</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/08/">August 2014</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/07/">July 2014</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/12/04/smooth-weighted-round-robin-algorithm/">平滑的基于权重的轮询算法</a>
          </li>
        
          <li>
            <a href="/2016/11/28/techempower-benchmarks-round13/">2016年Web框架性能基准</a>
          </li>
        
          <li>
            <a href="/2016/11/26/secrets-of-condoms/">嘘，啪啪啪的秘密</a>
          </li>
        
          <li>
            <a href="/2016/11/17/Benchmarking-Scala-Collections/">[译]Scala Collection的性能</a>
          </li>
        
          <li>
            <a href="/2016/11/11/microservices-anti-patterns-and-pitfalls/">微服务的反模式和陷阱</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
      <ul>
        
          <li>
			 
            <a href="http://old.colobu.com" target="_blank">旧的博客</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://stackshare.io" target="_blank">技术栈</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://weekly.manong.io/issues/" target="_blank">码农周刊</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.tuicool.com/mags" target="_blank">编程狂人周刊</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.importnew.com/" target="_blank">importnew</a>
			
          </li>
        
          <li>
			 
            <a href="http://ifeve.com/" target="_blank">并发编程网</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://github.com" target="_blank">github</a>
			
          </li>
        
          <li>
			 
            <a href="http://stackoverflow.com/" target="_blank">stackoverflow</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.javacodegeeks.com/" target="_blank">javacodegeeks</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.infoq.com/" target="_blank">infoq</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.dzone.com/links/index.html" target="_blank">dzone</a>
			
          </li>
        
          <li>
			 
            <a href="https://oj.leetcode.com/problems/" target="_blank">leetcode</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://www.503error.com/" target="_blank">503error</a>
			
          </li>
        
      </ul>
    </div>
  </div>

  
      <div class="widget-wrap">
    <h3 class="widget-title">热评文章</h3>
    <div class="widget">
		<div class="ds-top-threads" data-range="monthly" data-num-items="5"></div>
    </div>
  </div>

  
      <div class="widget-wrap">
    <h3 class="widget-title">最新评论</h3>
    <div class="widget">
		<ul class="ds-recent-comments" data-num-items="10" data-show-avatars="1" data-show-time="1" data-show-admin="1" data-excerpt-length="70"></ul>
		<script type="text/javascript">
		var duoshuoQuery = {short_name:"colobu"};
		(function() {
			var ds = document.createElement('script');
			ds.type = 'text/javascript';ds.async = true;
			ds.src = 'http://static.duoshuo.com/embed.js';
			ds.charset = 'UTF-8';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
		})();
		</script>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 smallnest<br>
	  Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">首页</a>
  
    <a href="/archives" class="mobile-nav-link">归档</a>
  
    <a href="https://github.com/smallnest" class="mobile-nav-link">github</a>
  
    <a href="http://tr.colobu.com" class="mobile-nav-link">技术流</a>
  
    <a href="/ScalaCollectionsCookbook" class="mobile-nav-link">Scala集合技术手册</a>
  
    <a href="/techreview" class="mobile-nav-link">技术快报</a>
  
    <a href="/about" class="mobile-nav-link">关于</a>
  
</nav>
    
<script src="//cdn.staticfile.org/jquery/1.11.1/jquery.min.js"></script>
<script src="//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>

<script src="//libs.cncdn.cn/fancybox/2.1.5/jquery.fancybox.pack.js"></script>


<script src="/js/script.js" type="text/javascript"></script>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.0-beta.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div id="totop" style="position:fixed;bottom:150px;right:10px;cursor: pointer;z-index: 2000;">
	<a title="返回顶部"><img src="/images/scrollup.png"/></a>
</div>
<script src="/js/totop.js" type="text/javascript"></script>



<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-142561-4']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>


<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e085d87993250aab11f3e0c15f1c2785";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </div>
</body>
</html>