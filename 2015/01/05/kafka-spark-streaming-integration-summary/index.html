<!DOCTYPE html><html><head><meta charset="utf-8"><title>Spark Streaming 集成 Kafka 总结</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="description" content="最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。
本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。当前的版本：

Spark: 1.2.0
Kafka: 0.8.1.1

Spark Streaming属于Spark的核心api，它"><meta property="og:type" content="article"><meta property="og:title" content="Spark Streaming 集成 Kafka 总结"><meta property="og:url" content="https://colobu.com/2015/01/05/kafka-spark-streaming-integration-summary/"><meta property="og:site_name" content="鸟窝"><meta property="og:description" content="最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。
本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。当前的版本：

Spark: 1.2.0
Kafka: 0.8.1.1

Spark Streaming属于Spark的核心api，它"><meta property="og:image" content="/images/logos/Spark.png"><meta property="og:image" content="/images/logos/kafka.png"><meta property="og:image" content="http://spark.apache.org/docs/latest/img/streaming-arch.png"><meta property="og:image" content="https://spark.apache.org/docs/latest/img/streaming-flow.png"><meta property="og:image" content="https://spark.apache.org/docs/latest/img/cluster-overview.png"><meta property="og:image" content="http://kafka.apache.org/images/log_anatomy.png"><meta property="og:image" content="http://kafka.apache.org/images/consumer-groups.png"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Spark Streaming 集成 Kafka 总结"><meta name="twitter:description" content="最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。
本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。当前的版本：

Spark: 1.2.0
Kafka: 0.8.1.1

Spark Streaming属于Spark的核心api，它"><link rel="alternative" href="/atom.xml" title="鸟窝" type="application/atom+xml"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/style.css" type="text/css"><link href="//cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/css/jquery.fancybox.min.css" media="screen" type="text/css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" media="screen" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css"></head><body><div id="container"><div id="wrap"><header id="header"><div id="banner"></div><div id="header-outer" class="outer"><div id="header-title" class="inner"><h1 id="logo-wrap" class="animated bounceInLeft"><a href="/" id="logo">鸟窝</a></h1><h2 id="subtitle-wrap" class="animated bounceInLeft"><a href="/" id="subtitle">大道至简 Simplicity is the ultimate form of sophistication</a></h2></div><div id="header-inner" class="inner"><nav id="main-nav"><a id="main-nav-toggle" class="nav-icon"></a> <a class="main-nav-link" href="/"><i class="fa fa-home">&nbsp;</i>首页</a> <a class="main-nav-link" href="/archives"><i class="fa fa-folder-o">&nbsp;</i>归档</a> <a class="main-nav-link" href="https://github.com/smallnest"><i class="fa fa-github">&nbsp;</i>github</a><div class="dropdown main-nav-link"><a class="main-nav-link" href="#"><i class="fa fa-bars">&nbsp;</i>网站群</a><div class="dropdown-content"><a href="/goasm"><i class="fa fa-language"></i>&nbsp;Go汇编示例</a> <a href="https://gowebexamples.com"><i class="fa fa-external-link"></i>&nbsp;Go Web开发示例</a> <a href="http://go-database-sql.org"><i class="fa fa-external-link"></i>&nbsp;Go 数据库开发教程</a><hr><a href="http://rpcx.io"><i class="fa undefined"></i>&nbsp;RPCX官网</a> <a href="http://cn.doc.rpcx.io"><i class="fa undefined"></i>&nbsp;RPC开发指南</a></div></div><a class="main-nav-link" href="/ScalaCollectionsCookbook"><i class="fa fa-book">&nbsp;</i>Scala集合技术手册</a> <a class="main-nav-link" href="/about"><i class="fa fa-lemon-o">&nbsp;</i>关于</a></nav><nav id="sub-nav"><a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a> <a id="nav-search-btn" class="nav-icon" title="Search"></a></nav><div id="search-form-wrap"><form action="http://www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form"><input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search"> <input type="submit" class="search-form-submit"> <input name="tn" type="hidden" value="bds"> <input name="cl" type="hidden" value="3"> <input name="ct" type="hidden" value="2097152"> <input type="hidden" name="si" value="colobu.com"></form></div></div></div></header><div class="outer"><section id="main"><article id="post-kafka-spark-streaming-integration-summary" class="article article-type-post" itemscope itemprop="blogPost"><div class="article-meta"><a href="/2015/01/05/kafka-spark-streaming-integration-summary/" class="article-date"><time datetime="2015-01-05T08:04:57.000Z" itemprop="datePublished">2015年01月05日</time></a><div class="article-category"><a class="article-category-link" href="/categories/大数据/">大数据</a></div><div class="article-author">by smallnest</div></div><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">Spark Streaming 集成 Kafka 总结</h1></header><div class="article-entry" itemprop="articleBody"><h1 id="expanderHead" style="cursor:pointer">目录 <span id="expanderSign">[−]</span></h1><div id="article-entry-toc" data-role="collapsible" class="article-entry-toc"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一个简单例子"><span class="toc-text">一个简单例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#核心概念"><span class="toc-text">核心概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从Kafka并行读取"><span class="toc-text">从Kafka并行读取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark并行处理"><span class="toc-text">Spark并行处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#注意事项"><span class="toc-text">注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#容错"><span class="toc-text">容错</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#worker节点失败"><span class="toc-text">worker节点失败</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#driver节点失败"><span class="toc-text">driver节点失败</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li></ol></div><script>function show_answer(e,l){"显示答案"===e.value?e.value="隐藏答案":e.value="显示答案";var n=document.getElementById(l);"none"===n.style.display?n.style.display="block":n.style.display="none"}</script><p><a rel="article0" href="/images/logos/Spark.png" class="fancybox"><img src="/images/logos/Spark.png" alt="" align="left"></a><a rel="article0" href="/images/logos/kafka.png" class="fancybox"><img src="/images/logos/kafka.png" alt="" align="left" style="margin-right: 30px;margin-left: 30px"></a><br>最近在做利用Spark streaming和Kafka进行数据分析的研究， 整理一些相应的开发文档， 做了一些代码实践。 本文特意将这些资料记录下来。</p><p>本文最后列出了一些参考的文档，实际调研中参考了很多的资料，并没有完全将它们记录下来， 只列出了主要的一些参考资料。<br>当前的版本：</p><ul><li>Spark: 1.2.0</li><li>Kafka: 0.8.1.1</li></ul><p>Spark Streaming属于Spark的核心api，它支持高吞吐量、支持容错的实时流数据处理。 有以下特点:</p><ul><li><p>易于使用<br>提供了和批处理一致的高级操作API，可以进行map, reduce, join, window。</p></li><li><p>容错<br>Spark Streaming可以恢复你计算的状态， 包括lost work和operator state (比如 sliding windows)。 支持worker节点和driver 节点恢复。</p></li><li><p>Spark集成<br>可以结合批处理流和交互式查询。 可以重用批处理的代码。还可以直接使用内置的机器学习算法、图算法包来处理数据。<br>它可以接受来自文件系统, Akka actors, rsKafka, Flume, Twitter, ZeroMQ和TCP Socket的数据源或者你自己定义的输入源。</p><a id="more"></a><p><img src="http://spark.apache.org/docs/latest/img/streaming-arch.png" alt=""></p></li></ul><p>它的工作流程像下面的图所示一样，接受到实时数据后，给数据分批次，然后传给Spark Engine处理最后生成该批次的结果流。<br><img src="https://spark.apache.org/docs/latest/img/streaming-flow.png" alt=""></p><p>Spark Streaming提供了一个高级的抽象模型，叫做<strong>discretized stream</strong>或者叫做<strong>DStream</strong>,它代表了一个持续的数据流。DStream既可以从Kafka, Flume, 和 Kinesis中产生, 或者在其它DStream上应用高级操作得到。 内部实现上一个DStream代表一个RDD序列。</p><h2 id="一个简单例子">一个简单例子</h2><p>在我们开始进入编写我们自己的Spark Streaming程序细节之前， 让我们先快速的看一个简单的Sparking Streaming程序是什么样子的。 这个程序接收网络发过来的文本数据，让我们统计一下文本中单词的数量。 全部代码如下：</p><div class="tabs"><div class="tab"><input class="tab-radio" type="radio" id="tab-1" name="tab-group-1" checked><label class="tab-label" for="tab-1">Scala</label><div class="tab-panel"><div class="tab-content">首先, 我们导入Spark Streaming类名以及StreamingContext的一些隐式转换到我们的环境中， 这样可以为我们需要的类(比如DStream)增加一些有用的方法。. StreamingContext是所有功能的主入口。 我们创建了一个本地StreamingContext， 它使用两个线程， 批处理间隔为1秒.<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming._</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.StreamingContext._</div><div class="line"></div><div class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span></div><div class="line"><span class="comment">// The master requires 2 cores to prevent from a starvation scenario.</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</div><div class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">1</span>))</div></pre></td></tr></table></figure>使用这个context, 我们可以创建一个DStream， 代表来自TCP源的流数据。需要指定主机名和端口(如 localhost 和 9999).<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></div><div class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</div></pre></td></tr></table></figure>这一行代表从数据服务器接受到的数据流. DStream中每条记录是一行文本. 接下来, 我们想使用空格分隔每一行，这样就可以得到文本中的单词。<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Split each line into words</span></div><div class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</div></pre></td></tr></table></figure>flatMap 是一个一对多的DStream操作， 它从源DStream中的每一个Record产生多个Record， 这些新产生的Record组成了一个新的DStream。 在我们的例子中， 每一行文本被分成了多个单词， 结果得到单词流DStream. 下一步， 我们想统计以下单词的数量.<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.StreamingContext._</div><div class="line"><span class="comment">// Count each word in each batch</span></div><div class="line"><span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></div><div class="line">wordCounts.print()</div></pre></td></tr></table></figure>单词DStream 被mapped (one-to-one transformation) 成*(word, 1)对*的DStream ,然后reduced 得到每一批单词的频度. 最后， wordCounts.print()会打印出每一秒产生的一些单词的统计值。 注意当这些行执行时，Spark Streaming仅仅设置这些计算， 它并没有马上被执行。 当所有的计算设置完后，我们可以调用下面的代码启动处理<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ssc.start()             <span class="comment">// Start the computation</span></div><div class="line">ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></div></pre></td></tr></table></figure>完整的代码可以在例子 [NetworkWordCount](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala) 中找到. 如果你已经下载并编译了Spark, 你可以按照下面的命令运行例子. 你要先运行Netcat工具作为数据服务器<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ nc -lk <span class="number">9999</span></div></pre></td></tr></table></figure>然后, 在另一个终端中, 你可以启动例子<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/run-example streaming.NetworkWordCount localhost <span class="number">9999</span></div></pre></td></tr></table></figure>然后, 在netcat服务器中输入的每一行都会被统计，然后统计结果被输出到屏幕上。 类似下面的输出<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># TERMINAL 2: RUNNING NetworkWordCount</span></div><div class="line"></div><div class="line">$ ./bin/run-example streaming.NetworkWordCount localhost <span class="number">9999</span></div><div class="line">...</div><div class="line">-------------------------------------------</div><div class="line">Time: <span class="number">1357008430000</span> ms</div><div class="line">-------------------------------------------</div><div class="line">(hello,<span class="number">1</span>)</div><div class="line">(world,<span class="number">1</span>)</div><div class="line">...</div></pre></td></tr></table></figure></div></div></div><div class="tab"><input class="tab-radio" type="radio" id="tab-2" name="tab-group-1"><label class="tab-label" for="tab-2">Java</label><div class="tab-panel"><div class="tab-content">首先我们创建一个JavaStreamingContext对象， 它是处理流的功能的主入口. 我们创建了一个本地的StreamingContext， 使用两个线程, 批处理间隔为1秒.<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> org.apache.spark.*;</div><div class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.*;</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.*;</div><div class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.*;</div><div class="line"><span class="keyword">import</span> scala.Tuple2;</div><div class="line"></div><div class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second</span></div><div class="line">SparkConf conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</div><div class="line">JavaStreamingContext jssc = <span class="keyword">new</span> JavaStreamingContext(conf, Durations.seconds(<span class="number">1</span>))</div></pre></td></tr></table></figure>使用这个context, 我们可以创建一个DStream， 代表来自TCP源的流数据。需要指定主机名和端口(如 localhost 和 9999).<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></div><div class="line">JavaReceiverInputDStream&lt;String&gt; lines = jssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>);</div></pre></td></tr></table></figure>这一行代表从数据服务器接受到的数据流. DStream中每条记录是一行文本. 接下来, 我们想使用空格分隔每一行，这样就可以得到文本中的单词。<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Split each line into words</span></div><div class="line">JavaDStream&lt;String&gt; words = lines.flatMap(</div><div class="line">  <span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() {</div><div class="line">    <span class="annotation">@Override</span> <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">call</span>(String x) {</div><div class="line">      <span class="keyword">return</span> Arrays.asList(x.split(<span class="string">" "</span>));</div><div class="line">    }</div><div class="line">  });</div></pre></td></tr></table></figure>flatMap 是一个一对多的DStream操作， 它从源DStream中的每一个Record产生多个Record， 这些新产生的Record组成了一个新的DStream。 在我们的例子中， 每一行文本被分成了多个单词， 结果得到单词流DStream. 下一步， 我们想统计以下单词的数量.<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Count each word in each batch</span></div><div class="line">JavaPairDStream&lt;String, Integer&gt; pairs = words.map(</div><div class="line">  <span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() {</div><div class="line">    <span class="annotation">@Override</span> <span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span>(String s) <span class="keyword">throws</span> Exception {</div><div class="line">      <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, Integer&gt;(s, <span class="number">1</span>);</div><div class="line">    }</div><div class="line">  });</div><div class="line">JavaPairDStream&lt;String, Integer&gt; wordCounts = pairs.reduceByKey(</div><div class="line">  <span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() {</div><div class="line">    <span class="annotation">@Override</span> <span class="keyword">public</span> Integer <span class="title">call</span>(Integer i1, Integer i2) <span class="keyword">throws</span> Exception {</div><div class="line">      <span class="keyword">return</span> i1 + i2;</div><div class="line">    }</div><div class="line">  });</div><div class="line"></div><div class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></div><div class="line">wordCounts.print();</div></pre></td></tr></table></figure>单词DStream 被mapped (one-to-one transformation) 成*(word, 1)对*的DStream ,然后reduced 得到每一批单词的频度. 最后， wordCounts.print()会打印出每一秒产生的一些单词的统计值。 注意当这些行执行时，Spark Streaming仅仅设置这些计算， 它并没有马上被执行。 当所有的计算设置完后，我们可以调用下面的代码启动处理<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">jssc.start();              <span class="comment">// Start the computation</span></div><div class="line">jssc.awaitTermination();   <span class="comment">// Wait for the computation to terminate</span></div></pre></td></tr></table></figure>完整的的代码看例子 [JavaNetworkWordCount](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java). 如果你已经下载并编译了Spark, 你可以按照下面的命令运行例子. 你要先运行Netcat工具作为数据服务器<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ nc -lk <span class="number">9999</span></div></pre></td></tr></table></figure>然后, 在另一个终端中, 你可以启动例子<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="number">9999</span></div></pre></td></tr></table></figure>然后, 在netcat服务器中输入的每一行都会被统计，然后统计结果被输出到屏幕上。 类似下面的输出<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># TERMINAL 2: RUNNING JavaNetworkWordCount</span></div><div class="line"></div><div class="line">$ ./bin/run-example streaming.JavaNetworkWordCount localhost <span class="number">9999</span></div><div class="line">...</div><div class="line">-------------------------------------------</div><div class="line">Time: <span class="number">1357008430000</span> ms</div><div class="line">-------------------------------------------</div><div class="line">(hello,<span class="number">1</span>)</div><div class="line">(world,<span class="number">1</span>)</div><div class="line">...</div></pre></td></tr></table></figure></div></div></div></div><h2 id="核心概念">核心概念</h2><p>本节介绍一些Spark和Kafka的概念<br><i class="fa fa-cog"></i> <strong>Spark cluster</strong>:<br>一个Spark集群至少包含一个worker节点。<br><img src="https://spark.apache.org/docs/latest/img/cluster-overview.png" alt=""></p><p><i class="fa fa-cog"></i> <strong>worker node</strong>:<br>一个工作节点可以执行一个或者多个executor.</p><p><i class="fa fa-cog"></i> <strong>executor</strong>:<br>executor就是一个进程， 负责启在一个worker节点上启动应用，运行task执行计算，存储数据到内存或者磁盘上。 每个Spark应用都有自己的executor。一个executor拥有一定数量的cores, 也被叫做“slots”， 可以执行指派给它的task。</p><p><i class="fa fa-cog"></i> <strong>job</strong>:<br>一个并行的计算单元，包含多个task。 在执行Spark action (比如 save, collect)产生; 在log中可以看到这个词。</p><p><i class="fa fa-cog"></i> <strong>task</strong>:<br>一个task就是一个工作单元， 可以发送给一个executor执行。 它执行你的应用的实际计算的部分工作。 每个task占用父executor的一个slot (core)。</p><p><i class="fa fa-cog"></i> <strong>stage</strong>:<br>每个job都被分隔成多个彼此依赖称之为stage的task(类似MapReduce中的map 和 reduce stage);</p><p><i class="fa fa-cog"></i> <strong>共享变量</strong>: 普通可序列化的变量复制到远程各个节点。在远程节点上的更新并不会返回到原始节点。因为我们需要共享变量。 Spark提供了两种类型的共享变量。</p><pre><code><span class="keyword">*</span> Broadcast 变量。  SparkContext.broadcast(v)通过创建， <span class="keyword">*</span><span class="keyword">*</span>只读<span class="keyword">*</span><span class="keyword">*</span>。
<span class="keyword">*</span> Accumulator: 累加器，通过SparkContext.accumulator(v)创建，在任务中只能调用add或者+操作，不能读取值。只有驱动程序才可以读取值。
</code></pre><p><i class="fa fa-cog"></i> <strong>receiver</strong>:<br>receiver长时间（可能7*24小时）运行在executor。 每个receiver负责一个 input DStream (例如 一个 读取Kafka消息的input stream)。 每个receiver， 加上input DStream会占用一个core/slot.</p><p><i class="fa fa-cog"></i> <strong>input DStream</strong>:<br>一个input DStream是一个特殊的DStream， 将Spark Streaming连接到一个外部数据源来读取数据。</p><p><i class="fa fa-cog"></i> <strong>kafka topic</strong>:<br>topic是发布消息发布的category 或者 feed名. 对于每个topic, Kafka管理一个分区的log，如下图所示：<br><img src="http://kafka.apache.org/images/log_anatomy.png" alt=""><br>分区内的消息都是有序不可变的。</p><p><i class="fa fa-cog"></i> <strong>kafka partition</strong>:<br>partitions的设计目的有多个.最根本原因是kafka基于文件存储.通过分区,可以将日志内容分散到多个server上,来避免文件尺寸达到单机磁盘的上限,每个partiton都会被当前server(kafka实例)保存;可以将一个topic切分多任意多个partitions(备注:基于sharding),来消息保存/消费的效率.此外越多的partitions意味着可以容纳更多的consumer,有效提升并发消费的能力.</p><p><i class="fa fa-cog"></i> <strong>kafka consumer group</strong>:<br>在kafka中，每个消费者要标记自己在那个组中。<br>如果所有的消费者都在同一个组中，则类似传统的queue消息模式，消息只发给一个消费者。<br>如果消费者都在不同的组中， 则类似发布-订阅消息模式。 每个消费者都会得到所有的消息。<br>最通用的模式是混用这两种模式，如下图：<br><img src="http://kafka.apache.org/images/consumer-groups.png" alt=""></p><p>关于kafka和消费者线程， 遵循下面的约束：<br>如果你的消费者读取包含10个分区的 <code>test</code>的topic,</p><ul><li>如果你配置你的消费者只使用1个线程， 则它负责读取十个分区</li><li>如果你配置你的消费者只使用5个线程， 则每个线程负责读取2个分区</li><li>如果你配置你的消费者只使用10个线程， 则每个线程负责读取1个分区</li><li>如果你配置你的消费者只使用14个线程， 则10个线程各负责读取1个分区,4个空闲</li><li>如果你配置你的消费者只使用8个线程， 则6个线程个负责一个分区，2个线程各负责2个分区</li></ul><h2 id="从Kafka并行读取">从Kafka并行读取</h2><p>有几种方法可以并行的读取Kafka的消息。</p><p>Spark的KafkaInputDStream (也叫做Kafka “connector”)使用 Kafka high-level consumer API读取数据，所以有两种方式可以并行的读取数据。</p><ul><li>多个input DStream： Spark为每个input dstream运行一个receiver. 这意味着多个input dstream可以运行在多个core上并行读取。 如果它们使用相同的topic,则相当于一个load balancer， 一个时间点上只有一个receiver读取。 如果不同的topic，可以同时读取。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ??? <span class="comment">// ignore for now</span></div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"test"</span>, <span class="comment">/* ignore rest */</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> numInputDStreams = <span class="number">5</span></div><div class="line"><span class="keyword">val</span> kafkaDStreams = (<span class="number">1</span> to numInputDStreams).map { _ =&gt; KafkaUtils.createStream(...) }</div></pre></td></tr></table></figure><ul><li>每个input dstream的消费者线程数。 同一个receiver可以运行多个线程。 可以配置和分区相同的线程。</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ??? <span class="comment">// ignore for now</span></div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"terran"</span>, ...)</div><div class="line"></div><div class="line"><span class="keyword">val</span> consumerThreadsPerInputDstream = <span class="number">3</span></div><div class="line"><span class="keyword">val</span> topics = Map(<span class="string">"test"</span> -&gt; consumerThreadsPerInputDstream)</div><div class="line"><span class="keyword">val</span> stream = KafkaUtils.createStream(ssc, kafkaParams, topics, ...)</div></pre></td></tr></table></figure><p>或者你还可以混合这两种情况：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ???</div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"terran"</span>, ...)</div><div class="line"></div><div class="line"><span class="keyword">val</span> numDStreams = <span class="number">5</span></div><div class="line"><span class="keyword">val</span> topics = Map(<span class="string">"zerg.hydra"</span> -&gt; <span class="number">1</span>)</div><div class="line"><span class="keyword">val</span> kafkaDStreams = (<span class="number">1</span> to numDStreams).map { _ =&gt;</div><div class="line">    KafkaUtils.createStream(ssc, kafkaParams, topics, ...)</div><div class="line">  }</div></pre></td></tr></table></figure><h2 id="Spark并行处理">Spark并行处理</h2><p>上面介绍了Kafka的并行化读取的控制，在Spark中我们可以进行并行化处理。类似Kafka，Spark将parallelism设置的与（RDD）分区数量有关， 通过在每个RDD分区上运行task进行。在有些文档中，分区仍然被称为“slices”。<br>同样两个控制手段：</p><ul><li>input DStreams的数量</li><li>DStream transformation的重分配(repartition): 这里将获得一个全新的DStream，其parallelism等级可能增加、减少，或者保持原样。在DStream中每个返回的RDD都有指定的N个分区。DStream由一系列的RDD组成，DStream.repartition则是通过RDD.repartition实现。<br>因此，repartition是从processing parallelism分隔read parallelism的主要途径。在这里，我们可以设置processing tasks的数量，也就是说设置处理过程中所有core的数量。间接上，我们同样设置了投入machines/NICs的数量。</li></ul><p>一个DStream转换相关是 union。这个方法同样在StreamingContext中，它将从多个DStream中返回一个统一的DStream，它将拥有相同的类型和滑动时间。union会将多个 DStreams压缩到一个 DStreams或者RDD中，但是需要注意的是，这里的parallelism并不会发生改变。</p><p>你的用例将决定你如何分区。如果你的用例是CPU密集型的，你希望对test topic进行5 read parallelism读取。也就是说，每个消费者进程使用5个receiver，但是却可以将processing parallelism提升到20。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ssc: StreamingContext = ???</div><div class="line"><span class="keyword">val</span> kafkaParams: Map[String, String] = Map(<span class="string">"group.id"</span> -&gt; <span class="string">"terran"</span>, ...)</div><div class="line"><span class="keyword">val</span> readParallelism = <span class="number">5</span></div><div class="line"><span class="keyword">val</span> topics = Map(<span class="string">"test"</span> -&gt; <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="keyword">val</span> kafkaDStreams = (<span class="number">1</span> to readParallelism).map { _ =&gt;</div><div class="line">    KafkaUtils.createStream(ssc, kafkaParams, topics, ...)</div><div class="line">  }</div><div class="line"><span class="comment">//&gt; collection of five *input* DStreams = handled by five receivers/tasks</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> unionDStream = ssc.union(kafkaDStreams) <span class="comment">// often unnecessary, just showcasing how to do it</span></div><div class="line"><span class="comment">//&gt; single DStream</span></div><div class="line"></div><div class="line"><span class="keyword">val</span> processingParallelism = <span class="number">20</span></div><div class="line"><span class="keyword">val</span> processingDStream = unionDStream(processingParallelism)</div><div class="line"><span class="comment">//&gt; single DStream but now with 20 partitions</span></div></pre></td></tr></table></figure><h2 id="注意事项">注意事项</h2><p>在对Kafka进行读写上仍然存在一些含糊不清的问题，你可以在类似 <a href="http://apache-spark-user-list.1001560.n3.nabble.com/Multiple-Kafka-Receivers-and-Union-td14901.html" target="_blank" rel="external">Multiple Kafka Receivers and Union</a>和 <a href="http://apache-spark-user-list.1001560.n3.nabble.com/How-to-scale-more-consumer-to-Kafka-stream-td13883.html" target="_blank" rel="external">How to scale more consumer to Kafka stream mailing list</a>的讨论中发现。</p><ul><li><p>Spark 1.1并不会恢复那些已经接收却没有进行处理的原始数据（<a href="https://www.mail-archive.com/user@spark.apache.org/msg10572.html" target="_blank" rel="external">查看</a>）。因此，在某些情况下，你的Spark可能会丢失数据。Tathagata Das指出驱动恢复问题会在Spark的1.2版本中解决，现在已经提供<a href="https://github.com/apache/spark/blob/4b4b50c9e596673c1534df97effad50d107a8007/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/ReliableKafkaReceiver.scala" target="_blank" rel="external">Reliable Receiver</a> 和Unreliable Receiver两种Receiver。</p></li><li><p>1.1版本中的Kafka连接器是基于Kafka的高级消费者API。这样就会造成一个问题，Spark Streaming不可以依赖其自身的KafkaInputDStream将数据从Kafka中重新发送，从而无法解决下游数据丢失问题（比如Spark服务器发生故障）。<br>Dibyendu Bhattacharya 实现了使用简单消费者API: <a href="https://github.com/dibbhatt/kafka-spark-consumer" target="_blank" rel="external">kafka-spark-consumer</a>.</p></li><li><p>使用最新的Spark和Kafka,一些bugs已经在最新的Spark和Kafka中修复。</p></li><li><p>在使用window操作时，window duration和sliding duration必须是DStream批处理的duration的整数倍。</p></li><li><p>如果分配给应用的core的数量小于或者等于input DStream/receiver数量，则系统只接收数据， 没有额外的core处理数据</p></li><li><p>接上一条， 你在本地进行测试的时候，如果将master URL设置为“local”的话，则只有一个core运行任务，这明显违反上一条， 只能接收数据，无法处理。</p></li><li><p>Kafak Topic 的分区和 Spark RDD的分区没有任何关系。 它俩是分别设置的。</p></li></ul><h2 id="容错">容错</h2><p>有两种情况的机器失败。</p><h3 id="worker节点失败">worker节点失败</h3><p>receiver接收到的消息在集群间有备份。如果只是一个节点失败， Spark可以恢复。<br>但是如果是receiver所在的那个节点失败，可能会有一点点数据丢失。 但是Receiver可以在其它节点上恢复启动，继续接收数据。</p><h3 id="driver节点失败">driver节点失败</h3><p>如果7*24工作的应用， 如果driver节点失败，Spark Streaming也可以恢复。 Spark streaming定期的把元数据写到HDFS中。 你需要设置checkpoint 文件夹。<br>为了支持恢复，必须遵循下面的处理：</p><ol><li>当应用首次启动时， 它会创建一个新的StreamingContext, 设置所有的流，然后启动start().</li><li>当应用因失败而恢复时， 它会从checkpoint文件中的checkpoint重建StreamingContext.</li></ol><p>就像这样：</p><div class="tabs"><div class="tab"><input class="tab-radio" type="radio" id="tab-3" name="tab-group-2" checked><label class="tab-label" for="tab-3">Scala</label><div class="tab-panel"><div class="tab-content"><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Function to create and setup a new StreamingContext</span></div><div class="line"><span class="keyword">def</span> functionToCreateContext(): StreamingContext = {</div><div class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> StreamingContext(...)   <span class="comment">// new context</span></div><div class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(...) <span class="comment">// create DStreams</span></div><div class="line">    ...</div><div class="line">    ssc.checkpoint(checkpointDirectory)   <span class="comment">// set checkpoint directory</span></div><div class="line">    ssc</div><div class="line">}</div><div class="line"></div><div class="line"><span class="comment">// Get StreamingContext from checkpoint data or create a new one</span></div><div class="line"><span class="keyword">val</span> context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext _)</div><div class="line"></div><div class="line"><span class="comment">// Do additional setup on context that needs to be done,</span></div><div class="line"><span class="comment">// irrespective of whether it is being started or restarted</span></div><div class="line">context. ...</div><div class="line"></div><div class="line"><span class="comment">// Start the context</span></div><div class="line">context.start()</div><div class="line">context.awaitTermination()</div></pre></td></tr></table></figure></div></div></div><div class="tab"><input class="tab-radio" type="radio" id="tab-4" name="tab-group-2"><label class="tab-label" for="tab-4">Java</label><div class="tab-panel"><div class="tab-content"><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// Create a factory object that can create a and setup a new JavaStreamingContext</span></div><div class="line">JavaStreamingContextFactory contextFactory = <span class="keyword">new</span> JavaStreamingContextFactory() {</div><div class="line">  <span class="annotation">@Override</span> <span class="keyword">public</span> JavaStreamingContext <span class="title">create</span>() {</div><div class="line">    JavaStreamingContext jssc = <span class="keyword">new</span> JavaStreamingContext(...);  <span class="comment">// new context</span></div><div class="line">    JavaDStream&lt;String&gt; lines = jssc.socketTextStream(...);     <span class="comment">// create DStreams</span></div><div class="line">    ...</div><div class="line">    jssc.checkpoint(checkpointDirectory);                       <span class="comment">// set checkpoint directory</span></div><div class="line">    <span class="keyword">return</span> jssc;</div><div class="line">  }</div><div class="line">};</div><div class="line"></div><div class="line"><span class="comment">// Get JavaStreamingContext from checkpoint data or create a new one</span></div><div class="line">JavaStreamingContext context = JavaStreamingContext.getOrCreate(checkpointDirectory, contextFactory);</div><div class="line"></div><div class="line"><span class="comment">// Do additional setup on context that needs to be done,</span></div><div class="line"><span class="comment">// irrespective of whether it is being started or restarted</span></div><div class="line">context. ...</div><div class="line"></div><div class="line"><span class="comment">// Start the context</span></div><div class="line">context.start();</div><div class="line">context.awaitTermination();</div></pre></td></tr></table></figure></div></div></div></div><h2 id="参考">参考</h2><ol><li><a href="http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/" target="_blank" rel="external">http://www.michael-noll.com/blog/2014/10/01/kafka-spark-streaming-integration-example-tutorial/</a></li><li><a href="https://spark.apache.org/docs/1.2.0/streaming-kafka-integration.html" target="_blank" rel="external">https://spark.apache.org/docs/1.2.0/streaming-kafka-integration.html</a></li><li><a href="https://spark.apache.org/docs/1.2.0/streaming-programming-guide.html" target="_blank" rel="external">https://spark.apache.org/docs/1.2.0/streaming-programming-guide.html</a></li></ol></div><footer class="article-footer"><ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/">Kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul><section id="comments"><script src="https://utteranc.es/client.js" repo="smallnest/gitalk" issue-term="title" theme="github-light" crossorigin="anonymous" async></script><noscript>为正常使用评论功能请激活JavaScript</noscript></section></footer></div><nav id="article-nav"><a href="/2015/01/07/Protobuf-language-guide/" id="article-nav-newer" class="article-nav-link-wrap"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">[译]Protobuf 语法指南</div></a> <a href="/2015/01/04/How-to-Leak-a-Context-Handlers-Inner-Classes/" id="article-nav-older" class="article-nav-link-wrap"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">Android中Handler引起的内存泄露</div></a></nav></article></section><aside id="sidebar"><div class="widget-wrap"><h3 class="widget-title">微信公众号</h3><div class="widget"><img width="100%" src="/images/widgets/gopatterns.jpg"></div></div><div class="widget-wrap"><h3 class="widget-title">极客时间专栏</h3><div class="widget"><a href="https://time.geekbang.org/column/intro/100061801"><img width="100%" src="/images/widgets/geekbang.png"></a></div></div><div class="widget-wrap"><h3 class="widget-title">原创图书</h3><div class="widget"><a href="/ScalaCollectionsCookbook/"><img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook.jpg"> <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook_tw.png"></a></div></div><div class="widget-wrap"><h3 class="widget-title">分类</h3><div class="widget"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DOTNET/">DOTNET</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a><span class="category-list-count">183</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">64</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Rust/">Rust</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/分享/">分享</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端开发/">前端开发</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/区块链/">区块链</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">28</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/架构/">架构</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/管理/">管理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络编程/">网络编程</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/运维/">运维</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发编程/">高并发编程</a><span class="category-list-count">20</span></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">标签云</h3><div class="widget tagcloud"><a href="/tags/Android/" style="font-size: 15.71px">Android</a><a href="/tags/ApacheBench/" style="font-size: 11.43px">ApacheBench</a><a href="/tags/Bower/" style="font-size: 10.00px">Bower</a><a href="/tags/C/" style="font-size: 10.00px">C#</a><a href="/tags/CDN/" style="font-size: 10.00px">CDN</a><a href="/tags/CQRS/" style="font-size: 10.00px">CQRS</a><a href="/tags/CRC/" style="font-size: 10.00px">CRC</a><a href="/tags/CSS/" style="font-size: 11.43px">CSS</a><a href="/tags/CompletableFuture/" style="font-size: 10.00px">CompletableFuture</a><a href="/tags/Comsat/" style="font-size: 10.00px">Comsat</a><a href="/tags/Curator/" style="font-size: 18.57px">Curator</a><a href="/tags/DSL/" style="font-size: 10.00px">DSL</a><a href="/tags/Disruptor/" style="font-size: 10.00px">Disruptor</a><a href="/tags/Docker/" style="font-size: 11.43px">Docker</a><a href="/tags/Ember/" style="font-size: 11.43px">Ember</a><a href="/tags/FastJson/" style="font-size: 10.00px">FastJson</a><a href="/tags/Fiber/" style="font-size: 10.00px">Fiber</a><a href="/tags/GAE/" style="font-size: 10.00px">GAE</a><a href="/tags/GC/" style="font-size: 12.86px">GC</a><a href="/tags/Gnuplot/" style="font-size: 10.00px">Gnuplot</a><a href="/tags/Go/" style="font-size: 14.29px">Go</a><a href="/tags/Gradle/" style="font-size: 10.00px">Gradle</a><a href="/tags/Grunt/" style="font-size: 10.00px">Grunt</a><a href="/tags/Gulp/" style="font-size: 10.00px">Gulp</a><a href="/tags/Hadoop/" style="font-size: 10.00px">Hadoop</a><a href="/tags/Hazelcast/" style="font-size: 10.00px">Hazelcast</a><a href="/tags/IPFS/" style="font-size: 10.00px">IPFS</a><a href="/tags/Ignite/" style="font-size: 10.00px">Ignite</a><a href="/tags/JVM/" style="font-size: 10.00px">JVM</a><a href="/tags/Java/" style="font-size: 17.14px">Java</a><a href="/tags/Kafka/" style="font-size: 20.00px">Kafka</a><a href="/tags/Lambda/" style="font-size: 14.29px">Lambda</a><a href="/tags/Linux/" style="font-size: 12.86px">Linux</a><a href="/tags/LongAdder/" style="font-size: 10.00px">LongAdder</a><a href="/tags/MathJax/" style="font-size: 10.00px">MathJax</a><a href="/tags/Maven/" style="font-size: 11.43px">Maven</a><a href="/tags/Memcached/" style="font-size: 10.00px">Memcached</a><a href="/tags/Metrics/" style="font-size: 10.00px">Metrics</a><a href="/tags/Mongo/" style="font-size: 12.86px">Mongo</a><a href="/tags/Netty/" style="font-size: 15.71px">Netty</a></div></div><div class="widget-wrap"><h3 class="widget-title">归档</h3><div class="widget"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/08/">August 2014</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/07/">July 2014</a><span class="archive-list-count">1</span></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">近期文章</h3><div class="widget"><ul><li><a href="/2022/01/15/Writing-maintainable-Go-code/">编写可维护的Go代码</a></li><li><a href="/2022/01/11/go-generic-supports-specialization-no/">Go泛型编程: 支持特例化?</a></li><li><a href="/2022/01/08/the-interface-is-not-that-interface-in-go-1-18/">Go泛型编程: interface 不再是那个interface</a></li><li><a href="/2022/01/03/go-fuzzing/">[译] go fuzzing</a></li><li><a href="/2021/12/22/no-parameterized-methods/">Go泛型不支持泛型方法，这是一个悲伤的故事</a></li></ul></div></div><div class="widget-wrap"><h3 class="widget-title">友情链接</h3><div class="widget"><ul><li><a href="http://stackshare.io" target="_blank">技术栈</a></li><li>&nbsp;</li><li><a href="https://toutiao.io/" target="_blank">开发者头条</a></li><li><a href="http://weekly.manong.io/issues/" target="_blank">码农周刊</a></li><li><a href="http://www.tuicool.com/mags" target="_blank">编程狂人周刊</a></li><li><a href="http://www.importnew.com/" target="_blank">importnew</a></li><li><a href="http://ifeve.com/" target="_blank">并发编程网</a></li><li>&nbsp;</li><li><a href="http://github.com" target="_blank">github</a></li><li><a href="http://stackoverflow.com/" target="_blank">stackoverflow</a></li><li><a href="http://www.javacodegeeks.com/" target="_blank">javacodegeeks</a></li><li><a href="http://www.infoq.com/" target="_blank">infoq</a></li><li><a href="http://www.dzone.com/links/index.html" target="_blank">dzone</a></li><li><a href="https://oj.leetcode.com/problems/" target="_blank">leetcode</a></li><li><a href="http://tutorials.jenkov.com" target="_blank">jenkov</a></li><li><a href="https://howtodoinjava.com" target="_blank">HowToDoInJava</a></li><li><a href="https://java-design-patterns.com/patterns/" target="_blank">java design patterns</a></li><li>&nbsp;</li><li><a href="https://medium.com/netflix-techblog" target="_blank">Netflix技术博客</a></li><li><a href="https://www.techiedelight.com" target="_blank">Techie Delight</a></li><li><a href="https://engineering.linkedin.com/blog" target="_blank">Linkedin技术博客</a></li><li><a href="https://blogs.dropbox.com/tech/" target="_blank">Dropbox技术博客</a></li><li><a href="https://code.fb.com" target="_blank">Facebook技术博客</a></li><li><a href="http://jm.taobao.org" target="_blank">淘宝中间件团队</a></li><li><a href="https://tech.meituan.com" target="_blank">美团技术博客</a></li><li><a href="http://blogs.360.cn" target="_blank">360技术博客</a></li><li><a href="https://xiaomi-info.github.io" target="_blank">小米信息部技术团队</a></li></ul></div></div></aside></div><footer id="footer"><div class="outer"><div id="footer-info" class="inner">&copy; 2022 smallnest<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></div></div></footer></div><nav id="mobile-nav"><a href="/" class="mobile-nav-link"><i class="fa fa-home">&nbsp;</i>首页</a> <a href="/archives" class="mobile-nav-link"><i class="fa fa-folder-o">&nbsp;</i>归档</a> <a href="https://github.com/smallnest" class="mobile-nav-link"><i class="fa fa-github">&nbsp;</i>github</a> <a class="mobile-nav-link" href="#"><i class="fa fa-bars">&nbsp;</i>网站群</a> <a class="mobile-nav-link" href="/goasm">&nbsp;&nbsp;<i class="fa fa-language">&nbsp;</i>Go汇编示例</a> <a class="mobile-nav-link" href="https://gowebexamples.com">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go Web开发示例</a> <a class="mobile-nav-link" href="http://go-database-sql.org">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go 数据库开发教程</a> <a class="mobile-nav-link" href="http://rpcx.io">&nbsp;&nbsp;<i class="fa undefined">&nbsp;</i>RPCX官网</a> <a class="mobile-nav-link" href="http://cn.doc.rpcx.io">&nbsp;&nbsp;<i class="fa undefined">&nbsp;</i>RPC开发指南</a> <a href="/ScalaCollectionsCookbook" class="mobile-nav-link"><i class="fa fa-book">&nbsp;</i>Scala集合技术手册</a> <a href="/about" class="mobile-nav-link"><i class="fa fa-lemon-o">&nbsp;</i>关于</a></nav><script src="//cdn.staticfile.org/jquery/1.11.1/jquery.min.js"></script><script src="//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"></script><script src="/js/script.js" type="text/javascript"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0,skipTags:["script","noscript","style","textarea","pre","code"]}}),MathJax.Hub.Queue(function(){var a,e=MathJax.Hub.getAllJax();for(a=0;a<e.length;a+=1)e[a].SourceElement().parentNode.className+=" has-jax"});</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.0-beta.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div id="totop" style="position:fixed;bottom:150px;right:10px;cursor: pointer;z-index: 2000"><a title="返回顶部"><img src="/images/scrollup.png"></a></div><script src="/js/totop.js" type="text/javascript"></script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?e085d87993250aab11f3e0c15f1c2785";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}();</script></div></body></html>