<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  
  <title>[转][译] Linux 网络栈监控和调优：接收数据（2016）</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="这是一个关于Linux网络监控和调优系列文章的一篇，详尽的介绍了网络知识的各个方面，得到了广泛的转发和称赞，也被翻译成了多个中文版。本文转载的是携程的ArthurChiao翻译的版本，翻译通顺，还很有心的加上了章节，更方便阅读。他还翻译很多有价值的文章，读者可以到他的网站上阅读。
英文原文: Monitoring and Tuning the Linux Networking Stack: Rec">
<meta property="og:type" content="article">
<meta property="og:title" content="[转][译] Linux 网络栈监控和调优：接收数据（2016）">
<meta property="og:url" content="https://colobu.com/2019/12/09/monitoring-tuning-linux-networking-stack-receiving-data/">
<meta property="og:site_name" content="鸟窝">
<meta property="og:description" content="这是一个关于Linux网络监控和调优系列文章的一篇，详尽的介绍了网络知识的各个方面，得到了广泛的转发和称赞，也被翻译成了多个中文版。本文转载的是携程的ArthurChiao翻译的版本，翻译通顺，还很有心的加上了章节，更方便阅读。他还翻译很多有价值的文章，读者可以到他的网站上阅读。
英文原文: Monitoring and Tuning the Linux Networking Stack: Rec">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="[转][译] Linux 网络栈监控和调优：接收数据（2016）">
<meta name="twitter:description" content="这是一个关于Linux网络监控和调优系列文章的一篇，详尽的介绍了网络知识的各个方面，得到了广泛的转发和称赞，也被翻译成了多个中文版。本文转载的是携程的ArthurChiao翻译的版本，翻译通顺，还很有心的加上了章节，更方便阅读。他还翻译很多有价值的文章，读者可以到他的网站上阅读。
英文原文: Monitoring and Tuning the Linux Networking Stack: Rec">

  
  <link rel="alternative" href="/atom.xml" title="鸟窝" type="application/atom+xml">
  
  
  <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" href="/css/style.css" type="text/css">

  <link href="//cdn.staticfile.org/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/css/jquery.fancybox.min.css"
    media="screen" type="text/css">
  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" media="screen"
    type="text/css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/tonsky/FiraCode@1.207/distr/fira_code.css">
</head>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap" class="animated bounceInLeft">
        <a href="/" id="logo">鸟窝</a>
      </h1>
      
        <!-- <h2 id="subtitle-wrap" class="animated bounceInLeft"> -->
        <h2 id="subtitle-wrap">
          <!-- <a href="/" id="subtitle">大道至简 Simplicity is the ultimate form of sophistication</a> -->
          <a href="https://item.jd.com/14283252.html" target="_blank" style="color: #e32d40;text-decoration: none;"><b>《深入理解Go并发编程》新书发售中。一书在手，并发无忧</b></a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          
            <a class="main-nav-link" href="/"><i class="fa fa-home">&nbsp;</i>首页</a>
          
          
          
            <a class="main-nav-link" href="/archives"><i class="fa fa-folder-o">&nbsp;</i>归档</a>
          
          
          
            <a class="main-nav-link" href="https://github.com/smallnest"><i class="fa fa-github">&nbsp;</i>github</a>
          
          
          
            <div class="dropdown main-nav-link"><a class="main-nav-link" href="#"><i class="fa fa-bars">&nbsp;</i>网站群</a>
              <div class="dropdown-content">
          
            
              <a href="/goasm"><i class="fa fa-language"></i>&nbsp;Go汇编示例</a>
            
                
          
            
              <a href="https://gowebexamples.com"><i class="fa fa-external-link"></i>&nbsp;Go Web开发示例</a>
            
                
          
            
              <a href="http://go-database-sql.org"><i class="fa fa-external-link"></i>&nbsp;Go 数据库开发教程</a>
            
                
          
            
              <a href="https://colobu.com/gotips/"><i class="fa fa-external-link"></i>&nbsp;Go 语言编程技巧</a>
            
                
          
            
              <hr>
            
                
          
            
              <a href="/perf-book"><i class="fa fa-brands fa-rust"></i>&nbsp;Rust高性能编程指南</a>
            
                
          
            
              <a href="/rust100"><i class="fa fa-brands fa-rust"></i>&nbsp;100个练习题学习Rust</a>
            
                
          
            
              <hr>
            
                
          
            
              <a href="http://rpcx.io"><i class="fa undefined"></i>&nbsp;RPCX官网</a>
            
                
          
            
              <a href="http://cn.doc.rpcx.io"><i class="fa undefined"></i>&nbsp;RPC开发指南</a>
            
                
          
          </div>
        </div>
          
          
          
            <a class="main-nav-link" href="/ScalaCollectionsCookbook"><i class="fa fa-solid fa-book">&nbsp;</i>Scala集合技术手册</a>
          
          
          
            <a class="main-nav-link" href="/about"><i class="fa fa-lemon-o">&nbsp;</i>关于</a>
          
          


      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="https://www.google.com/search" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" id="search-word" name="q" maxlength="20" class="search-form-input" placeholder="Search">
          <input type=hidden name=ie value="utf-8">
          <input type=hidden name=oe value="utf-8">
          <input type=hidden name=hl value="zh-CN">
          <input type="submit" id="search-submit" value="" class="search-form-submit">
        </form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-monitoring-tuning-linux-networking-stack-receiving-data" class="article article-type-post" itemscope
  itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/12/09/monitoring-tuning-linux-networking-stack-receiving-data/" class="article-date">
  <time datetime="2019-12-09T15:22:42.000Z" itemprop="datePublished">2019年12月09日</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/网络编程/">网络编程</a>
  </div>

    
  <div class="article-author"> by ArthurChiao</div>

  </div>
  <div class="article-inner">
    
    
    <header class="article-header">
      
  
    <h1 class="article-title" itemprop="name">
      [转][译] Linux 网络栈监控和调优：接收数据（2016）
	  
    </h1>
  

    </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      <h1 id="expanderHead" style="cursor:pointer;">
        目录 <span id="expanderSign">[−]</span>
      </h1>
      <div id="article-entry-toc" data-role="collapsible" class="article-entry-toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#监控和调优网络栈：常规建议"><span class="toc-text">监控和调优网络栈：常规建议</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#收包过程俯瞰"><span class="toc-text">收包过程俯瞰</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#网络设备驱动"><span class="toc-text">网络设备驱动</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#初始化"><span class="toc-text">初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#PCI_初始化"><span class="toc-text">PCI 初始化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网络设备初始化"><span class="toc-text">网络设备初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#更多_PCI_驱动信息"><span class="toc-text">更多 PCI 驱动信息</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网络设备启动"><span class="toc-text">网络设备启动</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#struct_net_device_ops"><span class="toc-text">struct net_device_ops</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ethtool_函数注册"><span class="toc-text">ethtool 函数注册</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#软中断（IRQ）"><span class="toc-text">软中断（IRQ）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NAPI"><span class="toc-text">NAPI</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#NAPI-1"><span class="toc-text">NAPI</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#igb_驱动的_NAPI_初始化"><span class="toc-text">igb 驱动的 NAPI 初始化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#启用网卡_(Bring_A_Network_Device_Up)"><span class="toc-text">启用网卡 (Bring A Network Device Up)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#准备从网络接收数据"><span class="toc-text">准备从网络接收数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Enable_NAPI"><span class="toc-text">Enable NAPI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#注册中断处理函数"><span class="toc-text">注册中断处理函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Enable_Interrupts"><span class="toc-text">Enable Interrupts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网卡监控"><span class="toc-text">网卡监控</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Using_ethtool_-S"><span class="toc-text">Using ethtool -S</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Using_sysfs"><span class="toc-text">Using sysfs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Using_/proc/net/dev"><span class="toc-text">Using /proc/net/dev</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网卡调优"><span class="toc-text">网卡调优</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#查看_RX_队列数量"><span class="toc-text">查看 RX 队列数量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#调整_RX_queues"><span class="toc-text">调整 RX queues</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#调整_RX_queue_的大小"><span class="toc-text">调整 RX queue 的大小</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#调整_RX_queue_的权重（weight）"><span class="toc-text">调整 RX queue 的权重（weight）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#调整_RX_哈希字段_for_network_flows"><span class="toc-text">调整 RX 哈希字段 for network flows</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ntuple_filtering_for_steering_network_flows"><span class="toc-text">ntuple filtering for steering network flows</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#软中断（SoftIRQ）"><span class="toc-text">软中断（SoftIRQ）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#软中断是什么"><span class="toc-text">软中断是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ksoftirqd"><span class="toc-text">ksoftirqd</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#__do_softirq"><span class="toc-text">__do_softirq</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#监控"><span class="toc-text">监控</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linux_网络设备子系统"><span class="toc-text">Linux 网络设备子系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#网络设备子系统的初始化"><span class="toc-text">网络设备子系统的初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#struct_softnet_data_变量初始化"><span class="toc-text">struct softnet_data 变量初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SoftIRQ_Handler_初始化"><span class="toc-text">SoftIRQ Handler 初始化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据来了"><span class="toc-text">数据来了</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#中断处理函数"><span class="toc-text">中断处理函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NAPI_和_napi_schedule"><span class="toc-text">NAPI 和 napi_schedule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#关于_CPU_和网络数据处理的一点笔记"><span class="toc-text">关于 CPU 和网络数据处理的一点笔记</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#监控网络数据到达"><span class="toc-text">监控网络数据到达</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#硬中断请求"><span class="toc-text">硬中断请求</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据接收调优"><span class="toc-text">数据接收调优</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#中断合并（Interrupt_coalescing）"><span class="toc-text">中断合并（Interrupt coalescing）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#调整硬中断亲和性（IRQ_affinities）"><span class="toc-text">调整硬中断亲和性（IRQ affinities）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#网络数据处理：开始"><span class="toc-text">网络数据处理：开始</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#net_rx_action_处理循环"><span class="toc-text">net_rx_action 处理循环</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NAPI_poll_函数及权重"><span class="toc-text">NAPI poll 函数及权重</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NAPI_和设备驱动的合约（contract）"><span class="toc-text">NAPI 和设备驱动的合约（contract）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Finishing_the_net_rx_action_loop"><span class="toc-text">Finishing the net_rx_action loop</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#到达_limit_时退出循环"><span class="toc-text">到达 limit 时退出循环</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NAPI_poll"><span class="toc-text">NAPI poll</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#igb_poll"><span class="toc-text">igb_poll</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#igb_clean_rx_irq"><span class="toc-text">igb_clean_rx_irq</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#监控网络数据处理"><span class="toc-text">监控网络数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#/proc/net/softnet_stat"><span class="toc-text">/proc/net/softnet_stat</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#网络数据处理调优"><span class="toc-text">网络数据处理调优</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#调整_net_rx_action_budget"><span class="toc-text">调整 net_rx_action budget</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GRO（Generic_Receive_Offloading）"><span class="toc-text">GRO（Generic Receive Offloading）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#使用_ethtool_修改_GRO_配置"><span class="toc-text">使用 ethtool 修改 GRO 配置</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#napi_gro_receive"><span class="toc-text">napi_gro_receive</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dev_gro_receive"><span class="toc-text">dev_gro_receive</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#napi_skb_finish"><span class="toc-text">napi_skb_finish</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RPS_(Receive_Packet_Steering)"><span class="toc-text">RPS (Receive Packet Steering)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RPS_调优"><span class="toc-text">RPS 调优</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RFS_(Receive_Flow_Steering)"><span class="toc-text">RFS (Receive Flow Steering)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#调优：打开_RFS"><span class="toc-text">调优：打开 RFS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#aRFS_(Hardware_accelerated_RFS)"><span class="toc-text">aRFS (Hardware accelerated RFS)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#调优:_启用_aRFS"><span class="toc-text">调优: 启用 aRFS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#从_netif_receive_skb_进入协议栈"><span class="toc-text">从 netif_receive_skb 进入协议栈</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#调优:_收包打时间戳（RX_packet_timestamping）"><span class="toc-text">调优: 收包打时间戳（RX packet timestamping）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#netif_receive_skb"><span class="toc-text">netif_receive_skb</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#不使用_RPS（默认）"><span class="toc-text">不使用 RPS（默认）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#使用_RPS"><span class="toc-text">使用 RPS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enqueue_to_backlog"><span class="toc-text">enqueue_to_backlog</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Flow_limits"><span class="toc-text">Flow limits</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#监控：由于_input_pkt_queue_打满或_flow_limit_导致的丢包"><span class="toc-text">监控：由于 input_pkt_queue 打满或 flow limit 导致的丢包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#调优"><span class="toc-text">调优</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Tuning:_Adjusting_netdev_max_backlog_to_prevent_drops"><span class="toc-text">Tuning: Adjusting netdev_max_backlog to prevent drops</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Tuning:_Adjust_the_NAPI_weight_of_the_backlog_poll_loop"><span class="toc-text">Tuning: Adjust the NAPI weight of the backlog poll loop</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Tuning:_Enabling_flow_limits_and_tuning_flow_limit_hash_table_size"><span class="toc-text">Tuning: Enabling flow limits and tuning flow limit hash table size</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#处理_backlog_队列：NAPI_poller"><span class="toc-text">处理 backlog 队列：NAPI poller</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#process_backlog"><span class="toc-text">process_backlog</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#__netif_receive_skb_core：将数据送到抓包点（tap）或协议层"><span class="toc-text">__netif_receive_skb_core：将数据送到抓包点（tap）或协议层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#送到抓包点（tap）"><span class="toc-text">送到抓包点（tap）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#送到协议层"><span class="toc-text">送到协议层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#协议层注册"><span class="toc-text">协议层注册</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#IP_协议层"><span class="toc-text">IP 协议层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ip_rcv"><span class="toc-text">ip_rcv</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#netfilter_and_iptables"><span class="toc-text">netfilter and iptables</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ip_rcv_finish"><span class="toc-text">ip_rcv_finish</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#调优:_打开或关闭_IP_协议的_early_demux_选项"><span class="toc-text">调优: 打开或关闭 IP 协议的 early demux 选项</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ip_local_deliver"><span class="toc-text">ip_local_deliver</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ip_local_deliver_finish"><span class="toc-text">ip_local_deliver_finish</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Monitoring:_IP_protocol_layer_statistics"><span class="toc-text">Monitoring: IP protocol layer statistics</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#高层协议注册"><span class="toc-text">高层协议注册</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UDP_协议层"><span class="toc-text">UDP 协议层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#udp_rcv"><span class="toc-text">udp_rcv</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#__udp4_lib_rcv"><span class="toc-text">__udp4_lib_rcv</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-3-3_udp_queue_rcv_skb"><span class="toc-text">11.3.3 udp_queue_rcv_skb</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sk_rcvqueues_full"><span class="toc-text">sk_rcvqueues_full</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#调优:_Socket_receive_queue_memory"><span class="toc-text">调优: Socket receive queue memory</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#udp_queue_rcv_skb"><span class="toc-text">udp_queue_rcv_skb</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#__udp_queue_rcv_skb"><span class="toc-text">__udp_queue_rcv_skb</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Monitoring:_UDP_protocol_layer_statistics"><span class="toc-text">Monitoring: UDP protocol layer statistics</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#监控_UDP_协议统计：/proc/net/snmp"><span class="toc-text">监控 UDP 协议统计：/proc/net/snmp</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#监控_UDP_socket_统计：/proc/net/udp"><span class="toc-text">监控 UDP socket 统计：/proc/net/udp</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#将数据放到_socket_队列"><span class="toc-text">将数据放到 socket 队列</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#其他"><span class="toc-text">其他</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#打时间戳_(timestamping)"><span class="toc-text">打时间戳 (timestamping)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#socket_低延迟选项：busy_polling"><span class="toc-text">socket 低延迟选项：busy polling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Netpoll：特殊网络场景支持"><span class="toc-text">Netpoll：特殊网络场景支持</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SO_INCOMING_CPU"><span class="toc-text">SO_INCOMING_CPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DMA_引擎"><span class="toc-text">DMA 引擎</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Intel’s_I/O_Acceleration_Technology_(IOAT)"><span class="toc-text">Intel’s I/O Acceleration Technology (IOAT)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#直接缓存访问_(DCA,_Direct_cache_access)"><span class="toc-text">直接缓存访问 (DCA, Direct cache access)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Monitoring_IOAT_DMA_engine"><span class="toc-text">Monitoring IOAT DMA engine</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Tuning_IOAT_DMA_engine"><span class="toc-text">Tuning IOAT DMA engine</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#额外讨论和帮助"><span class="toc-text">额外讨论和帮助</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#相关文章"><span class="toc-text">相关文章</span></a></li></ol>
      </div>
      
      
      
      <script>function show_answer(btn, x) { if (btn.value === "显示答案") { btn.value = "隐藏答案" } else { btn.value = "显示答案" } var as = document.getElementById(x); if (as.style.display === "none") { as.style.display = "block" } else { as.style.display = "none" } }</script>
      <p>这是一个关于Linux网络监控和调优系列文章的一篇，详尽的介绍了网络知识的各个方面，得到了广泛的转发和称赞，也被翻译成了多个中文版。本文转载的是携程的ArthurChiao翻译的版本，翻译通顺，还很有心的加上了章节，更方便阅读。他还翻译很多有价值的文章，读者可以到他的<a href="https://arthurchiao.github.io/" target="_blank" rel="external">网站</a>上阅读。</p>
<p>英文原文: <a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/" target="_blank" rel="external">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a></p>
<p>中文翻译： <a href="https://arthurchiao.github.io/blog/tuning-stack-rx-zh/" target="_blank" rel="external">[译] Linux 网络栈监控和调优：接收数据（2016）</a></p>
<blockquote>
<p>这让我不禁想起前两年很火的话题： “当你在浏览器中访问一个网址的时候，背后发生了什么？”，有多种多样的回答，将细节掰的很细，但是我还没有看到一篇能将网络连接和传输介绍的像本文这么细的文章。</p>
</blockquote>
<a id="more"></a>
<p><strong>太长不读（TL; DR）</strong></p>
<p>本文介绍了 Linux 内核是如何<strong>收包</strong>（receive packets）的，包是怎样从网络栈到达用<br>户空间程序的，以及如何<strong>监控</strong>（monitoring）和<strong>调优</strong>（tuning）这一路径上的各个<br>网络栈组件。</p>
<p>这篇文章的姊妹篇 <a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/" target="_blank" rel="external">Monitoring and Tuning the Linux Networking Stack: Sending Data</a>。</p>
<p>这篇文章的图文注释版 <a href="https://blog.packagecloud.io/eng/2016/10/11/monitoring-tuning-linux-networking-stack-receiving-data-illustrated/" target="_blank" rel="external">the Illustrated Guide to Monitoring and Tuning the Linux Networking Stack: Receiving Data</a>。</p>
<p>想对 Linux 网络栈进行监控或调优，必须对它的行为（what exactly is happening）和原理有深入的理解，而这是离不开读内核源码的。希望本文可以给那些正准备投身于此的人提供一份参考。</p>
<p><strong>特别鸣谢</strong></p>
<p>特别感谢 <a href="https://privateinternetaccess.com/" target="_blank" rel="external">Private Internet Access</a> 的各位同僚。公司雇佣我们做一些包括本文主题在内的网络研究，并非常慷慨地允许我们将研究成果以文章的形式发表。</p>
<p>本文基于在 <a href="https://privateinternetaccess.com/" target="_blank" rel="external">Private Internet Access</a> 时的研究成果，最开始以 <a href="https://www.privateinternetaccess.com/blog/2016/01/linux-networking-stack-from-the-ground-up-part-1/" target="_blank" rel="external">5 篇连载</a>的形式出现。</p>
<h2 id="监控和调优网络栈：常规建议">监控和调优网络栈：常规建议</h2>
<p>网络栈很复杂，没有一种通用的方式适用于所有场景。如果网络的性能和健康（performance and health）对你或你的业务非常关键，那你没有别的选择，只能花大量的时间、精力和金钱去深入理解系统的各个部分之间是如何交互的。</p>
<p>理想情况下，你应该考虑在网络栈的各层测量丢包状况，这样就可以缩小范围，确定哪个组件需要调优。</p>
<p><strong>然而，这也是一些网络管理员开始走偏的地方</strong>：他们想当然地认为通过一波 <code>sysctl</code>或 <code>/proc</code> 操作可以解决问题，并且这些配置适用于所有场景。在某些场景下，可能确实如此；但是，整个系统是如此细微而精巧地交织在一起，如果想做有意义的监控和调优，你必须得努力在更深层次搞清系统是如何工作的。否则，你虽然可以使用默认配置，并在相当长的时间内运行良好，但终会到某个时间点，你不得不（投时间、精力和金钱研究这些配置，然后）做优化。</p>
<p>本文中的一些示例配置仅为了方便理解（效果），并不作为任何特定配置或默认配置的建议。在做任何配置改动之前，你应该有一个能够对系统进行监控的框架，以看变更是否带来预期的效果。</p>
<p>对远程连接上的机器进行网络变更是相当危险的，机器很可能失联。另外，不要在生产环境直接调整这些配置；如果可能的话，在新机器上改配置，然后将机器灰度上线到生产。</p>
<h2 id="收包过程俯瞰">收包过程俯瞰</h2>
<p>本文将拿 <strong>Intel I350</strong> 网卡的 <code>igb</code> 驱动作为参考，网卡的 data sheet 这里可以下载<br><a href="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/ethernet-controller-i350-datasheet.pdf" target="_blank" rel="external">PDF</a>（警告：文件很大）。</p>
<p>从比较高的层次看，一个数据包从被网卡接收到进入 socket 接收队列的整个过程如下：</p>
<ol>
<li>加载网卡驱动，初始化</li>
<li>包从外部网络进入网卡</li>
<li>网卡（通过 DMA）将包 copy 到内核内存中的 ring buffer</li>
<li>产生硬件中断，通知系统收到了一个包</li>
<li>驱动调用 NAPI，如果轮询（poll）还没开始，就开始轮询</li>
<li><code>ksoftirqd</code> 进程调用 NAPI 的 <code>poll</code> 函数从 ring buffer 收包（<code>poll</code> 函数是网卡驱动在初始化阶段注册的；每个 CPU 上都运行着一个 <code>ksoftirqd</code> 进程，在系统启动期间就注册了）</li>
<li>ring buffer 里包对应的内存区域解除映射（unmapped）</li>
<li>（通过 DMA 进入）内存的数据包以 <code>skb</code> 的形式被送至更上层处理</li>
<li>如果 packet steering 功能打开，或者网卡有多队列，网卡收到的包会被分发到多个 CPU</li>
<li>包从队列进入协议层</li>
<li>协议层处理包</li>
<li>包从协议层进入相应 socket 的接收队列</li>
</ol>
<p>接下来会详细介绍这个过程。</p>
<p>协议层分析我们将会关注 IP 和 UDP 层，其他协议层可参考这个过程。</p>
<h2 id="网络设备驱动">网络设备驱动</h2>
<p>本文基于 Linux 3.13。</p>
<p>准确地理解 Linux 内核的收包过程是一件非常有挑战性的事情。我们需要仔细研究网卡驱动的工作原理，才能对网络栈的相应部分有更加清晰的理解。</p>
<p>本文将拿 <code>ibg</code> 驱动作为例子，它是常见的 Intel I350 网卡的驱动。先来看网卡驱动是如何工作的。</p>
<h3 id="初始化">初始化</h3>
<p>驱动会使用 <code>module_init</code> 向内核注册一个初始化函数，当驱动被加载时，内核会调用这个函数。</p>
<p>这个初始化函数（<code>igb_init_module</code>）的代码见 <a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L676-L697" target="_blank" rel="external"><code>drivers/net/ethernet/intel/igb/igb_main.c</code></a>.</p>
<p>过程非常简单直接：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> *  igb_init_module - Driver Registration Routine</div><div class="line"> *</div><div class="line"> *  igb_init_module is the first routine called when the driver is</div><div class="line"> *  loaded. All it does is register with the PCI subsystem.</div><div class="line"> **/</div><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> __init igb_init_module(<span class="keyword">void</span>)</div><div class="line">{</div><div class="line">  <span class="keyword">int</span> ret;</div><div class="line">  pr_info(<span class="string">"%s - version %s\n"</span>, igb_driver_string, igb_driver_version);</div><div class="line">  pr_info(<span class="string">"%s\n"</span>, igb_copyright);</div><div class="line"></div><div class="line">  <span class="comment">/* ... */</span></div><div class="line"></div><div class="line">  ret = pci_register_driver(&igb_driver);</div><div class="line">  <span class="keyword">return</span> ret;</div><div class="line">}</div><div class="line"></div><div class="line">module_init(igb_init_module);</div></pre></td></tr></table></figure>

<p>初始化的大部分工作在 <code>pci_register_driver</code> 里面完成，下面来细看。</p>
<h4 id="PCI_初始化">PCI 初始化</h4>
<p>Intel I350 网卡是 <a href="https://en.wikipedia.org/wiki/PCI_Express" target="_blank" rel="external">PCI express</a> 设备。<br>PCI 设备通过 <a href="https://en.wikipedia.org/wiki/PCI_configuration_space#Standardized_registers" target="_blank" rel="external">PCI Configuration<br>Space</a><br>里面的寄存器识别自己。</p>
<p>当设备驱动编译时，<code>MODULE_DEVICE_TABLE</code> 宏（定义在<br><a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/module.h#L145-L146" target="_blank" rel="external"><code>include/module.h</code></a>）<br>会导出一个 <strong>PCI 设备 ID 列表</strong>（a table of PCI device IDs），驱动据此识别它可以控制的设备，内核也会依据这个列表对不同设备加载相应驱动。</p>
<p><code>igb</code> 驱动的设备表和 PCI 设备 ID 分别见：<br><a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L79-L117" target="_blank" rel="external">drivers/net/ethernet/intel/igb/igb_main.c</a><br>和<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/e1000_hw.h#L41-L75" target="_blank" rel="external">drivers/net/ethernet/intel/igb/e1000_hw.h</a>。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> DEFINE_PCI_DEVICE_TABLE(igb_pci_tbl) = {</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_1GBPS) },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_SGMII) },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_2_5GBPS) },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I211_COPPER), board_82575 },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER), board_82575 },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_FIBER), board_82575 },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES), board_82575 },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SGMII), board_82575 },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER_FLASHLESS), board_82575 },</div><div class="line">  { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES_FLASHLESS), board_82575 },</div><div class="line">  <span class="comment">/* ... */</span></div><div class="line">};</div><div class="line">MODULE_DEVICE_TABLE(pci, igb_pci_tbl);</div></pre></td></tr></table></figure>

<p>前面提到，驱动初始化的时候会调用 <code>pci_register_driver</code>，这个函数会将该驱动的各种回调方法注册到一个 <code>struct pci_driver</code> 变量，<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L238-L249" target="_blank" rel="external">drivers/net/ethernet/intel/igb/igb_main.c</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">struct</span> pci_driver igb_driver = {</div><div class="line">  .name     = igb_driver_name,</div><div class="line">  .id_table = igb_pci_tbl,</div><div class="line">  .probe    = igb_probe,</div><div class="line">  .remove   = igb_remove,</div><div class="line">  <span class="comment">/* ... */</span></div><div class="line">};</div></pre></td></tr></table></figure>

<h3 id="网络设备初始化">网络设备初始化</h3>
<p>通过 PCI ID 识别设备后，内核就会为它选择合适的驱动。每个 PCI 驱动注册了一个<code>probe()</code> 方法，内核会对每个设备依次调用其驱动的 <code>probe</code> 方法，一旦找到一个合适的驱动，就不会再为这个设备尝试其他驱动。</p>
<p>很多驱动都需要大量代码来使得设备 ready，具体做的事情各有差异。典型的过程：</p>
<ol>
<li>启用 PCI 设备</li>
<li>请求（requesting）内存范围和 IO 端口</li>
<li>设置 DMA 掩码</li>
<li>注册设备驱动支持的 ethtool 方法（后面介绍）</li>
<li>注册所需的 watchdog（例如，e1000e 有一个检测设备是否僵死的 watchdog）</li>
<li>其他和具体设备相关的事情，例如一些 workaround，或者特定硬件的非常规处理</li>
<li>创建、初始化和注册一个 <code>struct net_device_ops</code> 类型变量，这个变量包含了用于设备相关的回调函数，例如打开设备、发送数据到网络、设置 MAC 地址等</li>
<li>创建、初始化和注册一个更高层的 <code>struct net_device</code> 类型变量（一个变量就代表了一个设备）</li>
</ol>
<p>我们来简单看下 <code>igb</code> 驱动的 <code>igb_probe</code> 包含哪些过程。下面的代码来自 <code>igb_probe</code>，<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059" target="_blank" rel="external">drivers/net/ethernet/intel/igb/igb_main.c</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">err = pci_enable_device_mem(pdev);</div><div class="line"><span class="comment">/* ... */</span></div><div class="line">err = dma_set_mask_and_coherent(&pdev-&gt;dev, DMA_BIT_MASK(<span class="number">64</span>));</div><div class="line"><span class="comment">/* ... */</span></div><div class="line">err = pci_request_selected_regions(pdev, pci_select_bars(pdev,</div><div class="line">           IORESOURCE_MEM),</div><div class="line">           igb_driver_name);</div><div class="line"></div><div class="line">pci_enable_pcie_error_reporting(pdev);</div><div class="line"></div><div class="line">pci_set_master(pdev);</div><div class="line">pci_save_state(pdev);</div></pre></td></tr></table></figure>

<h4 id="更多_PCI_驱动信息">更多 PCI 驱动信息</h4>
<p>详细的 PCI 驱动讨论不在本文范围，如果想进一步了解，推荐如下材料：<br><a href="http://free-electrons.com/doc/pci-drivers.pdf" target="_blank" rel="external">分享</a>，<br><a href="http://wiki.osdev.org/PCI" target="_blank" rel="external">wiki</a>，<br><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/PCI/pci.txt" target="_blank" rel="external">Linux Kernel Documentation: PCI</a>。</p>
<h3 id="网络设备启动">网络设备启动</h3>
<p><code>igb_probe</code> 做了很多重要的设备初始化工作。除了 PCI 相关的，还有如下一些通用网络功能和网络设备相关的工作：</p>
<ol>
<li>注册 <code>struct net_device_ops</code> 变量</li>
<li>注册 ethtool 相关的方法</li>
<li>从网卡获取默认 MAC 地址</li>
<li>设置 <code>net_device</code> 特性标记</li>
</ol>
<p>我们逐一看下这些过程，后面会用到。</p>
<h4 id="struct_net_device_ops"><code>struct net_device_ops</code></h4>
<p>网络设备相关的操作函数都注册到这个类型的变量中。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059" target="_blank" rel="external">igb_main.c</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">struct</span> net_device_ops igb_netdev_ops = {</div><div class="line">  .ndo_open               = igb_open,</div><div class="line">  .ndo_stop               = igb_close,</div><div class="line">  .ndo_start_xmit         = igb_xmit_frame,</div><div class="line">  .ndo_get_stats64        = igb_get_stats64,</div><div class="line">  .ndo_set_rx_mode        = igb_set_rx_mode,</div><div class="line">  .ndo_set_mac_address    = igb_set_mac,</div><div class="line">  .ndo_change_mtu         = igb_change_mtu,</div><div class="line">  .ndo_do_ioctl           = igb_ioctl,</div><div class="line">  <span class="comment">/* ... */</span></div></pre></td></tr></table></figure>

<p>这个变量会在 <code>igb_probe()</code>中赋给 <code>struct net_device</code> 中的 <code>netdev_ops</code> 字段：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> igb_probe(<span class="keyword">struct</span> pci_dev *pdev, <span class="keyword">const</span> <span class="keyword">struct</span> pci_device_id *ent)</div><div class="line">{</div><div class="line">  ...</div><div class="line">  netdev-&gt;netdev_ops = &igb_netdev_ops;</div><div class="line">}</div></pre></td></tr></table></figure>

<h4 id="ethtool_函数注册"><code>ethtool</code> 函数注册</h4>
<p><a href="https://www.kernel.org/pub/software/network/ethtool/" target="_blank" rel="external"><code>ethtool</code></a> 是一个命令行工具，可以查看和修改网络设备的一些配置，常用于收集网卡统计数据。在 Ubuntu 上，可以通过 <code>apt-get install ethtool</code> 安装。</p>
<p><code>ethtool</code> 通过 <a href="http://man7.org/linux/man-pages/man2/ioctl.2.html" target="_blank" rel="external">ioctl</a> 和设备驱动通信。内核实现了一个通用 <code>ethtool</code> 接口，网卡驱动实现这些接口，就可以被<code>ethtool</code> 调用。当 <code>ethtool</code> 发起一个系统调用之后，内核会找到对应操作的回调函数。回调实现了各种简单或复杂的函数，简单的如改变一个 flag 值，复杂的包括调整网卡硬件如何运行。</p>
<p>相关实现见：<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_ethtool.c" target="_blank" rel="external">igb_ethtool.c</a>。</p>
<h4 id="软中断（IRQ）">软中断（IRQ）</h4>
<p>当一个数据帧通过 DMA 写到 RAM（内存）后，网卡是如何通知其他系统这个包可以被处理了呢？</p>
<p>传统的方式是，网卡会产生一个硬件中断（IRQ），通知数据包到了。有<strong>三种常见的硬中断类型</strong>：</p>
<ul>
<li>MSI-X</li>
<li>MSI</li>
<li>legacy IRQ</li>
</ul>
<p>稍后详细介绍到。</p>
<p>先来思考这样一个问题：如果有大量的数据包到达，就会产生大量的硬件中断。CPU 忙于处理硬件中断的时候，可用于处理其他任务的时间就会减少。</p>
<p>NAPI（New API）是一种新的机制，可以减少产生的硬件中断的数量（但不能完全消除硬中断）。</p>
<h4 id="NAPI">NAPI</h4>
<h5 id="NAPI-1">NAPI</h5>
<p>NAPI 接收数据包的方式和传统方式不同，它允许设备驱动注册一个 <code>poll</code> 方法，然后调用这个方法完成收包。</p>
<p>NAPI 的使用方式：</p>
<ol>
<li>驱动打开 NAPI 功能，默认处于未工作状态（没有在收包）</li>
<li>数据包到达，网卡通过 DMA 写到内存</li>
<li>网卡触发一个硬中断，<strong>中断处理函数开始执行</strong></li>
<li>软中断（softirq，稍后介绍），唤醒 NAPI 子系统。这会触发<strong>在一个单独的线程里，调用驱动注册的 <code>poll</code> 方法收包</strong></li>
<li>驱动禁止网卡产生新的硬件中断。这样做是为了 NAPI 能够在收包的时候不会被新的中断打扰</li>
<li>一旦没有包需要收了，NAPI 关闭，网卡的硬中断重新开启</li>
<li>转步骤 2</li>
</ol>
<p>和传统方式相比，NAPI 一次中断会接收多个包，因此可以减少硬件中断的数量。</p>
<p><code>poll</code> 方法是通过调用 <code>netif_napi_add</code> 注册到 NAPI 的，同时还可以指定权重<code>weight</code>，大部分驱动都 hardcode 为 64。后面会进一步解释这个 weight 以及 hardcode 64。</p>
<p>通常来说，驱动在初始化的时候注册 NAPI poll 方法。</p>
<h4 id="igb_驱动的_NAPI_初始化"><code>igb</code> 驱动的 NAPI 初始化</h4>
<p><code>igb</code> 驱动的初始化过程是一个很长的调用链：</p>
<ol>
<li><code>igb_probe</code> <code>-&gt;</code> <code>igb_sw_init</code></li>
<li><code>igb_sw_init</code> <code>-&gt;</code> <code>igb_init_interrupt_scheme</code></li>
<li><code>igb_init_interrupt_scheme</code> <code>-&gt;</code> <code>igb_alloc_q_vectors</code></li>
<li><code>igb_alloc_q_vectors</code> <code>-&gt;</code> <code>igb_alloc_q_vector</code></li>
<li><code>igb_alloc_q_vector</code> <code>-&gt;</code> <code>netif_napi_add</code></li>
</ol>
<p>从较高的层面来看，这个调用过程会做以下事情：</p>
<ol>
<li>如果支持 <code>MSI-X</code>，调用 <code>pci_enable_msix</code> 打开它</li>
<li>计算和初始化一些配置，包括网卡收发队列的数量</li>
<li>调用 <code>igb_alloc_q_vector</code> 创建每个发送和接收队列</li>
<li><code>igb_alloc_q_vector</code> 会进一步调用 <code>netif_napi_add</code> 注册 poll 方法到 NAPI 变量</li>
</ol>
<p>我们来看下 <code>igb_alloc_q_vector</code> 是如何注册 poll 方法和私有数据的：<br><a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1145-L1271" target="_blank" rel="external">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> igb_alloc_q_vector(<span class="keyword">struct</span> igb_adapter *adapter,</div><div class="line">                              <span class="keyword">int</span> v_count, <span class="keyword">int</span> v_idx,</div><div class="line">                              <span class="keyword">int</span> txr_count, <span class="keyword">int</span> txr_idx,</div><div class="line">                              <span class="keyword">int</span> rxr_count, <span class="keyword">int</span> rxr_idx)</div><div class="line">{</div><div class="line">  <span class="comment">/* ... */</span></div><div class="line"></div><div class="line">  <span class="comment">/* allocate q_vector and rings */</span></div><div class="line">  q_vector = kzalloc(size, GFP_KERNEL);</div><div class="line">  <span class="keyword">if</span> (!q_vector)</div><div class="line">          <span class="keyword">return</span> -ENOMEM;</div><div class="line"></div><div class="line">  <span class="comment">/* initialize NAPI */</span></div><div class="line">  netif_napi_add(adapter-&gt;netdev, &q_vector-&gt;napi, igb_poll, <span class="number">64</span>);</div><div class="line"></div><div class="line">  <span class="comment">/* ... */</span></div></pre></td></tr></table></figure>

<p><code>q_vector</code> 是新分配的队列，<code>igb_poll</code> 是 poll 方法，当它收包的时候，会通过<br>这个接收队列找到关联的 NAPI 变量（<code>q_vector-&gt;napi</code>）。</p>
<p>这里很重要，后面我们介绍从驱动到网络协议栈的 flow（根据 IP 头信息做哈希，哈希相<br>同的属于同一个 flow）时会看到。</p>
<h3 id="启用网卡_(Bring_A_Network_Device_Up)">启用网卡 (Bring A Network Device Up)</h3>
<p>回忆前面我们提到的 <code>structure net_device_ops</code> 变量，它包含网卡启用、发包、设置mac 地址等回调函数（函数指针）。</p>
<p>当启用一个网卡时（例如，通过 <code>ifconfig eth0 up</code>），<code>net_device_ops</code> 的 <code>ndo_open</code>方法会被调用。它通常会做以下事情：</p>
<ol>
<li>分配 RX、TX 队列内存</li>
<li>打开 NAPI 功能</li>
<li>注册中断处理函数</li>
<li>打开（enable）硬中断</li>
<li>其他</li>
</ol>
<p><code>igb</code> 驱动中，这个方法对应的是 <code>igb_open</code> 函数。</p>
<h4 id="准备从网络接收数据">准备从网络接收数据</h4>
<p>今天的大部分网卡都<strong>使用 DMA 将数据直接写到内存</strong>，接下来<strong>操作系统可以直接从里面读取</strong>。实现这一目的所使用的数据结构是 ring buffer（环形缓冲区）。</p>
<p>要实现这一功能，设备驱动必须和操作系统合作，<strong>预留（reserve）出一段内存来给网卡使用</strong>。预留成功后，网卡知道了这块内存的地址，接下来收到的包就会放到这里，进而被操作系统取走。</p>
<p>由于这块内存区域是有限的，如果数据包的速率非常快，单个 CPU 来不及取走这些包，新来的包就会被丢弃。这时候，Receive Side Scaling（RSS，接收端扩展）或者多队列（multiqueue）一类的技术可能就会排上用场。</p>
<p>一些网卡有能力将接收到的包写到<strong>多个不同的内存区域，每个区域都是独立的接收队列</strong>。这样操作系统就可以利用多个 CPU（硬件层面）并行处理收到的包。只有部分网卡支持这个功能。</p>
<p>Intel I350 网卡支持多队列，我们可以在 <code>igb</code> 的驱动里看出来。<code>igb</code> 驱动启用的时候，最开始做的事情之一就是调用<br><a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2801-L2804" target="_blank" rel="external"><code>igb_setup_all_rx_resources</code></a>函数。这个函数会对每个 RX 队列调用 <code>igb_setup_rx_resources</code>, 里面会管理 DMA的内存.</p>
<p>如果对其原理感兴趣，可以进一步查看 <a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/DMA-API-HOWTO.txt" target="_blank" rel="external">Linux kernel’s DMA API HOWTO</a>。</p>
<p>RX 队列的数量和大小可以通过 ethtool 进行配置，调整这两个参数会对收包或者丢包产生可见影响。</p>
<p>网卡通过对 packet 头（例如源地址、目的地址、端口等）做哈希来决定将 packet 放到哪个 RX 队列。只有很少的网卡支持调整哈希算法。如果支持的话，那你可以根据算法将特定的 flow 发到特定的队列，甚至可以做到在硬件层面直接将某些包丢弃。</p>
<p>一些网卡支持调整 RX 队列的权重，你可以有意地将更多的流量发到指定的 queue。</p>
<p>后面会介绍如何对这些参数进行调优。</p>
<h4 id="Enable_NAPI">Enable NAPI</h4>
<p>前面看到了驱动如何注册 NAPI <code>poll</code> 方法，但是，一般直到网卡被启用之后，NAPI 才被启用。</p>
<p>启用 NAPI 很简单，调用 <code>napi_enable</code> 函数就行，这个函数会设置 NAPI 变量（<code>struct napi_struct</code>）中一个表示是否启用的标志位。前面说到，NAPI 启用后并不是立即开始工作（而是等硬中断触发）。</p>
<p>对于 <code>igb</code>，驱动初始化或者通过 ethtool 修改 queue 数量或大小的时候，会启用每个<code>q_vector</code> 的 NAPI 变量。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2833-L2834" target="_blank" rel="external">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; adapter-&gt;num_q_vectors; i++)</div><div class="line">  napi_enable(&(adapter-&gt;q_vector[i]-&gt;napi));</div></pre></td></tr></table></figure>

<h4 id="注册中断处理函数">注册中断处理函数</h4>
<p>启用 NAPI 之后，下一步就是注册中断处理函数。设备有多种方式触发一个中断：</p>
<ul>
<li>MSI-X</li>
<li>MSI</li>
<li>legacy interrupts</li>
</ul>
<p>设备驱动的实现也因此而异。</p>
<p>驱动必须判断出设备支持哪种中断方式，然后注册相应的中断处理函数，这些函数在中断发生的时候会被执行。</p>
<p>一些驱动，例如 <code>igb</code>，会试图为每种中断类型注册一个中断处理函数，如果注册失败，就尝试下一种（没测试过的）类型。</p>
<p><strong>MSI-X 中断是比较推荐的方式，尤其是对于支持多队列的网卡</strong>。因为每个 RX 队列有独立的MSI-X 中断，因此可以被不同的 CPU 处理（通过 <code>irqbalance</code> 方式，或者修改<br><code>/proc/irq/IRQ_NUMBER/smp_affinity</code>）。我们后面会看到，处理中断的 CPU 也是随后处理这个包的 CPU。这样的话，从网卡硬件中断的层面就可以设置让收到的包被不同的 CPU处理。</p>
<p>如果不支持 MSI-X，那 MSI 相比于传统中断方式仍然有一些优势，驱动仍然会优先考虑它。这个 <a href="https://en.wikipedia.org/wiki/Message_Signaled_Interrupts" target="_blank" rel="external">wiki</a> 介绍了更多关于 MSI 和 MSI-X 的信息。</p>
<p>在 <code>igb</code> 驱动中，函数 <code>igb_msix_ring</code>, <code>igb_intr_msi</code>, <code>igb_intr</code> 分别是 MSI-X,MSI, 和传统中断方式的中断处理函数。</p>
<p>这段代码显式了驱动是如何尝试各种中断类型的，<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1360-L1413" target="_blank" rel="external">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> igb_request_irq(<span class="keyword">struct</span> igb_adapter *adapter)</div><div class="line">{</div><div class="line">  <span class="keyword">struct</span> net_device *netdev = adapter-&gt;netdev;</div><div class="line">  <span class="keyword">struct</span> pci_dev *pdev = adapter-&gt;pdev;</div><div class="line">  <span class="keyword">int</span> err = <span class="number">0</span>;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (adapter-&gt;msix_entries) {</div><div class="line">    err = igb_request_msix(adapter);</div><div class="line">    <span class="keyword">if</span> (!err)</div><div class="line">      <span class="keyword">goto</span> request_done;</div><div class="line">    <span class="comment">/* fall back to MSI */</span></div><div class="line">    <span class="comment">/* ... */</span></div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="comment">/* ... */</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span> (adapter-&gt;flags & IGB_FLAG_HAS_MSI) {</div><div class="line">    err = request_irq(pdev-&gt;irq, igb_intr_msi, <span class="number">0</span>,</div><div class="line">          netdev-&gt;name, adapter);</div><div class="line">    <span class="keyword">if</span> (!err)</div><div class="line">      <span class="keyword">goto</span> request_done;</div><div class="line"></div><div class="line">    <span class="comment">/* fall back to legacy interrupts */</span></div><div class="line">    <span class="comment">/* ... */</span></div><div class="line">  }</div><div class="line"></div><div class="line">  err = request_irq(pdev-&gt;irq, igb_intr, IRQF_SHARED,</div><div class="line">        netdev-&gt;name, adapter);</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (err)</div><div class="line">    dev_err(&pdev-&gt;dev, <span class="string">"Error %d getting interrupt\n"</span>, err);</div><div class="line"></div><div class="line">request_done:</div><div class="line">  <span class="keyword">return</span> err;</div><div class="line">}</div></pre></td></tr></table></figure>

<p>这就是 <code>igb</code> 驱动注册中断处理函数的过程，这个函数在一个包到达网卡触发一个硬件中断时就会被执行。</p>
<h4 id="Enable_Interrupts">Enable Interrupts</h4>
<p>到这里，几乎所有的准备工作都就绪了。唯一剩下的就是打开硬中断，等待数据包进来。打开硬中断的方式因硬件而异，<code>igb</code> 驱动是在 <code>__igb_open</code> 里调用辅助函数<code>igb_irq_enable</code> 完成的。</p>
<p>中断通过写寄存器的方式打开：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">void</span> igb_irq_enable(<span class="keyword">struct</span> igb_adapter *adapter)</div><div class="line">{</div><div class="line"></div><div class="line">  <span class="comment">/* ... */</span></div><div class="line">    wr32(E1000_IMS, IMS_ENABLE_MASK | E1000_IMS_DRSTA);</div><div class="line">    wr32(E1000_IAM, IMS_ENABLE_MASK | E1000_IMS_DRSTA);</div><div class="line">  <span class="comment">/* ... */</span></div><div class="line">}</div></pre></td></tr></table></figure>

<p>现在，网卡已经启用了。驱动可能还会做一些额外的事情，例如启动定时器，工作队列（work queue），或者其他硬件相关的设置。这些工作做完后，网卡就可以收包了。</p>
<p>接下来看一下如何监控和调优网卡。</p>
<h3 id="网卡监控">网卡监控</h3>
<p>监控网络设备有几种不同的方式，每种方式的监控粒度（granularity）和复杂度不同。我们先从最粗的粒度开始，逐步细化。</p>
<h4 id="Using_ethtool_-S">Using <code>ethtool -S</code></h4>
<p><code>ethtool -S</code> 可以查看网卡统计信息（例如丢弃的包数量）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -S eth0</div><div class="line">NIC statistic<span class="variable">s:</span></div><div class="line">     rx_packet<span class="variable">s:</span> <span class="number">597028087</span></div><div class="line">     tx_packet<span class="variable">s:</span> <span class="number">5924278060</span></div><div class="line">     rx_byte<span class="variable">s:</span> <span class="number">112643393747</span></div><div class="line">     tx_byte<span class="variable">s:</span> <span class="number">990080156714</span></div><div class="line">     rx_broadcas<span class="variable">t:</span> <span class="number">96</span></div><div class="line">     tx_broadcas<span class="variable">t:</span> <span class="number">116</span></div><div class="line">     rx_multicas<span class="variable">t:</span> <span class="number">20294528</span></div><div class="line">     ....</div></pre></td></tr></table></figure>

<p>监控这些数据比较困难。因为用命令行获取很容易，但是以上字段并没有一个统一的标准。不同的驱动，甚至同一驱动的不同版本可能字段都会有差异。</p>
<p>你可以先粗略的查看 “drop”, “buffer”, “miss” 等字样。然后，在驱动的源码里找到对应的更新这些字段的地方，这可能是在软件层面更新的，也有可能是在硬件层面通过寄存器更新的。如果是通过硬件寄存器的方式，你就得查看网卡的 data sheet（说明书），搞清楚这个寄存器代表什么。ethtoool 给出的这些字段名，有一些是有误导性的（misleading）。</p>
<h4 id="Using_sysfs">Using <code>sysfs</code></h4>
<p>sysfs 也提供了统计信息，但相比于网卡层的统计，要更上层一些。</p>
<p>例如，获取 eth0 的接收端 drop 的数量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ cat <span class="regexp">/sys/</span><span class="keyword">class</span><span class="regexp">/net/</span>eth0<span class="regexp">/statistics/</span>rx_dropped</div><div class="line"><span class="number">2</span></div></pre></td></tr></table></figure>

<p>不同类型的统计分别位于<code>/sys/class/net/&lt;NIC&gt;/statistics/</code> 下面的不同文件，包括<code>collisions</code>, <code>rx_dropped</code>, <code>rx_errors</code>, <code>rx_missed_errors</code> 等等。</p>
<p>不幸的是，每种类型代表什么意思，是由驱动来决定的，因此也是由驱动决定何时以及在哪里更新这些计数的。你可能会发现一些驱动将一些特定类型的错误归类为 drop，而另外一些驱动可能将它们归类为 miss。</p>
<p>这些值至关重要，因此你需要查看对应的网卡驱动，搞清楚它们真正代表什么。</p>
<h4 id="Using_/proc/net/dev">Using <code>/proc/net/dev</code></h4>
<p><code>/proc/net/dev</code> 提供了更高一层的网卡统计。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ cat /<span class="keyword">proc</span>/net/dev</div><div class="line"><span class="type">Inter</span>-|   <span class="type">Receive</span>                                                |  <span class="type">Transmit</span></div><div class="line"> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed</div><div class="line">  eth0: <span class="number">110346752214</span> <span class="number">597737500</span>    <span class="number">0</span>    <span class="number">2</span>    <span class="number">0</span>     <span class="number">0</span>          <span class="number">0</span>  <span class="number">20963860</span> <span class="number">990024805984</span> <span class="number">6066582604</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     <span class="number">0</span>       <span class="number">0</span>          <span class="number">0</span></div><div class="line">    lo: <span class="number">428349463836</span> <span class="number">1579868535</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     <span class="number">0</span>          <span class="number">0</span>         <span class="number">0</span> <span class="number">428349463836</span> <span class="number">1579868535</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>     <span class="number">0</span>       <span class="number">0</span>          <span class="number">0</span></div></pre></td></tr></table></figure>

<p>这个文件里显式的统计只是 sysfs 里面的一个子集，但适合作为一个常规的统计参考。</p>
<p>前面的警告（caveat）也适用于此：如果这些数据对你非常重要，那你必须得查看内核源码、驱动源码和驱动手册，搞清楚每个字段真正代表什么意思，计数是如何以及何时被更新的。</p>
<h3 id="网卡调优">网卡调优</h3>
<h4 id="查看_RX_队列数量">查看 RX 队列数量</h4>
<p>如果网卡及其驱动支持 RSS/多队列，那你可以会调整 RX queue（也叫 RX channel）的数量。这可以用 ethtool 完成。</p>
<p>查看 RX queue 数量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -l eth0</div><div class="line">Channel parameters for eth0:</div><div class="line">Pre-<span class="keyword">set</span> maximums:</div><div class="line"><span class="label">RX:</span>   <span class="number">0</span></div><div class="line"><span class="label">TX:</span>   <span class="number">0</span></div><div class="line"><span class="label">Other:</span>    <span class="number">0</span></div><div class="line"><span class="label">Combined:</span> <span class="number">8</span></div><div class="line">Current hardware settings:</div><div class="line"><span class="label">RX:</span>   <span class="number">0</span></div><div class="line"><span class="label">TX:</span>   <span class="number">0</span></div><div class="line"><span class="label">Other:</span>    <span class="number">0</span></div><div class="line"><span class="label">Combined:</span> <span class="number">4</span></div></pre></td></tr></table></figure>

<p>这里可以看到允许的最大值（网卡及驱动限制），以及当前设置的值。</p>
<p>注意：不是所有网卡驱动都支持这个操作。如果你的网卡不支持，会看到如下类似的错误：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool <span class="operator">-l</span> eth0</div><div class="line">Channel parameters <span class="keyword">for</span> eth0:</div><div class="line">Cannot get device channel parameters</div><div class="line">: Operation not supported</div></pre></td></tr></table></figure>

<p>这意味着驱动没有实现 ethtool 的 <code>get_channels</code> 方法。可能的原因包括：该网卡不支持调整 RX queue 数量，不支持 RSS/multiqueue，或者驱动没有更新来支持此功能。</p>
<h4 id="调整_RX_queues">调整 RX queues</h4>
<p><code>ethtool -L</code> 可以修改 RX queue 数量。</p>
<p>注意：一些网卡和驱动只支持 combined queue，这种模式下，RX queue 和 TX queue 是一对一绑定的，上面的例子我们看到的就是这种。</p>
<p>设置 combined 类型网卡的收发队列为 8 个：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -L eth0 combined <span class="number">8</span></div></pre></td></tr></table></figure>

<p>如果你的网卡支持独立的 RX 和 TX 队列数量，那你可以只修改 RX queue 数量：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -L eth0 rx <span class="number">8</span></div></pre></td></tr></table></figure>

<p>注意：对于大部分驱动，修改以上配置会使网卡先 down 再 up，因此会造成丢包。请酌情使用。</p>
<h4 id="调整_RX_queue_的大小">调整 RX queue 的大小</h4>
<p>一些网卡和驱动也支持修改 RX queue 的大小。底层是如何工作的，随硬件而异，但幸运的是，ethtool 提供了一个通用的接口来做这件事情。增加 RX queue 的大小可以在流量很大的时候缓解丢包问题，但是，只调整这个还不够，软件层面仍然可能会丢包，因此还需要其他的一些调优才能彻底的缓解或解决丢包问题。</p>
<p><code>ethtool -g</code> 可以查看 queue 的大小。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool <span class="attribute">-g</span> eth0</div><div class="line">Ring parameters for eth0:</div><div class="line">Pre<span class="attribute">-set</span> maximums:</div><div class="line"><span class="literal">RX</span>:   <span class="number">4096</span></div><div class="line"><span class="literal">RX</span> Mini:  <span class="number">0</span></div><div class="line"><span class="literal">RX</span> Jumbo: <span class="number">0</span></div><div class="line">TX:   <span class="number">4096</span></div><div class="line">Current hardware settings:</div><div class="line"><span class="literal">RX</span>:   <span class="number">512</span></div><div class="line"><span class="literal">RX</span> Mini:  <span class="number">0</span></div><div class="line"><span class="literal">RX</span> Jumbo: <span class="number">0</span></div><div class="line">TX:   <span class="number">512</span></div></pre></td></tr></table></figure>

<p>以上显式网卡支持最多 4096 个接收和发送 descriptor（描述符，简单理解，存放的是指向包的指针），但是现在只用到了 512 个。</p>
<p>用 <code>ethtool -G</code> 修改 queue 大小：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -G eth0 rx <span class="number">4096</span></div></pre></td></tr></table></figure>

<p>注意：对于大部分驱动，修改以上配置会使网卡先 down 再 up，因此会造成丢包。请酌情使用。</p>
<h4 id="调整_RX_queue_的权重（weight）">调整 RX queue 的权重（weight）</h4>
<p>一些网卡支持给不同的 queue 设置不同的权重，以此分发不同数量的网卡包到不同的队列。</p>
<p>如果你的网卡支持以下功能，那你可以使用：</p>
<ol>
<li>网卡支持 flow indirection（flow 重定向，flow 是什么前面提到过）</li>
<li>网卡驱动实现了 <code>get_rxfh_indir_size</code> 和 <code>get_rxfh_indir</code> 方法</li>
<li>使用的 ethtool 版本足够新，支持 <code>-x</code> 和 <code>-X</code> 参数来设置 indirection table</li>
</ol>
<p><code>ethtool -x</code> 检查 flow indirection 设置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -x eth0</div><div class="line">RX flow <span class="built_in">hash</span> indirection table <span class="keyword">for</span> eth3 with <span class="number">2</span> RX ring(s):</div><div class="line"><span class="number">0</span>: <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></div><div class="line"><span class="number">8</span>: <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></div><div class="line"><span class="number">16</span>: <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></div><div class="line"><span class="number">24</span>: <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span></div></pre></td></tr></table></figure>

<p>第一列是哈希值索引，是该行的第一个哈希值；冒号后面的是每个哈希值对于的 queue，例如，第一行分别是哈希值 0，1，2，3，4，5，6，7，对应的 packet 应该分别被放到 RX queue 0，1，0，1，0，1，0，1。</p>
<p>例子：在前两个 RX queue 之间均匀的分发（接收到的包）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -X eth0 equal <span class="number">2</span></div></pre></td></tr></table></figure>

<p>例子：用 <code>ethtool -X</code> 设置自定义权重：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -X eth0 weight <span class="number">6</span> <span class="number">2</span></div></pre></td></tr></table></figure>

<p>以上命令分别给 rx queue 0 和 rx queue 1 不同的权重：6 和 2，因此 queue 0 接收到的数量更多。注意 queue 一般是和 CPU 绑定的，因此这也意味着相应的 CPU 也会花更多的时间片在收包上。</p>
<p>一些网卡还支持修改计算 hash 时使用哪些字段。</p>
<h4 id="调整_RX_哈希字段_for_network_flows">调整 RX 哈希字段 for network flows</h4>
<p>可以用 ethtool 调整 RSS 计算哈希时所使用的字段。</p>
<p>例子：查看 UDP RX flow 哈希所使用的字段：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -n eth0 rx-flow-hash udp4</div><div class="line">UDP over IPV4 flows <span class="operator"><span class="keyword">use</span> these <span class="keyword">fields</span> <span class="keyword">for</span> computing Hash flow <span class="keyword">key</span>:</span></div><div class="line">IP SA</div><div class="line">IP DA</div></pre></td></tr></table></figure>

<p>可以看到只用到了源 IP（SA：Source Address）和目的 IP。</p>
<p>我们接下来修改一下，加入源端口和目的端口：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -N eth0 rx-flow-hash udp4 sdfn</div></pre></td></tr></table></figure>

<p><code>sdfn</code> 的具体含义解释起来有点麻烦，请查看 ethtool 的帮助（man page）。</p>
<p>调整 hash 所用字段是有用的，而 <code>ntuple</code> 过滤对于更加细粒度的 flow control 更加有用。</p>
<h4 id="ntuple_filtering_for_steering_network_flows">ntuple filtering for steering network flows</h4>
<p>一些网卡支持 “ntuple filtering” 特性。该特性允许用户（通过 ethtool ）指定一些参数来在硬件上过滤收到的包，然后将其直接放到特定的 RX queue。例如，用户可以指定到特定目端口的 TCP 包放到 RX queue 1。</p>
<p>Intel 的网卡上这个特性叫 Intel Ethernet Flow Director，其他厂商可能也有他们的名字，这些都是出于市场宣传原因，底层原理是类似的。</p>
<p>我们后面会看到，ntuple filtering 是一个叫 Accelerated Receive Flow Steering(aRFS) 功能的核心部分之一，后者使得 ntuple filtering 的使用更加方便。</p>
<p>这个特性适用的场景：最大化数据本地性（data locality），以增加 CPU 处理网络数据时的缓存命中率。例如，考虑运行在 80 口的 web 服务器：</p>
<ol>
<li>webserver 进程运行在 80 口，并绑定到 CPU 2</li>
<li>和某个 RX queue 关联的硬中断绑定到 CPU 2</li>
<li>目的端口是 80 的 TCP 流量通过 ntuple filtering 绑定到 CPU 2</li>
<li>接下来所有到 80 口的流量，从数据包进来到数据到达用户程序的整个过程，都由 CPU 2 处理</li>
<li>仔细监控系统的缓存命中率、网络栈的延迟等信息，以验证以上配置是否生效</li>
</ol>
<p>检查 ntuple filtering 特性是否打开：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -k eth0</div><div class="line">Offload parameters <span class="keyword">for</span> eth0:</div><div class="line"><span class="keyword">...</span></div><div class="line">ntuple-filters: off</div><div class="line">receive-hashing: on</div></pre></td></tr></table></figure>

<p>可以看到，上面的 ntuple 是关闭的。</p>
<p>打开：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -K eth0 ntuple <span class="command"><span class="keyword">on</span></span></div></pre></td></tr></table></figure>

<p>打开 ntuple filtering 功能，并确认打开之后，可以用 <code>ethtool -u</code> 查看当前的 ntuple<br>rules：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> ethtool -u eth0</div><div class="line"><span class="number">40</span> RX rings available</div><div class="line">Total <span class="number">0</span> rules</div></pre></td></tr></table></figure>

<p>可以看到当前没有 rules。</p>
<p>我们来加一条：目的端口是 80 的放到 RX queue 2：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -U eth0 flow-<span class="class"><span class="keyword">type</span> <span class="title">tcp4</span> <span class="title">dst</span>-<span class="title">port</span> 80 <span class="title">action</span> 2</span></div></pre></td></tr></table></figure>

<p>你也可以用 ntuple filtering 在硬件层面直接 drop 某些 flow 的包。当特定 IP 过来的流量太大时，这种功能可能会派上用场。更多关于 ntuple 的信息，参考 ethtool man page。</p>
<p><code>ethtool -S &lt;DEVICE&gt;</code> 的输出统计里，Intel 的网卡有 <code>fdir_match</code> 和 <code>fdir_miss</code> 两项，是和 ntuple filtering 相关的。关于具体的、详细的统计计数，需要查看相应网卡的设备驱动和 data sheet。</p>
<h2 id="软中断（SoftIRQ）">软中断（SoftIRQ）</h2>
<p>在查看网络栈之前，让我们先开个小差，看下内核里一个叫 SoftIRQ 的东西。</p>
<h3 id="软中断是什么">软中断是什么</h3>
<p>内核的软中断系统是一种<strong>在硬中断处理上下文（驱动中）之外执行代码</strong>的机制。<strong>硬中断处理函数（handler）执行时，会屏蔽部分或全部（新的）硬中断</strong>。中断被屏蔽的时间越长，丢失事件的可能性也就越大。所以，<strong>所有耗时的操作都应该从硬中断处理逻辑中剥离出来</strong>，硬中断因此能尽可能快地执行，然后再重新打开硬中断。</p>
<p>内核中也有其他机制将耗时操作转移出去，不过对于网络栈，我们接下来只看软中断这种方式。</p>
<p>可以把软中断系统想象成一系列<strong>内核线程</strong>（每个 CPU 一个），这些线程执行针对不同事件注册的处理函数（handler）。如果你执行过 <code>top</code> 命令，可能会注意到<code>ksoftirqd/0</code> 这个内核线程，其表示这个软中断线程跑在 CPU 0 上。</p>
<p>内核子系统（比如网络）能通过 <code>open_softirq</code> 函数注册软中断处理函数。接下来我们会看到网络系统是如何注册它的处理函数的。现在先来学习一下软中断是如何工作的。</p>
<h3 id="ksoftirqd"><code>ksoftirqd</code></h3>
<p>软中断对分担硬中断的工作量非常重要，因此软中断线程在内核启动的很早阶段就 <code>spawn</code> 出来了。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/kernel/softirq.c#L743-L758" target="_blank" rel="external"><code>kernel/softirq.c</code></a>展示了 <code>ksoftirqd</code> 系统是如何初始化的：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">struct</span> smp_hotplug_thread softirq_threads = {</div><div class="line">      .store              = &ksoftirqd,</div><div class="line">      .thread_should_run  = ksoftirqd_should_run,</div><div class="line">      .thread_fn          = run_ksoftirqd,</div><div class="line">      .thread_comm        = <span class="string">"ksoftirqd/%u"</span>,</div><div class="line">};</div><div class="line"></div><div class="line"><span class="keyword">static</span> __init <span class="keyword">int</span> spawn_ksoftirqd(<span class="keyword">void</span>)</div><div class="line">{</div><div class="line">      register_cpu_notifier(&cpu_nfb);</div><div class="line"></div><div class="line">      BUG_ON(smpboot_register_percpu_thread(&softirq_threads));</div><div class="line"></div><div class="line">      <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">}</div><div class="line">early_initcall(spawn_ksoftirqd);</div></pre></td></tr></table></figure>

<p>看到注册了两个回调函数: <code>ksoftirqd_should_run</code> 和 <code>run_ksoftirqd</code>。这两个函数都会从<a href="https://github.com/torvalds/linux/blob/v3.13/kernel/smpboot.c#L94-L163" target="_blank" rel="external"><code>kernel/smpboot.c</code></a>里调用，作为事件处理循环的一部分。</p>
<p><code>kernel/smpboot.c</code> 里面的代码首先调用 <code>ksoftirqd_should_run</code> 判断是否有 pending 的软中断，如果有，就执行 <code>run_ksoftirqd</code>，后者做一些 bookeeping 工作，然后调用<code>__do_softirq</code>。</p>
<h3 id="__do_softirq"><code>__do_softirq</code></h3>
<p><code>__do_softirq</code> 做的几件事情：</p>
<ul>
<li>判断哪个 softirq 被 pending</li>
<li>计算 softirq 时间，用于统计</li>
<li>更新 softirq 执行相关的统计数据</li>
<li>执行 pending softirq 的处理函数</li>
</ul>
<p><strong>查看 CPU 利用率时，<code>si</code> 字段对应的就是 softirq</strong>，度量（从硬中断转移过来的）软中断的 CPU 使用量。</p>
<h3 id="监控">监控</h3>
<p>软中断的信息可以从 <code>/proc/softirqs</code> 读取：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$ cat /<span class="keyword">proc</span>/softirqs</div><div class="line">                    <span class="type">CPU0</span>       <span class="type">CPU1</span>       <span class="type">CPU2</span>       <span class="type">CPU3</span></div><div class="line">          <span class="type">HI</span>:          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span></div><div class="line">       <span class="type">TIMER</span>: <span class="number">2831512516</span> <span class="number">1337085411</span> <span class="number">1103326083</span> <span class="number">1423923272</span></div><div class="line">      <span class="type">NET_TX</span>:   <span class="number">15774435</span>     <span class="number">779806</span>     <span class="number">733217</span>     <span class="number">749512</span></div><div class="line">      <span class="type">NET_RX</span>: <span class="number">1671622615</span> <span class="number">1257853535</span> <span class="number">2088429526</span> <span class="number">2674732223</span></div><div class="line">       <span class="type">BLOCK</span>: <span class="number">1800253852</span>    <span class="number">1466177</span>    <span class="number">1791366</span>     <span class="number">634534</span></div><div class="line"><span class="type">BLOCK_IOPOLL</span>:          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span></div><div class="line">     <span class="type">TASKLET</span>:         <span class="number">25</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span></div><div class="line">       <span class="type">SCHED</span>: <span class="number">2642378225</span> <span class="number">1711756029</span>  <span class="number">629040543</span>  <span class="number">682215771</span></div><div class="line">     <span class="type">HRTIMER</span>:    <span class="number">2547911</span>    <span class="number">2046898</span>    <span class="number">1558136</span>    <span class="number">1521176</span></div><div class="line">         <span class="type">RCU</span>: <span class="number">2056528783</span> <span class="number">4231862865</span> <span class="number">3545088730</span>  <span class="number">844379888</span></div></pre></td></tr></table></figure>

<p>监控这些数据可以得到软中断的执行频率信息。</p>
<p>例如，<code>NET_RX</code> 一行显示的是软中断在 CPU 间的分布。如果分布非常不均匀，那某一列的值就会远大于其他列，这预示着下面要介绍的 Receive Packet Steering / Receive Flow<br>Steering 可能会派上用场。但也要注意：不要太相信这个数值，<code>NET_RX</code> 太高并不一定都是网卡触发的，下面会看到其他地方也有可能触发之。</p>
<p>调整其他网络配置时，可以留意下这个指标的变动。</p>
<p>现在，让我们进入网络栈部分，跟踪一个包是如何被接收的。</p>
<h2 id="Linux_网络设备子系统">Linux 网络设备子系统</h2>
<p>我们已经知道了网络驱动和软中断是如何工作的，接下来看 Linux 网络设备子系统是如何初始化的。然后我们就可以从一个包到达网卡开始跟踪它的整个路径。</p>
<h3 id="网络设备子系统的初始化">网络设备子系统的初始化</h3>
<p>网络设备（netdev）的初始化在 <code>net_dev_init</code>，里面有些东西很有意思。</p>
<h4 id="struct_softnet_data_变量初始化"><code>struct softnet_data</code> 变量初始化</h4>
<p><code>net_dev_init</code> 为每个 CPU 创建一个 <code>struct softnet_data</code> 变量。这些变量包含一些指向重要信息的指针：</p>
<ul>
<li>需要注册到这个 CPU 的 NAPI 变量列表</li>
<li>数据处理 backlog</li>
<li>处理权重</li>
<li>receive offload 变量列表</li>
<li>receive packet steering 设置</li>
</ul>
<p>接下来随着逐步进入网络栈，我们会一一查看这些功能。</p>
<h4 id="SoftIRQ_Handler_初始化">SoftIRQ Handler 初始化</h4>
<p><code>net_dev_init</code> 分别为接收和发送数据注册了一个软中断处理函数。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> __init net_dev_init(<span class="keyword">void</span>)</div><div class="line">{</div><div class="line">  <span class="comment">/* ... */</span></div><div class="line"></div><div class="line">  open_softirq(NET_TX_SOFTIRQ, net_tx_action);</div><div class="line">  open_softirq(NET_RX_SOFTIRQ, net_rx_action);</div><div class="line"></div><div class="line"> <span class="comment">/* ... */</span></div><div class="line">}</div></pre></td></tr></table></figure>

<p>后面会看到驱动的中断处理函数是如何触发 <code>net_rx_action</code> 这个为 <code>NET_RX_SOFTIRQ</code>软中断注册的处理函数的。</p>
<h3 id="数据来了">数据来了</h3>
<p>终于，网络数据来了！</p>
<p>如果 RX 队列有足够的描述符（descriptors），包会<strong>通过 DMA 写到 RAM</strong>。设备然后发起对应于它的中断（或者在 MSI-X 的场景，中断和包达到的 RX 队列绑定）。</p>
<h4 id="中断处理函数">中断处理函数</h4>
<p>一般来说，中断处理函数应该将尽可能多的处理逻辑移出（到软中断），这至关重要，因为发起一个中断后，其他的中断就会被屏蔽。</p>
<p>我们来看一下 MSI-X 中断处理函数的代码，它展示了中断处理函数是如何尽量简单的。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2038-L2059" target="_blank" rel="external">igb_main.c</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> irqreturn_t igb_msix_ring(<span class="keyword">int</span> irq, <span class="keyword">void</span> *data)</div><div class="line">{</div><div class="line">  <span class="keyword">struct</span> igb_q_vector *q_vector = data;</div><div class="line"></div><div class="line">  <span class="comment">/* Write the ITR value calculated from the previous interrupt. */</span></div><div class="line">  igb_write_itr(q_vector);</div><div class="line"></div><div class="line">  napi_schedule(&q_vector-&gt;napi);</div><div class="line"></div><div class="line">  <span class="keyword">return</span> IRQ_HANDLED;</div><div class="line">}</div></pre></td></tr></table></figure>

<p>这个中断处理函数非常简短，只做了 2 个很快的操作就返回了。</p>
<p>首先，它调用 <code>igb_write_itr</code> 更新一个硬件寄存器。对这个例子，这个寄存器是记录硬件中断频率的。</p>
<p>这个寄存器和一个叫 <strong>&quot;Interrupt Throttling&quot;（也叫 &quot;Interrupt Coalescing&quot;）的硬件特性</strong>相关，这个特性可以平滑传送到 CPU 的中断数量。我们接下来会看到，ethtool 是怎么样提供了一个机制用于<strong>调整 IRQ 触发频率</strong>的。</p>
<p>第二，触发 <code>napi_schedule</code>，如果 NAPI 的处理循环还没开始的话，这会唤醒它。注意，这个处理循环是在软中断中执行的，而不是硬中断。</p>
<p>这段代码展示了硬中断尽量简短为何如此重要；为我们接下来理解多核 CPU 的接收逻辑很有帮助。</p>
<h4 id="NAPI_和_napi_schedule">NAPI 和 <code>napi_schedule</code></h4>
<p>接下来看从硬件中断中调用的 <code>napi_schedule</code> 是如何工作的。</p>
<p>注意，NAPI 存在的意义是<strong>无需硬件中断通知就可以接收网络数据</strong>。前面提到，NAPI 的轮询循环（poll loop）是受硬件中断触发而跑起来的。换句话说，NAPI 功能启用了，但是默认是没有工作的，直到第一个包到达的时候，网卡触发的一个硬件将它唤醒。后面会看到，也还有其他的情况，NAPI 功能也会被关闭，直到下一个硬中断再次将它唤起。</p>
<p><code>napi_schedule</code> 只是一个简单的封装，内层调用 <code>__napi_schedule</code>。<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> * __napi_schedule - schedule for receive</div><div class="line"> * @n: entry to schedule</div><div class="line"> *</div><div class="line"> * The entry's receive function will be scheduled to run</div><div class="line"> */</div><div class="line"><span class="keyword">void</span> __napi_schedule(<span class="keyword">struct</span> napi_struct *n)</div><div class="line">{</div><div class="line">  <span class="keyword">unsigned</span> <span class="keyword">long</span> flags;</div><div class="line"></div><div class="line">  local_irq_save(flags);</div><div class="line">  ____napi_schedule(&__get_cpu_var(softnet_data), n);</div><div class="line">  local_irq_restore(flags);</div><div class="line">}</div><div class="line">EXPORT_SYMBOL(__napi_schedule);</div></pre></td></tr></table></figure>

<p><code>__get_cpu_var</code> 用于获取属于这个 CPU 的 <code>structure softnet_data</code> 变量。</p>
<p><code>____napi_schedule</code>, <a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4154-L4168" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* Called with irq disabled */</span></div><div class="line"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">void</span> ____napi_schedule(<span class="keyword">struct</span> softnet_data *sd,</div><div class="line">                                     <span class="keyword">struct</span> napi_struct *napi)</div><div class="line">{</div><div class="line">  list_add_tail(&napi-&gt;poll_list, &sd-&gt;poll_list);</div><div class="line">  __raise_softirq_irqoff(NET_RX_SOFTIRQ);</div><div class="line">}</div></pre></td></tr></table></figure>

<p>这段代码了做了两个重要的事情：</p>
<ol>
<li>将（从驱动的中断函数中传来的）<code>napi_struct</code> 变量，添加到 poll list，后者 attach 到这个 CPU 上的 <code>softnet_data</code></li>
<li><code>__raise_softirq_irqoff</code> 触发一个 <code>NET_RX_SOFTIRQ</code> 类型软中断。这会触发执行<code>net_rx_action</code>（如果没有正在执行），后者是网络设备初始化的时候注册的</li>
</ol>
<p>接下来会看到，软中断处理函数 <code>net_rx_action</code> 会调用 NAPI 的 poll 函数来收包。</p>
<h4 id="关于_CPU_和网络数据处理的一点笔记">关于 CPU 和网络数据处理的一点笔记</h4>
<p>注意到目前为止，我们从硬中断处理函数中转移到软中断处理函数的逻辑，都是使用的本CPU 变量。</p>
<p>驱动的硬中断处理函数做的事情很少，但软中断将会在和硬中断相同的 CPU 上执行。<strong>这就是为什么给每个 CPU 一个特定的硬中断非常重要：这个 CPU 不仅处理这个硬中断，而且通过 NAPI 处理接下来的软中断来收包</strong>。</p>
<p>后面我们会看到，Receive Packet Steering 可以将软中断分给其他 CPU。</p>
<h4 id="监控网络数据到达">监控网络数据到达</h4>
<h5 id="硬中断请求">硬中断请求</h5>
<p>注意：监控硬件中断拿不到关于网络包处理的健康状况的全景图，一些驱动在 NAPI 运行的时候会关闭硬中断。这只是你整个监控方案的一个重要部分。</p>
<p>读取硬中断统计：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$ cat /<span class="keyword">proc</span>/interrupts</div><div class="line">            <span class="type">CPU0</span>       <span class="type">CPU1</span>       <span class="type">CPU2</span>       <span class="type">CPU3</span></div><div class="line">   <span class="number">0</span>:         <span class="number">46</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">IO</span>-<span class="type">APIC</span>-edge      timer</div><div class="line">   <span class="number">1</span>:          <span class="number">3</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">IO</span>-<span class="type">APIC</span>-edge      i8042</div><div class="line">  <span class="number">30</span>: <span class="number">3361234770</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">IO</span>-<span class="type">APIC</span>-fasteoi   aacraid</div><div class="line">  <span class="number">64</span>:          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">DMAR_MSI</span>-edge      dmar0</div><div class="line">  <span class="number">65</span>:          <span class="number">1</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">PCI</span>-<span class="type">MSI</span>-edge      eth0</div><div class="line">  <span class="number">66</span>:  <span class="number">863649703</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">PCI</span>-<span class="type">MSI</span>-edge      eth0-<span class="type">TxRx</span>-<span class="number">0</span></div><div class="line">  <span class="number">67</span>:  <span class="number">986285573</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">PCI</span>-<span class="type">MSI</span>-edge      eth0-<span class="type">TxRx</span>-<span class="number">1</span></div><div class="line">  <span class="number">68</span>:         <span class="number">45</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">PCI</span>-<span class="type">MSI</span>-edge      eth0-<span class="type">TxRx</span>-<span class="number">2</span></div><div class="line">  <span class="number">69</span>:        <span class="number">394</span>          <span class="number">0</span>          <span class="number">0</span>          <span class="number">0</span> <span class="type">IR</span>-<span class="type">PCI</span>-<span class="type">MSI</span>-edge      eth0-<span class="type">TxRx</span>-<span class="number">3</span></div><div class="line"> <span class="type">NMI</span>:    <span class="number">9729927</span>    <span class="number">4008190</span>    <span class="number">3068645</span>    <span class="number">3375402</span>  <span class="type">Non</span>-maskable interrupts</div><div class="line"> <span class="type">LOC</span>: <span class="number">2913290785</span> <span class="number">1585321306</span> <span class="number">1495872829</span> <span class="number">1803524526</span>  <span class="type">Local</span> timer interrupts</div></pre></td></tr></table></figure>

<p>可以看到有多少包进来、硬件中断频率，RX 队列被哪个 CPU 处理等信息。这里只能看到硬中断数量，不能看出实际多少数据被接收或处理，因为一些驱动在 NAPI 收包时会关闭硬中断。进一步，使用 Interrupt Coalescing 时也会影响这个统计。监控这个指标能帮你判断出你设置的 Interrupt Coalescing 是不是在工作。</p>
<p>为了使监控更加完整，需要同时监控 <code>/proc/softirqs</code> (前面提到)和 <code>/proc</code>。</p>
<h4 id="数据接收调优">数据接收调优</h4>
<h5 id="中断合并（Interrupt_coalescing）">中断合并（Interrupt coalescing）</h5>
<p>中断合并会将多个中断事件放到一起，到达一定的阈值之后才向 CPU 发起中断请求。</p>
<p>这可以防止中断风暴，提升吞吐。减少中断数量能使吞吐更高，但延迟也变大，CPU 使用量下降；中断数量过多则相反。</p>
<p>历史上，早期的 igb、e1000 版本，以及其他的都包含一个叫 InterruptThrottleRate 参数，最近的版本已经被 ethtool 可配置的参数取代。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -c eth0</div><div class="line">Coalesce parameters <span class="keyword">for</span> eth0:</div><div class="line">Adaptive RX: off  TX: off</div><div class="line">stats-block-usecs: <span class="number">0</span></div><div class="line">sample-interval: <span class="number">0</span></div><div class="line">pkt-rate-low: <span class="number">0</span></div><div class="line">pkt-rate-high: <span class="number">0</span></div><div class="line"><span class="keyword">...</span></div></pre></td></tr></table></figure>

<p>ethtool 提供了用于中断合并相关的通用的接口。但切记，不是所有的设备都支持完整的配置。你需要查看你的驱动文档或代码来确定哪些支持，哪些不支持。ethtool 的文档说的：“驱动没有实现的接口将会被静默忽略”。</p>
<p>某些驱动支持一个有趣的特性“自适应 RX/TX 硬中断合并”。这个特性一般是在硬件实现的。驱动通常需要做一些额外的工作来告诉网卡需要打开这个特性（前面的 igb 驱动代码里有涉及）。</p>
<p>自适应 RX/TX 硬中断合并带来的效果是：带宽比较低时降低延迟，带宽比较高时提升吞吐。</p>
<p>用 <code>ethtool -C</code> 打开自适应 RX IRQ 合并：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -C eth0 adaptive-rx <span class="command"><span class="keyword">on</span></span></div></pre></td></tr></table></figure>

<p>还可以用 <code>ethtool -C</code> 更改其他配置。常用的包括：</p>
<ul>
<li><code>rx-usecs</code>: How many usecs to delay an RX interrupt after a packet arrives.</li>
<li><code>rx-frames</code>: Maximum number of data frames to receive before an RX interrupt.</li>
<li><code>rx-usecs-irq</code>: How many usecs to delay an RX interrupt while an interrupt is being serviced by the host.</li>
<li><code>rx-frames-irq</code>: Maximum number of data frames to receive before an RX interrupt is generated while the system is servicing an interrupt.</li>
</ul>
<p>请注意你的硬件可能只支持以上列表的一个子集，具体请参考相应的驱动说明或源码。</p>
<p>不幸的是，通常并没有一个很好的文档来说明这些选项，最全的文档很可能是头文件。查看<a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/ethtool.h#L184-L255" target="_blank" rel="external">include/uapi/linux/ethtool.h</a> ethtool 每个每个选项的解释。</p>
<p>注意：虽然硬中断合并看起来是个不错的优化项，但要你的网络栈的其他一些相应部分也要针对性的调整。只合并硬中断很可能并不会带来多少收益。</p>
<h5 id="调整硬中断亲和性（IRQ_affinities）">调整硬中断亲和性（IRQ affinities）</h5>
<p>If your NIC supports RSS / multiqueue or if you are attempting to optimize for data locality, you may wish to use a specific set of CPUs for handling interrupts generated by your NIC.</p>
<p>Setting specific CPUs allows you to segment which CPUs will be used for processing which IRQs. These changes may affect how upper layers operate, as we’ve seen for the networking stack.</p>
<p>If you do decide to adjust your IRQ affinities, you should first check if you running the irqbalance daemon. This daemon tries to automatically balance IRQs to CPUs and it may overwrite your settings. If you are running irqbalance, you should either disable irqbalance or use the --banirq in conjunction with IRQBALANCE_BANNED_CPUS to let irqbalance know that it shouldn’t touch a set of IRQs and CPUs that you want to assign yourself.</p>
<p>Next, you should check the file /proc/interrupts for a list of the IRQ numbers for each network RX queue for your NIC.</p>
<p>Finally, you can adjust the which CPUs each of those IRQs will be handled by modifying /proc/irq/IRQ_NUMBER/smp_affinity for each IRQ number.</p>
<p>You simply write a hexadecimal bitmask to this file to instruct the kernel which CPUs it should use for handling the IRQ.</p>
<p>Example: Set the IRQ affinity for IRQ 8 to CPU 0</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo bash -c 'echo <span class="number">1</span> &gt; /<span class="keyword">proc</span>/irq/<span class="number">8</span>/smp_affinity'</div></pre></td></tr></table></figure>

<h3 id="网络数据处理：开始">网络数据处理：开始</h3>
<p>一旦软中断代码判断出有 softirq 处于 pending 状态，就会开始处理，执行<code>net_rx_action</code>，网络数据处理就此开始。</p>
<p>我们来看一下 <code>net_rx_action</code> 的循环部分，理解它是如何工作的。哪个部分可以调优，哪个可以被监控。</p>
<h4 id="net_rx_action_处理循环"><code>net_rx_action</code> 处理循环</h4>
<p><strong><code>net_rx_action</code> 从包所在的内存开始处理，包是被设备通过 DMA 直接送到内存的。</strong><br>函数遍历本 CPU 队列的 NAPI 变量列表，依次出队并操作之。处理逻辑考虑任务量（work）和执行时间两个因素：</p>
<ol>
<li>跟踪记录工作量预算（work budget），预算可以调整</li>
<li>记录消耗的时间</li>
</ol>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4380-L4383" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> (!list_empty(&sd-&gt;poll_list)) {</div><div class="line">    <span class="keyword">struct</span> napi_struct *n;</div><div class="line">    <span class="keyword">int</span> work, weight;</div><div class="line"></div><div class="line">    <span class="comment">/* If softirq window is exhausted then punt.</span></div><div class="line">     * Allow this to run for 2 jiffies since which will allow</div><div class="line">     * an average latency of 1.5/HZ.</div><div class="line">     */</div><div class="line">    <span class="keyword">if</span> (unlikely(budget &lt;= <span class="number">0</span> || time_after_eq(jiffies, time_limit)))</div><div class="line">      <span class="keyword">goto</span> softnet_break;</div></pre></td></tr></table></figure>

<p>这里可以看到内核是如何防止处理数据包过程霸占整个 CPU 的，其中 budget 是该 CPU 的所有 NAPI 变量的总预算。这也是多队列网卡应该精心调整 IRQ Affinity 的原因。回忆前面讲的，处理硬中断的 CPU接下来会处理相应的软中断，进而执行上面包含 budget 的这段逻辑。</p>
<p>多网卡多队列可能会出现这样的情况：多个 NAPI 变量注册到同一个 CPU 上。每个 CPU 上的所有 NAPI 变量共享一份 budget。</p>
<p>如果没有足够的 CPU 来分散网卡硬中断，可以考虑增加 <code>net_rx_action</code> 允许每个 CPU处理更多包。增加 budget 可以增加 CPU 使用量（<code>top</code> 等命令看到的 <code>sitime</code> 或 <code>si</code>部分），但可以减少延迟，因为数据处理更加及时。</p>
<p>Note: the CPU will still be bounded by a time limit of 2 jiffies, regardless of the assigned budget.</p>
<h4 id="NAPI_poll_函数及权重">NAPI poll 函数及权重</h4>
<p>回忆前面，网络设备驱动使用 <code>netif_napi_add</code> 注册 poll 方法，<code>igb</code> 驱动有如下代码片段：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* initialize NAPI */</span></div><div class="line"> netif_napi_add(adapter-&gt;netdev, &q_vector-&gt;napi, igb_poll, <span class="number">64</span>);</div></pre></td></tr></table></figure>

<p>这注册了一个 NAPI 变量，hardcode 64 的权重。我们来看在 <code>net_rx_action</code> 处理循环中这个值是如何使用的。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4322-L4338" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">weight = n-&gt;weight;</div><div class="line"></div><div class="line">work = <span class="number">0</span>;</div><div class="line"><span class="keyword">if</span> (test_bit(NAPI_STATE_SCHED, &n-&gt;state)) {</div><div class="line">        work = n-&gt;poll(n, weight);</div><div class="line">        trace_napi_poll(n);</div><div class="line">}</div><div class="line"></div><div class="line">WARN_ON_ONCE(work &gt; weight);</div><div class="line"></div><div class="line">budget -= work;</div></pre></td></tr></table></figure>

<p>其中的 <code>n</code> 是 <code>struct napi</code> 的变量。其中的 <code>poll</code> 指向 <code>igb_poll</code>。<code>poll()</code> 返回处理的数据帧数量，budget 会减去这个值。</p>
<p>所以，假设驱动使用 weight 值 64（Linux 3.13.0 的所有驱动都是 hardcode 这个值），设置 budget 默认值 300，那系统将在如下条件之一停止数据处理：</p>
<ol>
<li><code>igb_poll</code> 函数被调用了最多 5 次（如果没有数据需要处理，那次数就会很少）</li>
<li>时间经过了至少 2 个 jiffies</li>
</ol>
<h4 id="NAPI_和设备驱动的合约（contract）">NAPI 和设备驱动的合约（contract）</h4>
<p>NAPI 子系统和设备驱动之间的合约，最重要的一点是关闭 NAPI 的条件。具体如下：</p>
<ol>
<li>如果驱动的 <code>poll</code> 方法用完了它的全部 weight（默认 hardcode 64），那它<strong>不要更改</strong> NAPI 状态。接下来 <code>net_rx_action</code> loop 会做的</li>
<li>如果驱动的 <code>poll</code> 方法没有用完全部 weight，那它<strong>必须关闭</strong> NAPI。下次有硬件中断触发，驱动的硬件处理函数调用 <code>napi_schedule</code> 时，NAPI 会被重新打开</li>
</ol>
<p>接下来先看 <code>net_rx_action</code> 如何处理合约的第一部分，然后看 <code>poll</code> 方法如何处理第二部分。</p>
<h4 id="Finishing_the_net_rx_action_loop">Finishing the <code>net_rx_action</code> loop</h4>
<p><code>net_rx_action</code> 循环的最后一部分代码处理前面提到的 <strong>NAPI 合约的第一部分</strong>。<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L4342-L4363" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* Drivers must not modify the NAPI state if they</span></div><div class="line"> * consume the entire weight.  In such cases this code</div><div class="line"> * still "owns" the NAPI instance and therefore can</div><div class="line"> * move the instance around on the list at-will.</div><div class="line"> */</div><div class="line"><span class="keyword">if</span> (unlikely(work == weight)) {</div><div class="line">  <span class="keyword">if</span> (unlikely(napi_disable_pending(n))) {</div><div class="line">    local_irq_enable();</div><div class="line">    napi_complete(n);</div><div class="line">    local_irq_disable();</div><div class="line">  } <span class="keyword">else</span> {</div><div class="line">    <span class="keyword">if</span> (n-&gt;gro_list) {</div><div class="line">      <span class="comment">/* flush too old packets</span></div><div class="line">       * If HZ &lt; 1000, flush all packets.</div><div class="line">       */</div><div class="line">      local_irq_enable();</div><div class="line">      napi_gro_flush(n, HZ &gt;= <span class="number">1000</span>);</div><div class="line">      local_irq_disable();</div><div class="line">    }</div><div class="line">    list_move_tail(&n-&gt;poll_list, &sd-&gt;poll_list);</div><div class="line">  }</div><div class="line">}</div></pre></td></tr></table></figure>

<p>如果全部 <code>work</code> 都用完了，<code>net_rx_action</code> 会面临两种情况：</p>
<ol>
<li>网络设备需要关闭（例如，用户敲了 <code>ifconfig eth0 down</code> 命令）</li>
<li>如果设备不需要关闭，那检查是否有 GRO（后面会介绍）列表。如果时钟 tick rate<code>&gt;= 1000</code>，所有最近被更新的 GRO network flow 都会被 flush。将这个 NAPI 变量移到 list 末尾，这个循环下次再进入时，处理的就是下一个 NAPI 变量</li>
</ol>
<p>这就是包处理循环如何唤醒驱动注册的 <code>poll</code> 方法进行包处理的过程。接下来会看到，<code>poll</code> 方法会收割网络数据，发送到上层栈进行处理。</p>
<h4 id="到达_limit_时退出循环">到达 limit 时退出循环</h4>
<p><code>net_rx_action</code> 下列条件之一退出循环：</p>
<ol>
<li>这个 CPU 上注册的 poll 列表已经没有 NAPI 变量需要处理(<code>!list_empty(&amp;sd-&gt;poll_list)</code>)</li>
<li>剩余的 <code>budget &lt;= 0</code></li>
<li>已经满足 2 个 jiffies 的时间限制</li>
</ol>
<p>代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* If softirq window is exhausted then punt.</span></div><div class="line"> * Allow this to run for 2 jiffies since which will allow</div><div class="line"> * an average latency of 1.5/HZ.</div><div class="line"> */</div><div class="line"><span class="keyword">if</span> (unlikely(budget &lt;= <span class="number">0</span> || time_after_eq(jiffies, time_limit)))</div><div class="line">  <span class="keyword">goto</span> softnet_break;</div></pre></td></tr></table></figure>

<p>如果跟踪 <code>softnet_break</code>，会发现很有意思的东西：</p>
<p>From net/core/dev.c:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">softnet_break:</div><div class="line">  sd-&gt;time_squeeze++;</div><div class="line">  __raise_softirq_irqoff(NET_RX_SOFTIRQ);</div><div class="line">  <span class="keyword">goto</span> out;</div></pre></td></tr></table></figure>

<p><code>softnet_data</code> 变量更新统计信息，软中断的 <code>NET_RX_SOFTIRQ</code> 被关闭。</p>
<p><code>time_squeeze</code> 字段记录的是满足如下条件的次数：<code>net_rx_action</code> 有很多 <code>work</code> 要做但是 budget 用完了，或者 work 还没做完但时间限制到了。这对理解网络处理的瓶颈至关重要。我们后面会看到如何监控这个值。关闭 <code>NET_RX_SOFTIRQ</code> 是为了释放 CPU 时间给其他任务用。这行代码是有意义的，因为只有我们有更多工作要做（还没做完）的时候才会执行到这里，我们主动让出 CPU，不想独占太久。</p>
<p>然后执行到了 <code>out</code> 标签所在的代码。另外还有一种条件也会跳转到 <code>out</code> 标签：所有NAPI 变量都处理完了，换言之，budget 数量大于网络包数量，所有驱动都已经关闭 NAPI，没有什么事情需要 <code>net_rx_action</code> 做了。</p>
<p><code>out</code> 代码段在从 <code>net_rx_action</code> 返回之前做了一件重要的事情：调用<code>net_rps_action_and_irq_enable</code>。Receive Packet Steering 功能打开时这个函数有重要作用：唤醒其他 CPU 处理网络包。</p>
<p>我们后面会看到 RPS 是如何工作的。现在先看看怎样监控 <code>net_rx_action</code> 处理循环的健康状态，以及进入 NAPI <code>poll</code> 的内部，这样才能更好的理解网络栈。</p>
<h4 id="NAPI_poll">NAPI <code>poll</code></h4>
<p>回忆前文，驱动程序会分配一段内存用于 DMA，将数据包写到内存。就像这段内存是由驱动程序分配的一样，驱动程序也负责解绑（unmap）这些内存，读取数据，将数据送到网络栈。</p>
<p>我们看下 <code>igb</code> 驱动如何实现这一过程的。</p>
<h5 id="igb_poll"><code>igb_poll</code></h5>
<p>可以看到 <code>igb_poll</code> 代码其实相当简单。<br><a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018" target="_blank" rel="external">drivers/net/ethernet/intel/igb/igb_main.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line"> *  igb_poll - NAPI Rx polling callback</div><div class="line"> *  @napi: napi polling structure</div><div class="line"> *  @budget: count of how many packets we should handle</div><div class="line"> **/</div><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> igb_poll(<span class="keyword">struct</span> napi_struct *napi, <span class="keyword">int</span> budget)</div><div class="line">{</div><div class="line">        <span class="keyword">struct</span> igb_q_vector *q_vector = container_of(napi,</div><div class="line">                                                     <span class="keyword">struct</span> igb_q_vector,</div><div class="line">                                                     napi);</div><div class="line">        <span class="keyword">bool</span> clean_complete = <span class="keyword">true</span>;</div><div class="line"></div><div class="line"><span class="preprocessor">#ifdef CONFIG_IGB_DCA</span></div><div class="line">        <span class="keyword">if</span> (q_vector-&gt;adapter-&gt;flags & IGB_FLAG_DCA_ENABLED)</div><div class="line">                igb_update_dca(q_vector);</div><div class="line"><span class="preprocessor">#<span class="keyword">endif</span></span></div><div class="line"></div><div class="line">        <span class="comment">/* ... */</span></div><div class="line"></div><div class="line">        <span class="keyword">if</span> (q_vector-&gt;rx.ring)</div><div class="line">                clean_complete &= igb_clean_rx_irq(q_vector, budget);</div><div class="line"></div><div class="line">        <span class="comment">/* If all work not completed, return budget and keep polling */</span></div><div class="line">        <span class="keyword">if</span> (!clean_complete)</div><div class="line">                <span class="keyword">return</span> budget;</div><div class="line"></div><div class="line">        <span class="comment">/* If not enough Rx work done, exit the polling mode */</span></div><div class="line">        napi_complete(napi);</div><div class="line">        igb_ring_irq_enable(q_vector);</div><div class="line"></div><div class="line">        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">}</div></pre></td></tr></table></figure>

<p>几件有意思的事情：</p>
<ul>
<li>如果内核 <a href="https://lwn.net/Articles/247493/" target="_blank" rel="external">DCA</a>（Direct Cache Access）功能打开了，CPU 缓存是热的，对 RX ring 的访问会命中 CPU cache。更多 DCA 信息见本文 “Extra” 部分</li>
<li>然后执行 <code>igb_clean_rx_irq</code>，这里做的事情非常多，我们后面看</li>
<li>然后执行 <code>clean_complete</code>，判断是否仍然有 work 可以做。如果有，就返回 budget（回忆，这里是 hardcode 64）。在之前我们已经看到，<code>net_rx_action</code> 会将这个 NAPI变量移动到 poll 列表的末尾</li>
<li>如果所有 <code>work</code> 都已经完成，驱动通过调用 <code>napi_complete</code> 关闭 NAPI，并通过调用<code>igb_ring_irq_enable</code> 重新进入可中断状态。下次中断到来的时候回重新打开 NAPI</li>
</ul>
<p>我们来看 <code>igb_clean_rx_irq</code> 如何将网络数据送到网络栈。</p>
<h5 id="igb_clean_rx_irq"><code>igb_clean_rx_irq</code></h5>
<p><code>igb_clean_rx_irq</code> 方法是一个循环，每次处理一个包，直到 budget 用完，或者没有数据需要处理了。</p>
<p>做的几件重要事情：</p>
<ol>
<li>分配额外的 buffer 用于接收数据，因为已经用过的 buffer 被 clean out 了。一次分配 <code>IGB_RX_BUFFER_WRITE (16)</code>个。</li>
<li>从 RX 队列取一个 buffer，保存到一个 <code>skb</code> 类型的变量中</li>
<li>判断这个 buffer 是不是一个包的最后一个 buffer。如果是，继续处理；如果不是，继续从 buffer 列表中拿出下一个 buffer，加到 skb。当数据帧的大小比一个 buffer 大的时候，会出现这种情况</li>
<li>验证数据的 layout 和头信息是正确的</li>
<li>更新 <code>skb-&gt;len</code>，表示这个包已经处理的字节数</li>
<li>设置 <code>skb</code> 的 hash, checksum, timestamp, VLAN id, protocol 字段。hash，checksum，timestamp，VLAN ID 信息是硬件提供的，如果硬件报告 checksum error，<code>csum_error</code> 统计就会增加。如果 checksum 通过了，数据是 UDP 或者 TCP 数据，<code>skb</code> 就会被标记成 <code>CHECKSUM_UNNECESSARY</code></li>
<li>构建的 skb 经 <code>napi_gro_receive()</code>进入协议栈</li>
<li>更新处理过的包的统计信息</li>
<li>循环直至处理的包数量达到 budget</li>
</ol>
<p>循环结束的时候，这个函数设置收包的数量和字节数统计信息。</p>
<p>接下来在进入协议栈之前，我们先开两个小差：首先是看一些如何监控和调优软中断，其次是介绍 GRO。有了这个两个背景，后面（通过 <code>napi_gro_receive</code> 进入）协议栈部分会更容易理解。</p>
<h4 id="监控网络数据处理">监控网络数据处理</h4>
<h5 id="/proc/net/softnet_stat"><code>/proc/net/softnet_stat</code></h5>
<p>前面看到，如果 budget 或者 time limit 到了而仍有包需要处理，那 <code>net_rx_action</code> 在退出循环之前会更新统计信息。这个信息存储在该 CPU 的 <code>struct softnet_data</code> 变量中。</p>
<p>这些统计信息打到了<code>/proc/net/softnet_stat</code>，但不幸的是，关于这个的文档很少。每一列代表什么并没有标题，而且列的内容会随着内核版本可能发生变化。</p>
<p>在内核 3.13.0 中，你可以阅读内核源码，查看每一列分别对应什么。<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/net-procfs.c#L161-L165" target="_blank" rel="external">net/core/net-procfs.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">seq_printf(seq,</div><div class="line">       <span class="string">"%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n"</span>,</div><div class="line">       sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, <span class="number">0</span>,</div><div class="line">       <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="comment">/* was fastroute */</span></div><div class="line">       sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);</div></pre></td></tr></table></figure>

<p>其中一些的名字让人很困惑，而且在你意想不到的地方更新。在接下来的网络栈分析说，我们会举例说明其中一些字段是何时、在哪里被更新的。前面我们已经看到了 <code>squeeze_time</code>是在 <code>net_rx_action</code> 在被更新的，到此时，如下数据你应该能看懂了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ cat /<span class="keyword">proc</span>/net/softnet_stat</div><div class="line"><span class="number">6</span>dcad223 <span class="number">00000000</span> <span class="number">00000001</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span></div><div class="line"><span class="number">6</span>f0e1565 <span class="number">00000000</span> <span class="number">00000002</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span></div><div class="line"><span class="number">660774</span>ec <span class="number">00000000</span> <span class="number">00000003</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span></div><div class="line"><span class="number">61</span>c99331 <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span></div><div class="line"><span class="number">6794</span>b1b3 <span class="number">00000000</span> <span class="number">00000005</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span></div><div class="line"><span class="number">6488</span>cb92 <span class="number">00000000</span> <span class="number">00000001</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span> <span class="number">00000000</span></div></pre></td></tr></table></figure>

<p>关于<code>/proc/net/softnet_stat</code> 的重要细节:</p>
<ol>
<li>每一行代表一个 <code>struct softnet_data</code> 变量。因为每个 CPU 只有一个该变量，所以每行<br>其实代表一个 CPU</li>
<li>每列用空格隔开，数值用 16 进制表示</li>
<li>第一列 <code>sd-&gt;processed</code>，是处理的网络帧的数量。如果你使用了 ethernet bonding，<br>那这个值会大于总的网络帧的数量，因为 ethernet bonding 驱动有时会触发网络数据被<br>重新处理（re-processed）</li>
<li>第二列，<code>sd-&gt;dropped</code>，是因为处理不过来而 drop 的网络帧数量。后面会展开这一话题</li>
<li>第三列，<code>sd-&gt;time_squeeze</code>，前面介绍过了，由于 budget 或 time limit 用完而退出<code>net_rx_action</code> 循环的次数</li>
<li>接下来的 5 列全是 0</li>
<li>第九列，<code>sd-&gt;cpu_collision</code>，是为了发送包而获取锁的时候有冲突的次数</li>
<li>第十列，<code>sd-&gt;received_rps</code>，是这个 CPU 被其他 CPU 唤醒去收包的次数</li>
<li>最后一列，<code>flow_limit_count</code>，是达到 flow limit 的次数。flow limit 是 RPS 的特性，后面会稍微介绍一下</li>
</ol>
<p>如果你要画图监控这些数据，确保你的列和相应的字段是对的上的，最保险的方式是阅读相应版本的内核代码。</p>
<h4 id="网络数据处理调优">网络数据处理调优</h4>
<h5 id="调整_net_rx_action_budget">调整 <code>net_rx_action</code> budget</h5>
<p><code>net_rx_action</code> budget 表示一个 CPU 单次轮询（<code>poll</code>）所允许的最大收包数量。单次poll 收包是，所有注册到这个 CPU 的 NAPI 变量收包数量之和不能大于这个阈值。 调整：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.netdev_budget=<span class="number">600</span></div></pre></td></tr></table></figure>

<p>如果要保证重启仍然生效，需要将这个配置写到<code>/etc/sysctl.conf</code>。</p>
<p>Linux 3.13.0 的默认配置是 300。</p>
<h3 id="GRO（Generic_Receive_Offloading）">GRO（Generic Receive Offloading）</h3>
<p><strong>Large Receive Offloading (LRO) 是一个硬件优化，GRO 是 LRO 的一种软件实现。</strong></p>
<p>两种方案的主要思想都是：<strong>通过合并“足够类似”的包来减少传送给网络栈的包数，这有助于减少 CPU 的使用量</strong>。例如，考虑大文件传输的场景，包的数量非常多，大部分包都是一段文件数据。相比于每次都将小包送到网络栈，可以将收到的小包合并成一个很大的包再送到网络栈。这可以使得协议层只需要处理一个 header，而将包含大量数据的整个大包送到用户程序。</p>
<p>这类优化方式的缺点就是：信息丢失。如果一个包有一些重要的 option 或者 flag，那将这个包的数据合并到其他包时，这些信息就会丢失。这也是为什么大部分人不使用或不推荐使用LRO 的原因。</p>
<p>LRO 的实现，一般来说，对合并包的规则非常宽松。GRO 是 LRO 的软件实现，但是对于包合并的规则更严苛。</p>
<p>顺便说一下，<strong>如果你曾经用过 tcpdump 抓包，并收到看起来不现实的非常大的包，那很可能是你的系统开启了 GRO</strong>。接下来会看到，<strong>tcpdump 的抓包点（捕获包的 tap）在整个栈的更后面一些，在GRO 之后</strong>。</p>
<h4 id="使用_ethtool_修改_GRO_配置">使用 ethtool 修改 GRO 配置</h4>
<p><code>-k</code> 查看 GRO 配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ ethtool -k eth0 | grep <span class="keyword">generic</span>-receive-offload</div><div class="line"><span class="keyword">generic</span>-receive-offload: <span class="keyword">on</span></div></pre></td></tr></table></figure>

<p><code>-K</code> 修改 GRO 配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -K eth0 gro <span class="command"><span class="keyword">on</span></span></div></pre></td></tr></table></figure>

<p>注意：对于大部分驱动，修改 GRO 配置会涉及先 down 再 up 这个网卡，因此这个网卡上的连接<br>都会中断。</p>
<h3 id="napi_gro_receive"><code>napi_gro_receive</code></h3>
<p>如果开启了 GRO，<code>napi_gro_receive</code> 将负责处理网络数据，并将数据送到协议栈，大部分相关的逻辑在函数 <code>dev_gro_receive</code> 里实现。</p>
<h4 id="dev_gro_receive"><code>dev_gro_receive</code></h4>
<p>这个函数首先检查 GRO 是否开启了，如果是，就准备做 GRO。GRO 首先遍历一个 offloadfilter 列表，如果高层协议认为其中一些数据属于 GRO 处理的范围，就会允许其对数据进行操作。</p>
<p>协议层以此方式让网络设备层知道，这个 packet 是不是当前正在处理的一个需要做 GRO 的network flow 的一部分，而且也可以通过这种方式传递一些协议相关的信息。例如，TCP 协议需要判断是否以及合适应该将一个 ACK 包合并到其他包里。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3844-L3856" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">list_for_each_entry_rcu(ptype, head, <span class="built_in">list</span>) {</div><div class="line">  <span class="keyword">if</span> (ptype-&gt;type != type || !ptype-&gt;callbacks.gro_receive)</div><div class="line">    <span class="keyword">continue</span>;</div><div class="line"></div><div class="line">  skb_set_network_header(skb, skb_gro_offset(skb));</div><div class="line">  skb_reset_mac_len(skb);</div><div class="line">  NAPI_GRO_CB(skb)-&gt;same_flow = <span class="number">0</span>;</div><div class="line">  NAPI_GRO_CB(skb)-&gt;flush = <span class="number">0</span>;</div><div class="line">  NAPI_GRO_CB(skb)-&gt;<span class="built_in">free</span> = <span class="number">0</span>;</div><div class="line"></div><div class="line">  pp = ptype-&gt;callbacks.gro_receive(&napi-&gt;gro_list, skb);</div><div class="line">  <span class="keyword">break</span>;</div><div class="line">}</div></pre></td></tr></table></figure>

<p>如果协议层提示是时候 flush GRO packet 了，那就到下一步处理了。这发生在<code>napi_gro_complete</code>，会进一步调用相应协议的 <code>gro_complete</code> 回调方法，然后调用<code>netif_receive_skb</code> 将包送到协议栈。这个过程见<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3862-L3872" target="_blank" rel="external">net/core/dev.c</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (pp) {</div><div class="line">  <span class="keyword">struct</span> sk_buff *nskb = *pp;</div><div class="line"></div><div class="line">  *pp = nskb-&gt;next;</div><div class="line">  nskb-&gt;next = NULL;</div><div class="line">  napi_gro_complete(nskb);</div><div class="line">  napi-&gt;gro_count--;</div><div class="line">}</div></pre></td></tr></table></figure>

<p>接下来，如果协议层将这个包合并到一个已经存在的 flow，<code>napi_gro_receive</code> 就没什么事情需要做，因此就返回了。如果 packet 没有被合并，而且 GRO 的数量小于 <code>MAX_GRO_SKBS</code>（<br>默认是 8），就会创建一个新的 entry 加到本 CPU 的 NAPI 变量的 <code>gro_list</code>。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3877-L3886" target="_blank" rel="external">net/core/dev.c</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (NAPI_GRO_CB(skb)-&gt;flush || napi-&gt;gro_count &gt;= MAX_GRO_SKBS)</div><div class="line">  <span class="keyword">goto</span> normal;</div><div class="line"></div><div class="line">napi-&gt;gro_count++;</div><div class="line">NAPI_GRO_CB(skb)-&gt;count = <span class="number">1</span>;</div><div class="line">NAPI_GRO_CB(skb)-&gt;age = jiffies;</div><div class="line">skb_shinfo(skb)-&gt;gso_size = skb_gro_len(skb);</div><div class="line">skb-&gt;next = napi-&gt;gro_list;</div><div class="line">napi-&gt;gro_list = skb;</div><div class="line">ret = GRO_HELD;</div></pre></td></tr></table></figure>

<p><strong>这就是 Linux 网络栈中 GRO 的工作原理。</strong></p>
<h3 id="napi_skb_finish"><code>napi_skb_finish</code></h3>
<p>一旦 <code>dev_gro_receive</code> 完成，<code>napi_skb_finish</code> 就会被调用，其如果一个 packet 被合并了，就释放不用的变量；或者调用 <code>netif_receive_skb</code> 将数据发送到网络协议栈（因为已经<br>有 <code>MAX_GRO_SKBS</code> 个 flow 了，够 GRO 了）。</p>
<p>接下来，是看 <code>netif_receive_skb</code> 如何将数据交给协议层。但在此之前，我们先看一下 RPS。</p>
<h2 id="RPS_(Receive_Packet_Steering)">RPS (Receive Packet Steering)</h2>
<p>回忆前面我们讨论了网络设备驱动是如何注册 NAPI <code>poll</code> 方法的。每个 NAPI 变量都会运行在相应 CPU 的软中断的上下文中。而且，触发硬中断的这个 CPU 接下来会负责执行相应的软中断处理函数来收包。</p>
<p>换言之，同一个 CPU 既处理硬中断，又处理相应的软中断。</p>
<p>一些网卡（例如 Intel I350）在硬件层支持多队列。这意味着收进来的包会被通过 DMA 放到位于不同内存的队列上，而不同的队列有相应的 NAPI 变量管理软中断 <code>poll()</code>过程。因此，多个 CPU 同时处理从网卡来的中断，处理收包过程。</p>
<p>这个特性被称作 RSS（Receive Side Scaling，接收端扩展）。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222" target="_blank" rel="external">RPS</a>（Receive Packet Steering，接收包控制，接收包引导）是 RSS 的一种软件实现。因为是软件实现的，意味着任何网卡都可以使用这个功能，即便是那些只有一个接收队列的网卡。但是，因为它是软件实现的，这意味着 RPS 只能在 packet 通过 DMA 进入内存后，RPS 才能开始工作。</p>
<p>这意味着，RPS 并不会减少 CPU 处理硬件中断和 NAPI <code>poll</code>（软中断最重要的一部分）的时间，但是可以在 packet 到达内存后，将 packet 分到其他 CPU，从其他 CPU 进入协议栈。</p>
<p>RPS 的工作原理是对个 packet 做 hash，以此决定分到哪个 CPU 处理。然后 packet 放到每个 CPU独占的接收后备队列（backlog）等待处理。这个 CPU 会触发一个进程间中断（<a href="https://en.wikipedia.org/wiki/Inter-processor_interrupt" target="_blank" rel="external">IPI</a>，Inter-processor<br>Interrupt）向对端 CPU。如果当时对端 CPU 没有在处理 backlog 队列收包，这个进程间中断会触发它开始从 backlog 收包。<code>/proc/net/softnet_stat</code> 其中有一列是记录 <code>softnet_data</code><br>变量（也即这个 CPU）收到了多少 IPI（<code>received_rps</code> 列）。</p>
<p>因此，<code>netif_receive_skb</code> 或者继续将包送到协议栈，或者交给 RPS，后者会转交给其他 CPU 处理。</p>
<h3 id="RPS_调优">RPS 调优</h3>
<p>使用 RPS 需要在内核做配置（Ubuntu + Kernel 3.13.0 支持），而且需要一个掩码（bitmask）指定哪些 CPU 可以处理那些 RX 队列。相关的一些信息可以在<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L138-L164" target="_blank" rel="external">内核文档</a>里找到。</p>
<p>bitmask 配置位于：<code>/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus</code>。</p>
<p>例如，对于 eth0 的 queue 0，你需要更改<code>/sys/class/net/eth0/queues/rx-0/rps_cpus</code>。<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L160-L164" target="_blank" rel="external">内核文档</a>里说，对一些特定的配置下，RPS 没必要了。</p>
<p>注意：打开 RPS 之后，原来不需要处理软中断（softirq）的 CPU 这时也会参与处理。因此相应 CPU 的 <code>NET_RX</code> 数量，以及 <code>si</code> 或 <code>sitime</code> 占比都会相应增加。你可以对比启用 RPS 前后的<br>数据，以此来确定你的配置是否生效，以及是否符合预期（哪个 CPU 处理哪个网卡的哪个中断）。</p>
<h2 id="RFS_(Receive_Flow_Steering)">RFS (Receive Flow Steering)</h2>
<p>RFS（Receive flow steering）和 RPS 配合使用。RPS 试图在 CPU 之间平衡收包，但是没考虑数据的本地性问题，如何最大化 CPU 缓存的命中率。RFS 将属于相同 flow 的包送到相同的 CPU进行处理，可以提高缓存命中率。</p>
<h3 id="调优：打开_RFS">调优：打开 RFS</h3>
<p>RPS 记录一个全局的 hash table，包含所有 flow 的信息。这个 hash table 的大小可以在 <code>net.core.rps_sock_flow_entries</code>：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.rps_sock_flow_entries=<span class="number">32768</span></div></pre></td></tr></table></figure>

<p>其次，你可以设置每个 RX queue 的 flow 数量，对应着 <code>rps_flow_cnt</code>：</p>
<p>例如，eth0 的 RX queue0 的 flow 数量调整到 2048：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="input"><span class="prompt">$ sudo bash -c 'echo 2048 &gt;</span> /sys/<span class="class"><span class="keyword">class</span>/<span class="title">net</span>/<span class="title">eth0</span>/<span class="title">queues</span>/<span class="title">rx</span>-0/<span class="title">rps_flow_cnt</span>'</span></span></div></pre></td></tr></table></figure>

<h2 id="aRFS_(Hardware_accelerated_RFS)">aRFS (Hardware accelerated RFS)</h2>
<p>RFS 可以用硬件加速，网卡和内核协同工作，判断哪个 flow 应该在哪个 CPU 上处理。这需要网卡和网卡驱动的支持。</p>
<p>如果你的网卡驱动里对外提供一个 <code>ndo_rx_flow_steer</code> 函数，那就是支持 RFS。</p>
<h3 id="调优:_启用_aRFS">调优: 启用 aRFS</h3>
<p>假如你的网卡支持 aRFS，你可以开启它并做如下配置：</p>
<ul>
<li>打开并配置 RPS</li>
<li>打开并配置 RFS</li>
<li>内核中编译期间指定了 <code>CONFIG_RFS_ACCEL</code> 选项。Ubuntu kernel 3.13.0 是有的</li>
<li>打开网卡的 ntuple 支持。可以用 ethtool 查看当前的 ntuple 设置</li>
<li>配置 IRQ（硬中断）中每个 RX 和 CPU 的对应关系</li>
</ul>
<p>以上配置完成后，aRFS 就会自动将 RX queue 数据移动到指定 CPU 的内存，每个 flow 的包都会到达同一个 CPU，不需要你再通过 ntuple 手动指定每个 flow 的配置了。</p>
<h2 id="从_netif_receive_skb_进入协议栈">从 <code>netif_receive_skb</code> 进入协议栈</h2>
<p>重新捡起我们前面已经几次提到过的 <code>netif_receive_skb</code>，这个函数在好几个地方被调用。两个最重要的地方（前面都看到过了）：</p>
<ul>
<li><code>napi_skb_finish</code>：当 packet 不需要被合并到已经存在的某个 GRO flow 的时候</li>
<li><code>napi_gro_complete</code>：协议层提示需要 flush 当前的 flow 的时候</li>
</ul>
<p>提示：<code>netif_receive_skb</code> 和它调用的函数都运行在软中断处理循环（softirq processing loop）的上下文中，因此这里的时间会记录到 <code>top</code> 命令看到的 <code>si</code> 或者<code>sitime</code> 字段。</p>
<p><code>netif_receive_skb</code> 首先会检查用户有没有设置一个接收时间戳选项（sysctl），这个选项决定在 packet 在到达 backlog queue 之前还是之后打时间戳。如果启用，那立即打时间戳，在 RPS 之前（CPU 和 backlog queue 绑定）；如果没有启用，那只有在它进入到 backlog queue 之后才会打时间戳。如果 RPS 开启了，那这个选项可以将打时间戳的任务分散个其他CPU，但会带来一些延迟。</p>
<h3 id="调优:_收包打时间戳（RX_packet_timestamping）">调优: 收包打时间戳（RX packet timestamping）</h3>
<p>你可以调整包被收到后，何时给它打时间戳。</p>
<p>关闭收包打时间戳：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.netdev_tstamp_prequeue=<span class="number">0</span></div></pre></td></tr></table></figure>

<p>默认是 1。</p>
<h2 id="netif_receive_skb"><code>netif_receive_skb</code></h2>
<p>处理完时间戳后，<code>netif_receive_skb</code> 会根据 RPS 是否启用来做不同的事情。我们先来看简单情况，RPS 未启用。</p>
<h3 id="不使用_RPS（默认）">不使用 RPS（默认）</h3>
<p>如果 RPS 没启用，会调用<code>__netif_receive_skb</code>，它做一些 bookkeeping 工作，进而调用<code>__netif_receive_skb_core</code>，将数据移动到离协议栈更近一步。</p>
<p><code>__netif_receive_skb_core</code> 工作的具体细节我们稍后再看，先看一下 RPS 启用的情况下的代码调用关系，它也会调到这个函数的。</p>
<h3 id="使用_RPS">使用 RPS</h3>
<p>如果 RPS 启用了，它会做一些计算，判断使用哪个 CPU 的 backlog queue，这个过程由<code>get_rps_cpu</code> 函数完成。 <a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">cpu = get_rps_cpu(skb-&gt;dev, skb, &rflow);</div><div class="line"></div><div class="line"><span class="keyword">if</span> (cpu &gt;= <span class="number">0</span>) {</div><div class="line">  ret = enqueue_to_backlog(skb, cpu, &rflow-&gt;last_qtail);</div><div class="line">  rcu_read_unlock();</div><div class="line">  <span class="keyword">return</span> ret;</div><div class="line">}</div></pre></td></tr></table></figure>

<p><code>get_rps_cpu</code> 会考虑前面提到的 RFS 和 aRFS 设置，以此选出一个合适的 CPU，通过调用<code>enqueue_to_backlog</code> 将数据放到它的 backlog queue。</p>
<h3 id="enqueue_to_backlog"><code>enqueue_to_backlog</code></h3>
<p>首先从远端 CPU 的 <code>struct softnet_data</code> 变量获取 backlog queue 长度。如果 backlog 大于<code>netdev_max_backlog</code>，或者超过了 flow limit，直接 drop，并更新 <code>softnet_data</code> 的 drop统计。注意这是远端 CPU 的统计。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">    qlen = skb_queue_len(&sd-&gt;input_pkt_queue);</div><div class="line">    <span class="keyword">if</span> (qlen &lt;= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {</div><div class="line">        <span class="keyword">if</span> (skb_queue_len(&sd-&gt;input_pkt_queue)) {</div><div class="line">enqueue:</div><div class="line">            __skb_queue_tail(&sd-&gt;input_pkt_queue, skb);</div><div class="line">            input_queue_tail_incr_save(sd, qtail);</div><div class="line">            <span class="keyword">return</span> NET_RX_SUCCESS;</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="comment">/* Schedule NAPI for backlog device */</span></div><div class="line">        <span class="keyword">if</span> (!__test_and_set_bit(NAPI_STATE_SCHED, &sd-&gt;backlog.state)) {</div><div class="line">            <span class="keyword">if</span> (!rps_ipi_queued(sd))</div><div class="line">                ____napi_schedule(sd, &sd-&gt;backlog);</div><div class="line">        }</div><div class="line">        <span class="keyword">goto</span> enqueue;</div><div class="line">    }</div><div class="line">    sd-&gt;dropped++;</div><div class="line"></div><div class="line">    kfree_skb(skb);</div><div class="line">    <span class="keyword">return</span> NET_RX_DROP;</div></pre></td></tr></table></figure>

<p><code>enqueue_to_backlog</code> 被调用的地方很少。在基于 RPS 处理包的地方，以及 <code>netif_rx</code>，会调用到它。大部分驱动都不应该使用 <code>netif_rx</code>，而应该是用 <code>netif_receive_skb</code>。如果你没用到 RPS，你的驱动也没有使用 <code>netif_rx</code>，那增大 <code>backlog</code> 并不会带来益处，因为它根本没被用到。</p>
<p>注意：检查你的驱动，如果它调用了 <code>netif_receive_skb</code>，而且你没用 RPS，那增大<code>netdev_max_backlog</code> 并不会带来任何性能提升，因为没有数据包会被送到<code>input_pkt_queue</code>。</p>
<p>如果 <code>input_pkt_queue</code> 足够小，而 flow limit（后面会介绍）也还没达到（或者被禁掉了），那数据包将会被放到队列。这里的逻辑有点 funny，但大致可以归为为：</p>
<ul>
<li>如果 backlog 是空的：如果远端 CPU NAPI 变量没有运行，并且 IPI 没有被加到队列，那就<br>触发一个 IPI 加到队列，然后调用<code>____napi_schedule</code> 进一步处理</li>
<li>如果 backlog 非空，或者远端 CPU NAPI 变量正在运行，那就 enqueue 包</li>
</ul>
<p>这里使用了 <code>goto</code>，所以代码看起来有点 tricky。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3201-L3218" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">  <span class="keyword">if</span> (skb_queue_len(&sd-&gt;input_pkt_queue)) {</div><div class="line">enqueue:</div><div class="line">         __skb_queue_tail(&sd-&gt;input_pkt_queue, skb);</div><div class="line">         input_queue_tail_incr_save(sd, qtail);</div><div class="line">         rps_unlock(sd);</div><div class="line">         local_irq_restore(flags);</div><div class="line">         <span class="keyword">return</span> NET_RX_SUCCESS;</div><div class="line"> }</div><div class="line"></div><div class="line"> <span class="comment">/* Schedule NAPI for backlog device</span></div><div class="line">  * We can use non atomic operation since we own the queue lock</div><div class="line">  */</div><div class="line"> <span class="keyword">if</span> (!__test_and_set_bit(NAPI_STATE_SCHED, &sd-&gt;backlog.state)) {</div><div class="line">         <span class="keyword">if</span> (!rps_ipi_queued(sd))</div><div class="line">                 ____napi_schedule(sd, &sd-&gt;backlog);</div><div class="line"> }</div><div class="line"> <span class="keyword">goto</span> enqueue;</div></pre></td></tr></table></figure>

<h4 id="Flow_limits">Flow limits</h4>
<p>RPS 在不同 CPU 之间分发 packet，但是，如果一个 flow 特别大，会出现单个 CPU 被打爆，而其他 CPU 无事可做（饥饿）的状态。因此引入了 flow limit 特性，放到一个 backlog 队列的属<br>于同一个 flow 的包的数量不能超过一个阈值。这可以保证即使有一个很大的 flow 在大量收包，小 flow 也能得到及时的处理。</p>
<p>检查 flow limit 的代码，<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200" target="_blank" rel="external">net/core/dev.c</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (qlen &lt;= netdev_max_backlog && !skb_flow_limit(skb, qlen)) {</div></pre></td></tr></table></figure>

<p>默认，flow limit 功能是关掉的。要打开 flow limit，你需要指定一个 bitmap（类似于 RPS的 bitmap）。</p>
<h4 id="监控：由于_input_pkt_queue_打满或_flow_limit_导致的丢包">监控：由于 <code>input_pkt_queue</code> 打满或 flow limit 导致的丢包</h4>
<p>在<code>/proc/net/softnet_stat</code> 里面的 dropped 列计数，包含本节提到的原因导致的 drop。</p>
<h4 id="调优">调优</h4>
<h5 id="Tuning:_Adjusting_netdev_max_backlog_to_prevent_drops">Tuning: Adjusting netdev_max_backlog to prevent drops</h5>
<p>在调整这个值之前，请先阅读前面的“注意”。</p>
<p>如果你使用了 RPS，或者你的驱动调用了 <code>netif_rx</code>，那增加 <code>netdev_max_backlog</code> 可以改善在 <code>enqueue_to_backlog</code> 里的丢包：</p>
<p>例如：increase backlog to 3000 with sysctl.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.netdev_max_backlog=<span class="number">3000</span></div></pre></td></tr></table></figure>

<p>默认值是 1000。</p>
<h5 id="Tuning:_Adjust_the_NAPI_weight_of_the_backlog_poll_loop">Tuning: Adjust the NAPI weight of the backlog poll loop</h5>
<p><code>net.core.dev_weight</code> 决定了 backlog poll loop 可以消耗的整体 budget（参考前面更改<code>net.core.netdev_budget</code> 的章节）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.dev_weight=<span class="number">600</span></div></pre></td></tr></table></figure>

<p>默认值是 64。</p>
<p>记住，backlog 处理逻辑和设备驱动的 <code>poll</code> 函数类似，都是在软中断（softirq）的上下文中执行，因此受整体 budget 和处理时间的限制，前面已经分析过了。</p>
<h5 id="Tuning:_Enabling_flow_limits_and_tuning_flow_limit_hash_table_size">Tuning: Enabling flow limits and tuning flow limit hash table size</h5>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.flow_limit_table_len=<span class="number">8192</span></div></pre></td></tr></table></figure>

<p>默认值是 4096.</p>
<p>这只会影响新分配的 flow hash table。所以，如果你想增加 table size 的话，应该在打开flow limit 功能之前设置这个值。</p>
<p>打开 flow limit 功能的方式是，在<code>/proc/sys/net/core/flow_limit_cpu_bitmap</code> 中指定一个 bitmask，和通过 bitmask 打开 RPS 的操作类似。</p>
<h3 id="处理_backlog_队列：NAPI_poller">处理 backlog 队列：NAPI poller</h3>
<p>每个 CPU 都有一个 backlog queue，其加入到 NAPI 变量的方式和驱动差不多，都是注册一个<code>poll</code> 方法，在软中断的上下文中处理包。此外，还提供了一个 <code>weight</code>，这也和驱动类似。</p>
<p>注册发生在网络系统初始化的时候。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L6952-L6955" target="_blank" rel="external">net/core/dev.c</a>的 <code>net_dev_init</code> 函数：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sd<span class="variable">-&gt;backlog</span>.poll = process_backlog;</div><div class="line">sd<span class="variable">-&gt;backlog</span>.weight = weight_p;</div><div class="line">sd<span class="variable">-&gt;backlog</span>.gro_list = <span class="keyword">NULL</span>;</div><div class="line">sd<span class="variable">-&gt;backlog</span>.gro_count = <span class="number">0</span>;</div></pre></td></tr></table></figure>

<p>backlog NAPI 变量和设备驱动 NAPI 变量的不同之处在于，它的 weight 是可以调节的，而设备驱动是 hardcode 64。在下面的调优部分，我们会看到如何用 sysctl 调整这个设置。</p>
<h3 id="process_backlog"><code>process_backlog</code></h3>
<p><code>process_backlog</code> 是一个循环，它会一直运行直至 <code>weight</code>（前面介绍了）用完，或者backlog 里没有数据了。</p>
<p>backlog queue 里的数据取出来，传递给<code>__netif_receive_skb</code>。这个函数做的事情和 RPS关闭的情况下做的事情一样。即，<code>__netif_receive_skb</code> 做一些 bookkeeping 工作，然后调用<code>__netif_receive_skb_core</code> 将数据发送给更上面的协议层。</p>
<p><code>process_backlog</code> 和 NAPI 之间遵循的合约，和驱动和 NAPI 之间的合约相同：NAPI isdisabled if the total weight will not be used. The poller is restarted with the<br>call to <code>____napi_schedule</code> from <code>enqueue_to_backlog</code> as described above.</p>
<p>函数返回接收完成的数据帧数量（在代码中是变量 <code>work</code>），<code>net_rx_action</code>（前面介绍了）将会从 budget（通过 <code>net.core.netdev_budget</code> 可以调整，前面介绍了）里减去这个值。</p>
<h3 id="__netif_receive_skb_core：将数据送到抓包点（tap）或协议层"><code>__netif_receive_skb_core</code>：将数据送到抓包点（tap）或协议层</h3>
<p><code>__netif_receive_skb_core</code> 完成<strong>将数据送到协议栈</strong>这一繁重工作（the heavy lifting of delivering the data)。在此之前，它会先<strong>检查是否插入了 packet tap（探测点），这些 tap 是抓包用的</strong>。例如，<code>AF_PACKET</code> 地址族就可以插入这些抓包指令，一般通过 <code>libpcap</code> 库。</p>
<p><strong>如果存在抓包点（tap），数据就会先到抓包点，然后才到协议层。</strong></p>
<h3 id="送到抓包点（tap）">送到抓包点（tap）</h3>
<p>如果有 packet tap（通常通过 <code>libpcap</code>），packet 会送到那里。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">list_for_each_entry_rcu(ptype, &ptype_all, <span class="built_in">list</span>) {</div><div class="line">  <span class="keyword">if</span> (!ptype-&gt;dev || ptype-&gt;dev == skb-&gt;dev) {</div><div class="line">    <span class="keyword">if</span> (pt_prev)</div><div class="line">      ret = deliver_skb(skb, pt_prev, orig_dev);</div><div class="line">    pt_prev = ptype;</div><div class="line">  }</div><div class="line">}</div></pre></td></tr></table></figure>

<p>如果对 packet 如何经过 pcap 有兴趣，可以阅读<a href="https://github.com/torvalds/linux/blob/v3.13/net/packet/af_packet.c" target="_blank" rel="external">net/packet/af_packet.c</a>。</p>
<h3 id="送到协议层">送到协议层</h3>
<p>处理完 taps 之后，<code>__netif_receive_skb_core</code> 将数据发送到协议层。它会从数据包中取出协议信息，然后遍历注册在这个协议上的回调函数列表。</p>
<p>可以看<code>__netif_receive_skb_core</code> 函数，<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554" target="_blank" rel="external">net/core/dev.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">type = skb-&gt;protocol;</div><div class="line">list_for_each_entry_rcu(ptype,</div><div class="line">                &ptype_base[ntohs(type) & PTYPE_HASH_MASK], <span class="built_in">list</span>) {</div><div class="line">        <span class="keyword">if</span> (ptype-&gt;type == type &&</div><div class="line">            (ptype-&gt;dev == null_or_dev || ptype-&gt;dev == skb-&gt;dev ||</div><div class="line">             ptype-&gt;dev == orig_dev)) {</div><div class="line">                <span class="keyword">if</span> (pt_prev)</div><div class="line">                        ret = deliver_skb(skb, pt_prev, orig_dev);</div><div class="line">                pt_prev = ptype;</div><div class="line">        }</div><div class="line">}</div></pre></td></tr></table></figure>

<p>上面的 <code>ptype_base</code> 是一个 hash table，定义在<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L146" target="_blank" rel="external">net/core/dev.c</a>中:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">struct</span> list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;</div></pre></td></tr></table></figure>

<p>每种协议在上面的 hash table 的一个 slot 里，添加一个过滤器到列表里。这个列表的头用如下函数获取：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">struct</span> list_head *ptype_head(<span class="keyword">const</span> <span class="keyword">struct</span> packet_type *pt)</div><div class="line">{</div><div class="line">        <span class="keyword">if</span> (pt-&gt;type == htons(ETH_P_ALL))</div><div class="line">                <span class="keyword">return</span> &ptype_all;</div><div class="line">        <span class="keyword">else</span></div><div class="line">                <span class="keyword">return</span> &ptype_base[ntohs(pt-&gt;type) & PTYPE_HASH_MASK];</div><div class="line">}</div></pre></td></tr></table></figure>

<p>添加的时候用 <code>dev_add_pack</code> 这个函数。这就是协议层如何注册自身，用于处理相应协议的网络数据的。</p>
<p>现在，你已经知道了数据是如何从卡进入到协议层的了。</p>
<h2 id="协议层注册">协议层注册</h2>
<p>接下来我们看协议层注册自身的实现。</p>
<p>本文会拿 IP 层作为例子，因为它最常用，大部分读者都很熟悉。</p>
<h3 id="IP_协议层">IP 协议层</h3>
<p>IP 层在函数 <code>inet_init</code> 中将自身注册到 <code>ptype_base</code> 哈希表。<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1788" target="_blank" rel="external">net/ipv4/af_inet.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dev_add_pack(&ip_packet_type);</div></pre></td></tr></table></figure>

<p><code>struct packet_type</code> 的变量 <code>ip_packet_type</code> 定义在<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1673-L1676" target="_blank" rel="external">net/ipv4/af_inet.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">struct</span> packet_type ip_packet_type __read_mostly = {</div><div class="line">        .type = cpu_to_be16(ETH_P_IP),</div><div class="line">        .func = ip_rcv,</div><div class="line">};</div></pre></td></tr></table></figure>

<p><code>__netif_receive_skb_core</code> 会调用 <code>deliver_skb</code> (前面介绍过了), 后者会调用<code>.func</code><br>方法(这个例子中就是 <code>ip_rcv</code>)。</p>
<h4 id="ip_rcv"><code>ip_rcv</code></h4>
<p><code>ip_rcv</code> 方法的核心逻辑非常简单直接，此外就是一些数据合法性验证，统计计数器更新等等。它在最后会以 netfilter 的方式调用 <code>ip_rcv_finish</code> 方法。这样做的目的是，任何iptables 规则都能在 packet 刚进入 IP 层协议的时候被应用，在其他处理之前。</p>
<p>我们可以在 <code>ip_rcv</code> 结束的时候看到交给 netfilter 的代码：<a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L453-L454" target="_blank" rel="external">net/ipv4/ip_input.c</a></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">return</span> NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING, skb, dev, NULL, ip_rcv_finish);</div></pre></td></tr></table></figure>

<h5 id="netfilter_and_iptables">netfilter and iptables</h5>
<p>这里简单介绍下 <code>netfilter</code>, <code>iptables</code> 和 <code>conntrack</code>。</p>
<p><code>NF_HOOK_THRESH</code> 会检查是否有 filter 被安装，并会适时地返回到 IP 协议层，避免过深的进入 netfilter 处理，以及在 netfilter 下面再做 hook 的 iptables 和 conntrack。</p>
<p>注意：<strong>如果你有很多或者很复杂的 netfilter 或 iptables 规则，这些规则都是在软中断的上下文中执行的，会导致网络延迟。</strong>但如果你就是需要一些规则的话，那这个性能损失看起来是无法避免的。</p>
<h4 id="ip_rcv_finish"><code>ip_rcv_finish</code></h4>
<p>netfilter 完成对数据的处理之后，就会调用 <code>ip_rcv_finish</code>。当然，前提是 netfilter 没有决定丢掉这个包。</p>
<p><code>ip_rcv_finish</code> 开始的地方做了一次优化。为了能将包送到合适的目的地，需要一个路由子系统的 <code>dst_entry</code> 变量。为了获取这个变量，早期的代码调用了 <code>early_demux</code> 函数，从这个数据的目的端的高层协议中。</p>
<p><code>early_demux</code> 是一个优化项，试图路由这个包所需要的 <code>dst_entry</code> 变量，通过检查相应的变量是否缓存在 <code>socket</code> 变量上。<a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L317-L327" target="_blank" rel="external">net/ipv4/ip_input.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (sysctl_ip_early_demux && !skb_dst(skb) && skb-&gt;sk == NULL) {</div><div class="line">  <span class="keyword">const</span> <span class="keyword">struct</span> net_protocol *ipprot;</div><div class="line">  <span class="keyword">int</span> protocol = iph-&gt;protocol;</div><div class="line"></div><div class="line">  ipprot = rcu_dereference(inet_protos[protocol]);</div><div class="line">  <span class="keyword">if</span> (ipprot && ipprot-&gt;early_demux) {</div><div class="line">    ipprot-&gt;early_demux(skb);</div><div class="line">    <span class="comment">/* must reload iph, skb-&gt;head might have changed */</span></div><div class="line">    iph = ip_hdr(skb);</div><div class="line">  }</div><div class="line">}</div></pre></td></tr></table></figure>

<p>可以看到，这个函数只有在 <code>sysctl_ip_early_demux</code> 为 <code>true</code> 的时候才有可能被执行。默认 <code>early_demux</code> 是打开的。下一节会介绍如何关闭它，以及为什么你可能会需要关闭它。</p>
<p>如果这个优化打开了，但是并没有命中缓存（例如，这是第一个包），这个包就会被送到内核的路由子系统，在那里将会计算出一个 <code>dst_entry</code> 并赋给相应的字段。</p>
<p>路由子系统完成工作后，会更新计数器，然后调用 <code>dst_input(skb)</code>，后者会进一步调用<code>dst_entry</code> 变量中的 <code>input</code> 方法，这个方法是一个函数指针，有路由子系统初始化。例如，如果 packet 的最终目的地是本机（local system），路由子系统会将 <code>ip_local_deliver</code> 赋给 <code>input</code>。</p>
<h5 id="调优:_打开或关闭_IP_协议的_early_demux_选项">调优: 打开或关闭 IP 协议的 early demux 选项</h5>
<p>关闭 <code>early_demux</code> 优化：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.ipv4.ip_early_demux=<span class="number">0</span></div></pre></td></tr></table></figure>

<p>默认是 1，即该功能默认是打开的。</p>
<p>添加这个 <code>sysctl</code> 开关的原因是，一些用户报告说，在某些场景下 <code>early_demux</code> 优化会导致 ~5% 左右的吞吐量下降。</p>
<h4 id="ip_local_deliver"><code>ip_local_deliver</code></h4>
<p>回忆我们看到的 IP 协议层过程：</p>
<ol>
<li>调用 <code>ip_rcv</code> 做一些初始的 bookkeeping</li>
<li>将包交给 netfilter 处理，同时还有一个回调函数，netfilter 处理完毕后会调用这个函数</li>
<li>处理结束的时候，调用 <code>ip_rcv_finish</code>，将数据包送到协议栈的更上层</li>
</ol>
<p><code>ip_local_deliver</code> 的逻辑与此类似：<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/ip_input.c#L241-L258" target="_blank" rel="external">net/ipv4/ip_input.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"> *      Deliver IP Packets to the higher protocol layers.</div><div class="line"> */</div><div class="line"><span class="keyword">int</span> ip_local_deliver(<span class="keyword">struct</span> sk_buff *skb)</div><div class="line">{</div><div class="line">        <span class="comment">/*</span></div><div class="line">         *      Reassemble IP fragments.</div><div class="line">         */</div><div class="line"></div><div class="line">        <span class="keyword">if</span> (ip_is_fragment(ip_hdr(skb))) {</div><div class="line">                <span class="keyword">if</span> (ip_defrag(skb, IP_DEFRAG_LOCAL_DELIVER))</div><div class="line">                        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">        }</div><div class="line"></div><div class="line">        <span class="keyword">return</span> NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN, skb, skb-&gt;dev, NULL,</div><div class="line">                       ip_local_deliver_finish);</div><div class="line">}</div></pre></td></tr></table></figure>

<p>只要 packet 没有在 netfilter 被 drop，就会调用 <code>ip_local_deliver_finish</code> 函数。</p>
<h4 id="ip_local_deliver_finish"><code>ip_local_deliver_finish</code></h4>
<p><code>ip_local_deliver_finish</code> 从数据包中读取协议，寻找注册在这个协议上的 <code>struct net_protocol</code> 变量，并调用该变量中的回调方法。这样将包送到协议栈的更上层。</p>
<h5 id="Monitoring:_IP_protocol_layer_statistics">Monitoring: IP protocol layer statistics</h5>
<p>读取<code>/proc/net/snmp</code> 获取详细的 IP 协议统计：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat /<span class="keyword">proc</span>/net/snmp</div><div class="line"><span class="type">Ip</span>: <span class="type">Forwarding</span> <span class="type">DefaultTTL</span> <span class="type">InReceives</span> <span class="type">InHdrErrors</span> <span class="type">InAddrErrors</span> <span class="type">ForwDatagrams</span> <span class="type">InUnknownProtos</span> <span class="type">InDiscards</span> <span class="type">InDelivers</span> <span class="type">OutRequests</span> <span class="type">OutDiscards</span> <span class="type">OutNoRoutes</span> <span class="type">ReasmTimeout</span> <span class="type">ReasmReqds</span> <span class="type">ReasmOKs</span> <span class="type">ReasmFails</span> <span class="type">FragOKs</span> <span class="type">FragFails</span> <span class="type">FragCreates</span></div><div class="line"><span class="type">Ip</span>: <span class="number">1</span> <span class="number">64</span> <span class="number">25922988125</span> <span class="number">0</span> <span class="number">0</span> <span class="number">15771700</span> <span class="number">0</span> <span class="number">0</span> <span class="number">25898327616</span> <span class="number">22789396404</span> <span class="number">12987882</span> <span class="number">51</span> <span class="number">1</span> <span class="number">10129840</span> <span class="number">2196520</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></div><div class="line">...</div></pre></td></tr></table></figure>

<p>这个文件包含几个协议层的统计信息。先是 IP 层。</p>
<p>与这些列相关的，IP 层的统计类型都定义在<a href="https://github.com/torvalds/linux/blob/v3.13/include/uapi/linux/snmp.h#L10-L59" target="_blank" rel="external">include/uapi/linux/snmp.h</a>：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">enum</span></div><div class="line">{</div><div class="line">  IPSTATS_MIB_NUM = <span class="number">0</span>,</div><div class="line"><span class="comment">/* frequently written fields in fast path, kept in same cache line */</span></div><div class="line">  IPSTATS_MIB_INPKTS,     <span class="comment">/* InReceives */</span></div><div class="line">  IPSTATS_MIB_INOCTETS,     <span class="comment">/* InOctets */</span></div><div class="line">  IPSTATS_MIB_INDELIVERS,     <span class="comment">/* InDelivers */</span></div><div class="line">  IPSTATS_MIB_OUTFORWDATAGRAMS,   <span class="comment">/* OutForwDatagrams */</span></div><div class="line">  IPSTATS_MIB_OUTPKTS,      <span class="comment">/* OutRequests */</span></div><div class="line">  IPSTATS_MIB_OUTOCTETS,      <span class="comment">/* OutOctets */</span></div><div class="line"></div><div class="line">  <span class="comment">/* ... */</span></div></pre></td></tr></table></figure>

<p>读取<code>/proc/net/netstat</code> 获取更详细的 IP 层统计：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cat /proc/net/netstat | grep IpExt</div><div class="line">IpExt: InNoRoutes InTruncatedPkts InMcastPkts OutMcastPkts InBcastPkts OutBcastPkts InOctets OutOctets InMcastOctets OutMcastOctets InBcastOctets OutBcastOctets InCsumErrors InNoECTPkts InECT0Pktsu InCEPkts</div><div class="line">IpExt: <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">277959</span> <span class="number">0</span> <span class="number">14568040307695</span> <span class="number">32991309088496</span> <span class="number">0</span> <span class="number">0</span> <span class="number">58649349</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span></div></pre></td></tr></table></figure>

<p>格式和<code>/proc/net/snmp</code> 类似，除了每列的命字都一 <code>IpExt</code> 开头之外。</p>
<p>一些有趣的统计：</p>
<ul>
<li><code>InReceives</code>: The total number of IP packets that reached ip_rcv before any data integrity checks.</li>
<li><code>InHdrErrors</code>: Total number of IP packets with corrupted headers. The header was too short, too long, non-existent, had the wrong IP protocol version number, etc.</li>
<li><code>InAddrErrors</code>: Total number of IP packets where the host was unreachable.</li>
<li><code>ForwDatagrams</code>: Total number of IP packets that have been forwarded.</li>
<li><code>InUnknownProtos</code>: Total number of IP packets with unknown or unsupported protocol specified in the header.</li>
<li><code>InDiscards</code>: Total number of IP packets discarded due to memory allocation failure or checksum failure when packets are trimmed.</li>
<li><code>InDelivers</code>: Total number of IP packets successfully delivered to higher protocol layers. Keep in mind that those protocol layers may drop data even if the IP layer does not.</li>
<li>InCsumErrors: Total number of IP Packets with checksum errors.</li>
</ul>
<p>注意这些计数分别在 IP 层的不同地方被更新。由于代码一直在更新，重复计数或者计数错误的 bug 可能会引入。如果这些计数对你非常重要，强烈建议你阅读内核的相应源码，确定它们是在哪里被更新的，以及更新的对不对，是不是有 bug。</p>
<h3 id="高层协议注册">高层协议注册</h3>
<p>本文介绍 UDP 处理函数的注册过程，TCP 的注册过程与此一样，并且是在相同的时间注册的。</p>
<p>在 <code>net/ipv4/af_inet.c</code> 中定义了 UDP、TCP 和 ICMP 协议的回调函数相关的数据结构，IP 层处理完毕之后会调用相应的回调. From<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1526-L1547" target="_blank" rel="external">net/ipv4/af_inet.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">struct</span> net_protocol tcp_protocol = {</div><div class="line">        .early_demux    =       tcp_v4_early_demux,</div><div class="line">        .handler        =       tcp_v4_rcv,</div><div class="line">        .err_handler    =       tcp_v4_err,</div><div class="line">        .no_policy      =       <span class="number">1</span>,</div><div class="line">        .netns_ok       =       <span class="number">1</span>,</div><div class="line">};</div><div class="line"></div><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">struct</span> net_protocol udp_protocol = {</div><div class="line">        .early_demux =  udp_v4_early_demux,</div><div class="line">        .handler =      udp_rcv,</div><div class="line">        .err_handler =  udp_err,</div><div class="line">        .no_policy =    <span class="number">1</span>,</div><div class="line">        .netns_ok =     <span class="number">1</span>,</div><div class="line">};</div><div class="line"></div><div class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">struct</span> net_protocol icmp_protocol = {</div><div class="line">        .handler =      icmp_rcv,</div><div class="line">        .err_handler =  icmp_err,</div><div class="line">        .no_policy =    <span class="number">1</span>,</div><div class="line">        .netns_ok =     <span class="number">1</span>,</div><div class="line">};</div></pre></td></tr></table></figure>

<p>这些变量在 <code>inet</code> 地址族初始化的时候被注册。<a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/af_inet.c#L1720-L1725" target="_blank" rel="external">net/ipv4/af_inet.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"> *      Add all the base protocols.</div><div class="line"> */</div><div class="line"></div><div class="line"><span class="keyword">if</span> (inet_add_protocol(&icmp_protocol, IPPROTO_ICMP) &lt; <span class="number">0</span>)</div><div class="line">        pr_crit(<span class="string">"%s: Cannot add ICMP protocol\n"</span>, __func__);</div><div class="line"><span class="keyword">if</span> (inet_add_protocol(&udp_protocol, IPPROTO_UDP) &lt; <span class="number">0</span>)</div><div class="line">        pr_crit(<span class="string">"%s: Cannot add UDP protocol\n"</span>, __func__);</div><div class="line"><span class="keyword">if</span> (inet_add_protocol(&tcp_protocol, IPPROTO_TCP) &lt; <span class="number">0</span>)</div><div class="line">        pr_crit(<span class="string">"%s: Cannot add TCP protocol\n"</span>, __func__);</div></pre></td></tr></table></figure>

<p>接下来我们详细查看 UDP 协议。上面可以看到，UDP 的回调函数是 <code>udp_rcv</code>。这是从 IP 层进入 UDP 层的入口。我们就从这里开始探索。</p>
<h3 id="UDP_协议层">UDP 协议层</h3>
<p>UDP 协议层的实现见<a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c" target="_blank" rel="external">net/ipv4/udp.c</a>。</p>
<h4 id="udp_rcv"><code>udp_rcv</code></h4>
<p>这个函数只要一行，调用<code>__udp4_lib_rcv</code> 接收 UDP 报文。</p>
<h4 id="__udp4_lib_rcv"><code>__udp4_lib_rcv</code></h4>
<p><code>__udp4_lib_rcv</code> 首先对包数据进行合法性检查，获取 UDP 头、UDP 数据报长度、源地址、目标地址等信息。然后进行其他一些完整性检测和 checksum 验证。</p>
<p>回忆前面的 IP 层内容，在送到更上面一层协议（这里是 UDP）之前，会将一个 <code>dst_entry</code> 会关联到 <code>skb</code>。</p>
<p>如果对应的 <code>dst_entry</code> 找到了，并且有对应的 socket，<code>__udp4_lib_rcv</code> 会将 packet 放到 <code>socket</code> 的接收队列：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">sk = skb_steal_sock(skb);</div><div class="line"><span class="keyword">if</span> (sk) {</div><div class="line">  <span class="keyword">struct</span> dst_entry *dst = skb_dst(skb);</div><div class="line">  <span class="keyword">int</span> ret;</div><div class="line"></div><div class="line">  <span class="keyword">if</span> (unlikely(sk-&gt;sk_rx_dst != dst))</div><div class="line">    udp_sk_rx_dst_set(sk, dst);</div><div class="line"></div><div class="line">  ret = udp_queue_rcv_skb(sk, skb);</div><div class="line">  sock_put(sk);</div><div class="line">  <span class="comment">/* a return value &gt; 0 means to resubmit the input, but</span></div><div class="line">   * it wants the return to be -protocol, or 0</div><div class="line">   */</div><div class="line">  <span class="keyword">if</span> (ret &gt; <span class="number">0</span>)</div><div class="line">    <span class="keyword">return</span> -ret;</div><div class="line">  <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">} <span class="keyword">else</span> {</div></pre></td></tr></table></figure>

<p>如果 <code>early_demux</code> 中没有关联 socket 信息，那此时会调用<code>__udp4_lib_lookup_skb</code> 查找对应的 socket。</p>
<p>以上两种情况，最后都会将 packet 放到 socket 的接收队列：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ret = udp_queue_rcv_skb(sk, skb);</div><div class="line">sock_put(sk);</div></pre></td></tr></table></figure>

<p>如果 socket 没有找到，数据报(datagram)会被丢弃：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/* No socket. Drop packet silently, if checksum is wrong */</span></div><div class="line"><span class="keyword">if</span> (udp_lib_checksum_complete(skb))</div><div class="line">        <span class="keyword">goto</span> csum_error;</div><div class="line"></div><div class="line">UDP_INC_STATS_BH(net, UDP_MIB_NOPORTS, proto == IPPROTO_UDPLITE);</div><div class="line">icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, <span class="number">0</span>);</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"> * Hmm.  We got an UDP packet to a port to which we</div><div class="line"> * don't wanna listen.  Ignore it.</div><div class="line"> */</div><div class="line">kfree_skb(skb);</div><div class="line"><span class="keyword">return</span> <span class="number">0</span>;</div></pre></td></tr></table></figure>

<h4 id="11-3-3_udp_queue_rcv_skb">11.3.3 <code>udp_queue_rcv_skb</code></h4>
<p>这个函数的前面部分所做的工作：</p>
<ol>
<li>判断和这个数据报关联的 socket 是不是<a href="https://tools.ietf.org/html/rfc3948" target="_blank" rel="external">encapsulation</a> socket。如果是，将 packet 送到该层的处理函数</li>
<li>判断这个数据报是不是 UDP-Lite 数据报，做一些完整性检测</li>
<li>验证 UDP 数据报的校验和，如果校验失败，就丢弃</li>
</ol>
<p>最后，我们来到了 socket 的接收队列逻辑，判断队列是不是满了：<br><a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1548-L1549" target="_blank" rel="external">net/ipv4/udp.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> (sk_rcvqueues_full(sk, skb, sk-&gt;sk_rcvbuf))</div><div class="line">  <span class="keyword">goto</span> drop;</div></pre></td></tr></table></figure>

<h4 id="sk_rcvqueues_full"><code>sk_rcvqueues_full</code></h4>
<p>定义如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"> * Take into account size of receive queue and backlog queue</div><div class="line"> * Do not take into account this skb truesize,</div><div class="line"> * to allow even a single big packet to come.</div><div class="line"> */</div><div class="line"><span class="keyword">static</span> <span class="keyword">inline</span> <span class="keyword">bool</span> sk_rcvqueues_full(<span class="keyword">const</span> <span class="keyword">struct</span> sock *sk, <span class="keyword">const</span> <span class="keyword">struct</span> sk_buff *skb,</div><div class="line">                                     <span class="keyword">unsigned</span> <span class="keyword">int</span> limit)</div><div class="line">{</div><div class="line">        <span class="keyword">unsigned</span> <span class="keyword">int</span> qsize = sk-&gt;sk_backlog.len + atomic_read(&sk-&gt;sk_rmem_alloc);</div><div class="line"></div><div class="line">        <span class="keyword">return</span> qsize &gt; limit;</div><div class="line">}</div></pre></td></tr></table></figure>

<p>Tuning these values is a bit tricky as there are many things that can be adjusted.</p>
<h5 id="调优:_Socket_receive_queue_memory">调优: Socket receive queue memory</h5>
<p>上面看到，判断 socket 接收队列是否满了是和 <code>sk-&gt;sk_rcvbuf</code> 做比较。这个值可以被两个 sysctl 参数控制：最大值和默认值：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.rmem_max=<span class="number">8388608</span></div><div class="line"></div><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.core.rmem_default=<span class="number">8388608</span></div></pre></td></tr></table></figure>

<p>你也可以在你的应用里调用 <code>setsockopt</code> 带上 <code>SO_RCVBUF</code> 来修改这个值(<code>sk-&gt;sk_rcvbuf</code>)，能设置的最大值不能超过 <code>net.core.rmem_max</code>。</p>
<p>但是，你也可以 <code>setsockopt</code> 带上 <code>SO_RCVBUFFORCE</code> 来覆盖 <code>net.core.rmem_max</code>，但是执行应用的用户要有 <code>CAP_NET_ADMIN</code> 权限。</p>
<p><code>skb_set_owner_r</code> 函数设置 UDP 数据包的 owner，并会更新计数器 <code>sk-&gt;sk_rmem_alloc</code>。</p>
<p>我们接下来会看到。</p>
<p><code>sk_add_backlog</code> 函数会更新 <code>sk-&gt;sk_backlog.len</code> 计数，后面看。</p>
<h4 id="udp_queue_rcv_skb"><code>udp_queue_rcv_skb</code></h4>
<p>判断 queue 未满之后，就会将数据报放到里面：<a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1554-L1561" target="_blank" rel="external">net/ipv4/udp.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">bh_lock_sock(sk);</div><div class="line"><span class="keyword">if</span> (!sock_owned_by_user(sk))</div><div class="line">  rc = __udp_queue_rcv_skb(sk, skb);</div><div class="line"><span class="keyword">else</span> <span class="keyword">if</span> (sk_add_backlog(sk, skb, sk-&gt;sk_rcvbuf)) {</div><div class="line">  bh_unlock_sock(sk);</div><div class="line">  <span class="keyword">goto</span> drop;</div><div class="line">}</div><div class="line">bh_unlock_sock(sk);</div><div class="line"></div><div class="line"><span class="keyword">return</span> rc;</div></pre></td></tr></table></figure>

<p>第一步先判断有没有用户空间的程序正在这个 socket 上进行系统调用。如果没有，就可以调用<code>__udp_queue_rcv_skb</code>将数据报放到接收队列；如果有，就调用 <code>sk_add_backlog</code> 将它放到 backlog 队列。</p>
<p>当用户空间程序释放在这个 socket 上的系统调用时（通过向内核调用 <code>release_sock</code>），这个数据报就从 backlog 移动到了接收队列。</p>
<h4 id="__udp_queue_rcv_skb"><code>__udp_queue_rcv_skb</code></h4>
<p>这个函数调用 <code>sock_queue_rcv_skb</code> 将数据报送到 socket 接收队列；如果失败，更新统计计数并释放 skb。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/net/ipv4/udp.c#L1431-L1443" target="_blank" rel="external">net/ipv4/udp.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">rc = sock_queue_rcv_skb(sk, skb);</div><div class="line"><span class="keyword">if</span> (rc &lt; <span class="number">0</span>) {</div><div class="line">  <span class="keyword">int</span> is_udplite = IS_UDPLITE(sk);</div><div class="line"></div><div class="line">  <span class="comment">/* Note that an ENOMEM error is charged twice */</span></div><div class="line">  <span class="keyword">if</span> (rc == -ENOMEM)</div><div class="line">    UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,is_udplite);</div><div class="line"></div><div class="line">  UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS, is_udplite);</div><div class="line">  kfree_skb(skb);</div><div class="line">  trace_udp_fail_queue_rcv_skb(rc, sk);</div><div class="line">  <span class="keyword">return</span> -<span class="number">1</span>;</div><div class="line">}</div></pre></td></tr></table></figure>

<h4 id="Monitoring:_UDP_protocol_layer_statistics">Monitoring: UDP protocol layer statistics</h4>
<p>以下文件可以获取非常有用的 UDP 统计：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">/<span class="keyword">proc</span>/net/snmp</div><div class="line">/<span class="keyword">proc</span>/net/udp</div><div class="line">/<span class="keyword">proc</span>/net/snmp</div></pre></td></tr></table></figure>

<h5 id="监控_UDP_协议统计：/proc/net/snmp">监控 UDP 协议统计：<code>/proc/net/snmp</code></h5>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cat /<span class="keyword">proc</span>/net/snmp | grep <span class="type">Udp</span>\:</div><div class="line"><span class="type">Udp</span>: <span class="type">InDatagrams</span> <span class="type">NoPorts</span> <span class="type">InErrors</span> <span class="type">OutDatagrams</span> <span class="type">RcvbufErrors</span> <span class="type">SndbufErrors</span></div><div class="line"><span class="type">Udp</span>: <span class="number">16314</span> <span class="number">0</span> <span class="number">0</span> <span class="number">17161</span> <span class="number">0</span> <span class="number">0</span></div></pre></td></tr></table></figure>

<p>Much like the detailed statistics found in this file for the IP protocol, you will need to read the protocol layer source to determine exactly when and where these values are incremented.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">InDatagram<span class="variable">s:</span> Incremented when recvmsg was used by <span class="keyword">a</span> userland program <span class="keyword">to</span> <span class="keyword">read</span> datagram. Also incremented when <span class="keyword">a</span> UDP packet <span class="keyword">is</span> encapsulated <span class="built_in">and</span> sent back <span class="keyword">for</span> processing.</div><div class="line">NoPort<span class="variable">s:</span> Incremented when UDP packets arrive destined <span class="keyword">for</span> <span class="keyword">a</span> port where <span class="keyword">no</span> program <span class="keyword">is</span> listening.</div><div class="line">InError<span class="variable">s:</span> Incremented in several case<span class="variable">s:</span> <span class="keyword">no</span> memory in the receive queue, when <span class="keyword">a</span> <span class="keyword">bad</span> checksum <span class="keyword">is</span> seen, <span class="built_in">and</span> <span class="keyword">if</span> sk_add_backlog fails <span class="keyword">to</span> <span class="built_in">add</span> the datagram.</div><div class="line">OutDatagram<span class="variable">s:</span> Incremented when <span class="keyword">a</span> UDP packet <span class="keyword">is</span> handed down without error <span class="keyword">to</span> the IP protocol layer <span class="keyword">to</span> <span class="keyword">be</span> sent.</div><div class="line">RcvbufError<span class="variable">s:</span> Incremented when sock_queue_rcv_skb reports that <span class="keyword">no</span> memory <span class="keyword">is</span> available; this happens <span class="keyword">if</span> sk-&gt;sk_rmem_alloc <span class="keyword">is</span> greater than <span class="built_in">or</span> equal <span class="keyword">to</span> sk-&gt;sk_rcvbuf.</div><div class="line">SndbufError<span class="variable">s:</span> Incremented <span class="keyword">if</span> the IP protocol layer reported <span class="keyword">an</span> error when trying <span class="keyword">to</span> send the packet <span class="built_in">and</span> <span class="keyword">no</span> error queue <span class="built_in">has</span> been setup. Also incremented <span class="keyword">if</span> <span class="keyword">no</span> send queue space <span class="built_in">or</span> kernel memory are available.</div><div class="line">InCsumError<span class="variable">s:</span> Incremented when <span class="keyword">a</span> UDP checksum failure <span class="keyword">is</span> detected. Note that in <span class="keyword">all</span> cases I could <span class="keyword">find</span>, InCsumErrors <span class="keyword">is</span> incrememnted at the same time <span class="keyword">as</span> InErrors. Thus, InErrors - InCsumErros should yield the <span class="built_in">count</span> of memory related errors <span class="keyword">on</span> the receive side.</div></pre></td></tr></table></figure>

<h5 id="监控_UDP_socket_统计：/proc/net/udp">监控 UDP socket 统计：<code>/proc/net/udp</code></h5>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ cat /<span class="keyword">proc</span>/net/udp</div><div class="line">  sl  local_address rem_address   st tx_queue rx_queue tr tm-&gt;<span class="keyword">when</span> retrnsmt   uid  timeout inode <span class="keyword">ref</span> <span class="type">pointer</span> drops</div><div class="line">  <span class="number">515</span>: <span class="number">00000000</span>:<span class="type">B346</span> <span class="number">00000000</span>:<span class="number">0000</span> <span class="number">07</span> <span class="number">00000000</span>:<span class="number">00000000</span> <span class="number">00</span>:<span class="number">00000000</span> <span class="number">00000000</span>   <span class="number">104</span>        <span class="number">0</span> <span class="number">7518</span> <span class="number">2</span> <span class="number">0000000000000000</span> <span class="number">0</span></div><div class="line">  <span class="number">558</span>: <span class="number">00000000</span>:<span class="number">0371</span> <span class="number">00000000</span>:<span class="number">0000</span> <span class="number">07</span> <span class="number">00000000</span>:<span class="number">00000000</span> <span class="number">00</span>:<span class="number">00000000</span> <span class="number">00000000</span>     <span class="number">0</span>        <span class="number">0</span> <span class="number">7408</span> <span class="number">2</span> <span class="number">0000000000000000</span> <span class="number">0</span></div><div class="line">  <span class="number">588</span>: <span class="number">0100007</span>F:<span class="number">038</span>F <span class="number">00000000</span>:<span class="number">0000</span> <span class="number">07</span> <span class="number">00000000</span>:<span class="number">00000000</span> <span class="number">00</span>:<span class="number">00000000</span> <span class="number">00000000</span>     <span class="number">0</span>        <span class="number">0</span> <span class="number">7511</span> <span class="number">2</span> <span class="number">0000000000000000</span> <span class="number">0</span></div><div class="line">  <span class="number">769</span>: <span class="number">00000000</span>:<span class="number">0044</span> <span class="number">00000000</span>:<span class="number">0000</span> <span class="number">07</span> <span class="number">00000000</span>:<span class="number">00000000</span> <span class="number">00</span>:<span class="number">00000000</span> <span class="number">00000000</span>     <span class="number">0</span>        <span class="number">0</span> <span class="number">7673</span> <span class="number">2</span> <span class="number">0000000000000000</span> <span class="number">0</span></div><div class="line">  <span class="number">812</span>: <span class="number">00000000</span>:<span class="number">006</span>F <span class="number">00000000</span>:<span class="number">0000</span> <span class="number">07</span> <span class="number">00000000</span>:<span class="number">00000000</span> <span class="number">00</span>:<span class="number">00000000</span> <span class="number">00000000</span>     <span class="number">0</span>        <span class="number">0</span> <span class="number">7407</span> <span class="number">2</span> <span class="number">0000000000000000</span> <span class="number">0</span></div></pre></td></tr></table></figure>

<p>The first line describes each of the fields in the lines following:</p>
<ul>
<li><code>sl</code>: Kernel hash slot for the socket</li>
<li><code>local_address</code>: Hexadecimal local address of the socket and port number, separated by :.</li>
<li><code>rem_address</code>: Hexadecimal remote address of the socket and port number, separated by :.</li>
<li><code>st</code>: The state of the socket. Oddly enough, the UDP protocol layer seems to use some TCP socket states. In the example above, 7 is TCP_CLOSE.</li>
<li><code>tx_queue</code>: The amount of memory allocated in the kernel for outgoing UDP datagrams.</li>
<li><code>rx_queue</code>: The amount of memory allocated in the kernel for incoming UDP datagrams.</li>
<li><code>tr</code>, tm-&gt;when, retrnsmt: These fields are unused by the UDP protocol layer.</li>
<li><code>uid</code>: The effective user id of the user who created this socket.</li>
<li><code>timeout</code>: Unused by the UDP protocol layer.</li>
<li><code>inode</code>: The inode number corresponding to this socket. You can use this to help you determine which user process has this socket open. Check /proc/[pid]/fd, which will contain symlinks to socket[:inode].</li>
<li><code>ref</code>: The current reference count for the socket.</li>
<li><code>pointer</code>: The memory address in the kernel of the struct sock.</li>
<li><code>drops</code>: The number of datagram drops associated with this socket. Note that this does not include any drops related to sending datagrams (on corked UDP sockets or otherwise); this is only incremented in receive paths as of the kernel version examined by this blog post.</li>
</ul>
<p>打印这些信息的代码见<a href="https://github.com/torvalds/linux/blob/master/net/ipv4/udp.c#L2396-L2431" target="_blank" rel="external">net/ipv4/udp.c</a>.</p>
<h3 id="将数据放到_socket_队列">将数据放到 socket 队列</h3>
<p>网络数据通过 <code>sock_queue_rcv</code> 进入 socket 的接收队列。这个函数在将数据报最终送到接收队列之前，会做几件事情：</p>
<ol>
<li>检查 socket 已分配的内存，如果超过了 receive buffer 的大小，丢弃这个包并更新计数</li>
<li>应用 <code>sk_filter</code>，这允许 BPF（Berkeley Packet Filter）过滤器在 socket 上被应用</li>
<li>执行 <code>sk_rmem_scedule</code>，确保有足够大的 receive buffer 接收这个数据报</li>
<li>执行 <code>skb_set_owner_r</code>，这会计算数据报的长度并更新 <code>sk-&gt;sk_rmem_alloc</code> 计数</li>
<li>调用<code>__skb_queue_tail</code> 将数据加到队列尾端</li>
</ol>
<p>最后，所有在这个 socket 上等待数据的进程都收到一个通知通过 <code>sk_data_ready</code> 通知处理函数。</p>
<p><strong>这就是一个数据包从到达机器开始，依次穿过协议栈，到达 socket，最终被用户程序读取的过程。</strong></p>
<h2 id="其他">其他</h2>
<p>还有一些值得讨论的地方，放在前面哪里都不太合适，故统一放到这里。</p>
<h3 id="打时间戳_(timestamping)">打时间戳 (timestamping)</h3>
<p>前面提到，网络栈可以收集包的时间戳信息。如果使用了 RPS 功能，有相应的 <code>sysctl</code> 参数可以控制何时以及如何收集时间戳；更多关于 RPS、时间戳，以及网络栈在哪里完成这些工作的内容，请查看前面的章节。一些网卡甚至支持在硬件上打时间戳。</p>
<p>如果你想看内核网络栈给收包增加了多少延迟，那这个特性非常有用。</p>
<p>内核<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/timestamping.txt" target="_blank" rel="external">关于时间戳的文档</a>非常优秀，甚至还包括一个<a href="https://github.com/torvalds/linux/tree/v3.13/Documentation/networking/timestamping" target="_blank" rel="external">示例程序和相应的 Makefile</a>，有兴趣的话可以上手试试。</p>
<p>使用 <code>ethtool -T</code> 可以查看网卡和驱动支持哪种打时间戳方式：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$ sudo ethtool -T eth0</div><div class="line">Time stamping parameters <span class="keyword">for</span> eth0:</div><div class="line">Capabilities:</div><div class="line">  software-transmit     (SOF_TIMESTAMPING_TX_SOFTWARE)</div><div class="line">  software-receive      (SOF_TIMESTAMPING_RX_SOFTWARE)</div><div class="line">  software-<span class="keyword">system</span>-clock (SOF_TIMESTAMPING_SOFTWARE)</div><div class="line">PTP Hardware Clock: <span class="constant">none</span></div><div class="line">Hardware Transmit Timestamp Modes: <span class="constant">none</span></div><div class="line">Hardware Receive Filter Modes: <span class="constant">none</span></div></pre></td></tr></table></figure>

<p>从上面这个信息看，该网卡不支持硬件打时间戳。但这个系统上的软件打时间戳，仍然可以帮助我判断内核在接收路径上到底带来多少延迟。</p>
<h3 id="socket_低延迟选项：busy_polling">socket 低延迟选项：busy polling</h3>
<p>socket 有个 <code>SO_BUSY_POLL</code> 选项，可以让内核在<strong>阻塞式接收</strong>（blocking receive）的时候做 busy poll。这个选项会减少延迟，但会增加 CPU 使用量和耗电量。</p>
<p><strong>重要提示</strong>：要使用此功能，首先要检查你的设备驱动是否支持。Linux 内核 3.13.0 的<code>igb</code> 驱动不支持，但 <code>ixgbe</code> 驱动支持。如果你的驱动实现(并注册)了 <code>struct net_device_ops</code>(前面介绍过了)的 <code>ndo_busy_poll</code> 方法，那它就是支持 <code>SO_BUSY_POLL</code>。</p>
<p>Intel 有一篇非常好的<a href="http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/open-source-kernel-enhancements-paper.pdf" target="_blank" rel="external">文章</a>介绍其原理。</p>
<p>对单个 socket 设置此选项，需要传一个以微秒（microsecond）为单位的时间，内核会在这个时间内对设备驱动的接收队列做 busy poll。当在这个 socket 上触发一个阻塞式读请求时，内核会 busy poll 来收数据。</p>
<p>全局设置此选项，可以修改 <code>net.core.busy_poll</code> 配置（毫秒，microsecond），当 <code>poll</code> 或 <code>select</code> 方法以阻塞方式调用时，busy poll 的时长就是这个值。</p>
<h3 id="Netpoll：特殊网络场景支持">Netpoll：特殊网络场景支持</h3>
<p>Linux 内核提供了一种方式，在内核挂掉（crash）的时候，设备驱动仍然可以接收和发送数据，相应的 API 被称作 <code>Netpoll</code>。这个功能在一些特殊的网络场景有用途，比如最著名的两个例子：<br><a href="http://sysprogs.com/VisualKernel/kgdboe/launch/" target="_blank" rel="external"><code>kgdb</code></a>和<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/netconsole.txt" target="_blank" rel="external"><code>netconsole</code></a>。</p>
<p>大部分驱动都支持 <code>Netpoll</code> 功能。支持此功能的驱动需要实现 <code>struct net_device_ops</code> 的<code>ndo_poll_controller</code> 方法（回调函数，探测驱动模块的时候注册的，前面介绍过）。</p>
<p>当网络设备子系统收包或发包的时候，会首先检查这个包的目的端是不是 <code>netpoll</code>。</p>
<p>例如，我们来看下<code>__netif_receive_skb_core</code>，<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3511-L3514" target="_blank" rel="external">net/dev/core.c</a>:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">static</span> <span class="keyword">int</span> __netif_receive_skb_core(<span class="keyword">struct</span> sk_buff *skb, <span class="keyword">bool</span> pfmemalloc)</div><div class="line">{</div><div class="line"></div><div class="line">  <span class="comment">/* ... */</span></div><div class="line"></div><div class="line">  <span class="comment">/* if we've gotten here through NAPI, check netpoll */</span></div><div class="line">  <span class="keyword">if</span> (netpoll_receive_skb(skb))</div><div class="line">    <span class="keyword">goto</span> out;</div><div class="line"></div><div class="line">  <span class="comment">/* ... */</span></div><div class="line">}</div></pre></td></tr></table></figure>

<p>设备驱动收发包相关代码里，关于 <code>netpoll</code> 的判断逻辑在很前面。</p>
<p>Netpoll API 的消费者可以通过 <code>netpoll_setup</code> 函数注册 <code>struct netpoll</code> 变量，后者有收包和发包的 hook 方法（函数指针）。</p>
<p>如果你对使用 Netpoll API 感兴趣，可以看看<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/netconsole.c" target="_blank" rel="external">netconsole</a><br>的<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/netconsole.c" target="_blank" rel="external">驱动</a>，Netpoll API 的头文件<a href="https://github.com/torvalds/linux/blob/v3.13/include/linux/netpoll.h" target="_blank" rel="external"><code>include/linux/netpoll.h</code></a>，以及<a href="http://people.redhat.com/~jmoyer/netpoll-linux_kongress-2005.pdf" target="_blank" rel="external">这个</a>精彩的分享。</p>
<h3 id="SO_INCOMING_CPU"><code>SO_INCOMING_CPU</code></h3>
<p><code>SO_INCOMING_CPU</code> 直到 Linux 3.19 才添加, 但它非常有用，所以这里讨论一下。</p>
<p>使用 <code>getsockopt</code> 带 <code>SO_INCOMING_CPU</code> 选项，可以判断当前哪个 CPU 在处理这个 socket 的网络包。你的应用程序可以据此将 socket 交给在期望的 CPU 上运行的线程，增加数据本地性（data locality）和 CPU 缓存命中率。</p>
<p>在提出 <code>SO_INCOMING_CPU</code> 的<a href="https://patchwork.ozlabs.org/patch/408257/" target="_blank" rel="external">邮件列表</a>里有一个简单示例框架，展示在什么场景下使用这个功能。</p>
<h3 id="DMA_引擎">DMA 引擎</h3>
<p>DMA engine (直接内存访问引擎)是一个硬件，允许 CPU 将<strong>很大的复制操作</strong>（large copy operations）offload（下放）给它。这样 CPU 就从数据拷贝中解放出来，去做其他工作，而拷贝就交由硬件完成。合理的使用 DMA 引擎（代码要利用到 DMA 特性）可以减少 CPU 的使用量。</p>
<p>Linux 内核有一个通用的 DMA 引擎接口，DMA engine 驱动实现这个接口即可。更多关于这个接口的信息可以查看内核源码的<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/dmaengine.txt" target="_blank" rel="external">文档</a>。</p>
<p>内核支持的 DMA 引擎很多，这里我们拿 Intel 的<a href="https://en.wikipedia.org/wiki/I/O_Acceleration_Technology" target="_blank" rel="external">IOAT DMA engine</a>为例来看一下。</p>
<h4 id="Intel’s_I/O_Acceleration_Technology_(IOAT)">Intel’s I/O Acceleration Technology (IOAT)</h4>
<p>很多服务器都安装了<a href="http://www.intel.com/content/www/us/en/wireless-network/accel-technology.html" target="_blank" rel="external">Intel I/O AT bundle</a>，其中包含了一系列性能优化相关的东西，包括一个硬件 DMA 引擎。可以查看 <code>dmesg</code> 里面有没有 <code>ioatdma</code>，判断这个模块是否被加载，以及它是否找到了支持的硬件。</p>
<p>DMA 引擎在很多地方有用到，例如 TCP 协议栈。</p>
<p>Intel IOAT DMA engine 最早出现在 Linux 2.6.18，但随后 3.13.11.10 就禁用掉了，因为有一些 bug，会导致数据损坏。</p>
<p><code>3.13.11.10</code> 版本之前的内核默认是开启的，将来这些版本的内核如果有更新，可能也会禁用掉。</p>
<h5 id="直接缓存访问_(DCA,_Direct_cache_access)">直接缓存访问 (DCA, Direct cache access)</h5>
<p><a href="http://www.intel.com/content/www/us/en/wireless-network/accel-technology.html" target="_blank" rel="external">Intel I/O AT bundle</a>中的另一个有趣特性是直接缓存访问（DCA）。</p>
<p>该特性允许网络设备（通过各自的驱动）直接将网络数据放到 CPU 缓存上。至于是如何实现的，随各家驱动而异。对于 <code>igb</code> 的驱动，你可以查看 <code>igb_update_dca</code> 和<code>igb_update_rx_dca</code> 这两个函数的实现。<code>igb</code> 驱动使用 DCA，直接写硬件网卡的一个寄存器。</p>
<p>要使用 DCA 功能，首先检查你的 BIOS 里是否打开了此功能，然后确保 <code>dca</code> 模块加载了，还要确保你的网卡和驱动支持 DCA。</p>
<h5 id="Monitoring_IOAT_DMA_engine">Monitoring IOAT DMA engine</h5>
<p>如上所说，如果你不怕数据损坏的风险，那你可以使用 <code>ioatdma</code> 模块。监控上，可以看几个 sysfs 参数。</p>
<p>例如，监控一个 DMA 通道（channel）总共 offload 的 <code>memcpy</code> 操作次数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>cat /sys/<span class="class"><span class="keyword">class</span>/<span class="title">dma</span>/<span class="title">dma0chan0</span>/<span class="title">memcpy_count</span></span></div><div class="line"><span class="number">123205655</span></div></pre></td></tr></table></figure>

<p>类似的，一个 DMA 通道总共 offload 的字节数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>cat /sys/<span class="class"><span class="keyword">class</span>/<span class="title">dma</span>/<span class="title">dma0chan0</span>/<span class="title">bytes_transferred</span></span></div><div class="line"><span class="number">131791916307</span></div></pre></td></tr></table></figure>

<h5 id="Tuning_IOAT_DMA_engine">Tuning IOAT DMA engine</h5>
<p>IOAT DMA engine 只有在包大小超过一定的阈值之后才会使用，这个阈值叫 <code>copybreak</code>。之所以要设置阈值是因为，对于小包，设置和使用 DMA 的开销要大于其收益。</p>
<p>调整 DMA engine <code>copybreak</code> 参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">sudo</span> sysctl -w net.ipv4.tcp_dma_copybreak=<span class="number">2048</span></div></pre></td></tr></table></figure>

<p>默认值是 4096。</p>
<h2 id="总结">总结</h2>
<p>Linux 网络栈很复杂。</p>
<p>对于这样复杂的系统（以及类似的其他系统），如果不能在更深的层次理解它正在做什么，就不可能做监控和调优。当遇到网络问题时，你可能会在网上搜到一些 <code>sysctl.conf</code> 最优实践一类的东西，然后应<br>用在自己的系统上，但这并不是网络栈调优的最佳方式。</p>
<p>监控网络栈需要从驱动开始，逐步往上，仔细地在每一层统计网络数据。这样你才能清楚地看出哪里有丢包（drop），哪里有收包错误（errors），然后根据导致错误的原因做相应的配置调整。</p>
<p><strong>不幸的是，这项工作并没有捷径。</strong></p>
<h2 id="额外讨论和帮助">额外讨论和帮助</h2>
<p>需要一些额外的关于网络栈的指导(navigating the network stack)？对本文有疑问，或有相关内容本文没有提到？以上问题，都可以发邮件给<a href="support@packagecloud.io">我们</a>，以便我们知道如何提供帮助。</p>
<h2 id="相关文章">相关文章</h2>
<p>如果你喜欢本文，你可能对下面这些底层技术文章也感兴趣：</p>
<ul>
<li><a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/" target="_blank" rel="external">Monitoring and Tuning the Linux Networking Stack: Sending Data</a></li>
<li><a href="https://blog.packagecloud.io/eng/2016/04/05/the-definitive-guide-to-linux-system-calls/" target="_blank" rel="external">The Definitive Guide to Linux System Calls</a></li>
<li><a href="https://blog.packagecloud.io/eng/2016/02/29/how-does-strace-work/" target="_blank" rel="external">How does strace work?</a></li>
<li><a href="https://blog.packagecloud.io/eng/2016/03/14/how-does-ltrace-work/" target="_blank" rel="external">How does ltrace work?</a></li>
<li><a href="https://blog.packagecloud.io/eng/2016/03/21/apt-hash-sum-mismatch/" target="_blank" rel="external">APT Hash sum mismatch</a></li>
<li><a href="https://blog.packagecloud.io/eng/2014/10/28/howto-gpg-sign-verify-deb-packages-apt-repositories/" target="_blank" rel="external">HOWTO: GPG sign and verify deb packages and APT repositories</a></li>
<li><a href="https://blog.packagecloud.io/eng/2014/11/24/howto-gpg-sign-verify-rpm-packages-yum-repositories/" target="_blank" rel="external">HOWTO: GPG sign and verify RPM packages and yum repositories</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      

      
      <!-- <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
      <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script> -->
      <section id="comments">
        <script src="https://utteranc.es/client.js"
                repo="smallnest/gitalk"
                issue-term="title"
                theme="github-light"
                crossorigin="anonymous"
                async>
        </script>
        <!-- <div id="gitalk-container"></div>
        <script type="text/javascript">
          var gitalkOpts = {
            id: '2019/12/09/monitoring-tuning-linux-networking-st',
            owner: 'smallnest',
            repo: 'gitalk',
            title: '[转][译] Linux 网络栈监控和调优：接收数据（2016）',
            body: 'https://colobu.com/2019/12/09/monitoring-tuning-linux-networking-stack-receiving-data/',
            clientID: 'bc02724130ed5b7ee275',
            clientSecret: '68cb0bae2f93a8b88b09e0eb9b08c844b06a9047',
            admin: ['smallnest'],
            distractionFreeMode: false
          };

          const gitalk = new Gitalk(gitalkOpts)
          gitalk.render('gitalk-container')
        </script> -->
        <noscript> 为正常使用评论功能请激活JavaScript</noscript>
      </section>

      

    </footer>
  </div>
  
  
<nav id="article-nav">
  
    <a href="/2019/12/09/monitoring-tuning-linux-networking-stack-sending-data/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [转][译]Linux 网络栈监控和调优：发送数据（2017）
        
      </div>
    </a>
  
  
    <a href="/2019/12/02/How-to-Setup-MySQL-Master-Master-Replication/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[译]配置 MySQL 主主复制</div>
    </a>
  
</nav>

  
</article></section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">访问者来源</h3>
  <div class="widget">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=Hf4EJSi2XvL6TMcuFSH51Qn6nf5nZ8qnjVBnWCQ4FGc"></script>
  </div>
</div>

<div class="widget-wrap">
  <h3 class="widget-title">微信公众号</h3>
  <div class="widget">
    <img width="100%" src="/images/widgets/gopatterns.jpg">
  </div>
</div>

<div class="widget-wrap">
  <h3 class="widget-title">极客时间专栏</h3>
  <div class="widget">
    <a href="https://time.geekbang.org/column/intro/100061801">
      <img width="100%" src="/images/widgets/geekbang.png">
    </a>
  </div>
</div>

<div class="widget-wrap">
    <h3 class="widget-title">出版图书</h3>
    <div class="widget">
      <a href="https://cpgo.colobu.com/">
        <img width="100%" src="/cpgolang/cpgo.png">
      </a>
    </div>
    <div class="widget">
      <a href="https://item.jd.com/14347716.html">
        <img width="100%" src="/100gomistakes/cover.png">
      </a>
    </div>
    <div class="widget">
      <a href="/ScalaCollectionsCookbook/">
        <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook.jpg">
        <img width="100%" src="/ScalaCollectionsCookbook/scala_collections_cookbook_tw.png">
      </a>
    </div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DOTNET/">DOTNET</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Go/">Go</a><span class="category-list-count">283</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a><span class="category-list-count">64</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Rust/">Rust</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/go/">go</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/k8s/">k8s</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/rust/">rust</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/分享/">分享</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/前端开发/">前端开发</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/区块链/">区块链</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据/">大数据</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a><span class="category-list-count">28</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/架构/">架构</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/管理/">管理</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/网络编程/">网络编程</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/设计模式/">设计模式</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/读书笔记/">读书笔记</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/运维/">运维</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/高并发编程/">高并发编程</a><span class="category-list-count">20</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/Android/" style="font-size: 15.71px;">Android</a><a href="/tags/ApacheBench/" style="font-size: 11.43px;">ApacheBench</a><a href="/tags/Bower/" style="font-size: 10.00px;">Bower</a><a href="/tags/C/" style="font-size: 10.00px;">C#</a><a href="/tags/CDN/" style="font-size: 10.00px;">CDN</a><a href="/tags/CQRS/" style="font-size: 10.00px;">CQRS</a><a href="/tags/CRC/" style="font-size: 10.00px;">CRC</a><a href="/tags/CSS/" style="font-size: 11.43px;">CSS</a><a href="/tags/CompletableFuture/" style="font-size: 10.00px;">CompletableFuture</a><a href="/tags/Comsat/" style="font-size: 10.00px;">Comsat</a><a href="/tags/Curator/" style="font-size: 18.57px;">Curator</a><a href="/tags/DSL/" style="font-size: 10.00px;">DSL</a><a href="/tags/Disruptor/" style="font-size: 10.00px;">Disruptor</a><a href="/tags/Docker/" style="font-size: 11.43px;">Docker</a><a href="/tags/Ember/" style="font-size: 11.43px;">Ember</a><a href="/tags/FastJson/" style="font-size: 10.00px;">FastJson</a><a href="/tags/Fiber/" style="font-size: 10.00px;">Fiber</a><a href="/tags/GAE/" style="font-size: 10.00px;">GAE</a><a href="/tags/GC/" style="font-size: 12.86px;">GC</a><a href="/tags/Gnuplot/" style="font-size: 10.00px;">Gnuplot</a><a href="/tags/Go/" style="font-size: 14.29px;">Go</a><a href="/tags/Gradle/" style="font-size: 10.00px;">Gradle</a><a href="/tags/Grunt/" style="font-size: 10.00px;">Grunt</a><a href="/tags/Gulp/" style="font-size: 10.00px;">Gulp</a><a href="/tags/Hadoop/" style="font-size: 10.00px;">Hadoop</a><a href="/tags/Hazelcast/" style="font-size: 10.00px;">Hazelcast</a><a href="/tags/IPFS/" style="font-size: 10.00px;">IPFS</a><a href="/tags/Ignite/" style="font-size: 10.00px;">Ignite</a><a href="/tags/JVM/" style="font-size: 10.00px;">JVM</a><a href="/tags/Java/" style="font-size: 17.14px;">Java</a><a href="/tags/Kafka/" style="font-size: 20.00px;">Kafka</a><a href="/tags/Lambda/" style="font-size: 14.29px;">Lambda</a><a href="/tags/Linux/" style="font-size: 12.86px;">Linux</a><a href="/tags/LongAdder/" style="font-size: 10.00px;">LongAdder</a><a href="/tags/MathJax/" style="font-size: 10.00px;">MathJax</a><a href="/tags/Maven/" style="font-size: 11.43px;">Maven</a><a href="/tags/Memcached/" style="font-size: 10.00px;">Memcached</a><a href="/tags/Metrics/" style="font-size: 10.00px;">Metrics</a><a href="/tags/Mongo/" style="font-size: 12.86px;">Mongo</a><a href="/tags/Netty/" style="font-size: 15.71px;">Netty</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">October 2022</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">August 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/07/">July 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">June 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">May 2021</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">April 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">November 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">August 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">July 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">October 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/02/">February 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">December 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">March 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/11/">November 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/04/">April 2015</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/10/">October 2014</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/09/">September 2014</a><span class="archive-list-count">28</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/08/">August 2014</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/07/">July 2014</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">近期文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/06/04/redka-redis-with-sqlite/">Redka - 父亲是Redis，母亲是SQLite</a>
          </li>
        
          <li>
            <a href="/2024/06/03/command-dispacher-pattern/">命令分发模式</a>
          </li>
        
          <li>
            <a href="/2024/05/22/parse-tcp-timestamp-in-Rust/">使用Rust捕获和解析网络包</a>
          </li>
        
          <li>
            <a href="/2024/05/20/implemenmt-pping-in-go/">使用Go语言实现 pping</a>
          </li>
        
          <li>
            <a href="/2024/05/19/let-Rob-Pike-write-a-Red-Black-tree/">让 Rob Pike 或者字节跳动的同学实现一个红黑树</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">友情链接</h3>
    <div class="widget">
      <ul>
        
          <li>
			 
            <a href="http://stackshare.io" target="_blank">技术栈</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="https://toutiao.io/" target="_blank">开发者头条</a>
			
          </li>
        
          <li>
			 
            <a href="http://weekly.manong.io/issues/" target="_blank">码农周刊</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.tuicool.com/mags" target="_blank">编程狂人周刊</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.importnew.com/" target="_blank">importnew</a>
			
          </li>
        
          <li>
			 
            <a href="http://ifeve.com/" target="_blank">并发编程网</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="http://github.com" target="_blank">github</a>
			
          </li>
        
          <li>
			 
            <a href="http://stackoverflow.com/" target="_blank">stackoverflow</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.javacodegeeks.com/" target="_blank">javacodegeeks</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.infoq.com/" target="_blank">infoq</a>
			
          </li>
        
          <li>
			 
            <a href="http://www.dzone.com/links/index.html" target="_blank">dzone</a>
			
          </li>
        
          <li>
			 
            <a href="https://oj.leetcode.com/problems/" target="_blank">leetcode</a>
			
          </li>
        
          <li>
			 
            <a href="http://tutorials.jenkov.com" target="_blank">jenkov</a>
			
          </li>
        
          <li>
			 
            <a href="https://howtodoinjava.com" target="_blank">HowToDoInJava</a>
			
          </li>
        
          <li>
			 
            <a href="https://java-design-patterns.com/patterns/" target="_blank">java design patterns</a>
			
          </li>
        
          <li>
			 
			&nbsp;
			
          </li>
        
          <li>
			 
            <a href="https://medium.com/netflix-techblog" target="_blank">Netflix技术博客</a>
			
          </li>
        
          <li>
			 
            <a href="https://www.techiedelight.com" target="_blank">Techie Delight</a>
			
          </li>
        
          <li>
			 
            <a href="https://engineering.linkedin.com/blog" target="_blank">Linkedin技术博客</a>
			
          </li>
        
          <li>
			 
            <a href="https://blogs.dropbox.com/tech/" target="_blank">Dropbox技术博客</a>
			
          </li>
        
          <li>
			 
            <a href="https://code.fb.com" target="_blank">Facebook技术博客</a>
			
          </li>
        
          <li>
			 
            <a href="http://jm.taobao.org" target="_blank">淘宝中间件团队</a>
			
          </li>
        
          <li>
			 
            <a href="https://tech.meituan.com" target="_blank">美团技术博客</a>
			
          </li>
        
          <li>
			 
            <a href="http://blogs.360.cn" target="_blank">360技术博客</a>
			
          </li>
        
          <li>
			 
            <a href="https://xiaomi-info.github.io" target="_blank">小米信息部技术团队</a>
			
          </li>
        
      </ul>
    </div>
  </div>

  
      

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 smallnest<br>
	  Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    
      <a href="/" class="mobile-nav-link"><i class="fa fa-home">&nbsp;</i>首页</a>
    
  
    
      <a href="/archives" class="mobile-nav-link"><i class="fa fa-folder-o">&nbsp;</i>归档</a>
    
  
    
      <a href="https://github.com/smallnest" class="mobile-nav-link"><i class="fa fa-github">&nbsp;</i>github</a>
    
  
    
      <a class="mobile-nav-link" href="#"><i class="fa fa-bars">&nbsp;</i>网站群</a>
    
      
            <a class="mobile-nav-link" href="/goasm">&nbsp;&nbsp;<i class="fa fa-language">&nbsp;</i>Go汇编示例</a>
          
          
    
      
            <a class="mobile-nav-link" href="https://gowebexamples.com">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go Web开发示例</a>
          
          
    
      
            <a class="mobile-nav-link" href="http://go-database-sql.org">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go 数据库开发教程</a>
          
          
    
      
            <a class="mobile-nav-link" href="https://colobu.com/gotips/">&nbsp;&nbsp;<i class="fa fa-external-link">&nbsp;</i>Go 语言编程技巧</a>
          
          
    
      
            
          
          
    
      
            <a class="mobile-nav-link" href="/perf-book">&nbsp;&nbsp;<i class="fa fa-brands fa-rust">&nbsp;</i>Rust高性能编程指南</a>
          
          
    
      
            <a class="mobile-nav-link" href="/rust100">&nbsp;&nbsp;<i class="fa fa-brands fa-rust">&nbsp;</i>100个练习题学习Rust</a>
          
          
    
      
            
          
          
    
      
            <a class="mobile-nav-link" href="http://rpcx.io">&nbsp;&nbsp;<i class="fa undefined">&nbsp;</i>RPCX官网</a>
          
          
    
      
            <a class="mobile-nav-link" href="http://cn.doc.rpcx.io">&nbsp;&nbsp;<i class="fa undefined">&nbsp;</i>RPC开发指南</a>
          
          
    
    
  
    
      <a href="/ScalaCollectionsCookbook" class="mobile-nav-link"><i class="fa fa-solid fa-book">&nbsp;</i>Scala集合技术手册</a>
    
  
    
      <a href="/about" class="mobile-nav-link"><i class="fa fa-lemon-o">&nbsp;</i>关于</a>
    
  
</nav>
    
<script src="//cdn.staticfile.org/jquery/1.11.1/jquery.min.js"></script>
<script src="//cdn.bootcss.com/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"></script>


<script src="/js/script.js" type="text/javascript"></script>

<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.0-beta.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<div id="totop" style="position:fixed;bottom:150px;right:10px;cursor: pointer;z-index: 2000;">
	<a title="返回顶部"><img src="/images/scrollup.png"/></a>
</div>
<script src="/js/totop.js" type="text/javascript"></script>




<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?e085d87993250aab11f3e0c15f1c2785";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </div>
</body>
</html>